{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# FLIS baseline Notebook\n",
        "#### The reimplementation of FLIS baseline was carried out on Google Colab Pro\n",
        "#### We used the following codebase: https://github.com/MMorafah/FLIS\n",
        "#### To reimplement, download, unzip and run the code like below:"
      ],
      "metadata": {
        "id": "SPsWhNE_1wV_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOtqq1sAgjPD",
        "outputId": "3e42fcc5-c944-42a6-84f1-3f73edf18bb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  FLIS.zip\n",
            "   creating: FLIS/\n",
            "   creating: FLIS/.git/\n",
            "   creating: FLIS/.git/branches/\n",
            "  inflating: FLIS/.git/config        \n",
            "  inflating: FLIS/.git/description   \n",
            "  inflating: FLIS/.git/HEAD          \n",
            "   creating: FLIS/.git/hooks/\n",
            "  inflating: FLIS/.git/hooks/applypatch-msg.sample  \n",
            "  inflating: FLIS/.git/hooks/commit-msg.sample  \n",
            "  inflating: FLIS/.git/hooks/fsmonitor-watchman.sample  \n",
            "  inflating: FLIS/.git/hooks/post-update.sample  \n",
            "  inflating: FLIS/.git/hooks/pre-applypatch.sample  \n",
            "  inflating: FLIS/.git/hooks/pre-commit.sample  \n",
            "  inflating: FLIS/.git/hooks/pre-merge-commit.sample  \n",
            "  inflating: FLIS/.git/hooks/pre-push.sample  \n",
            "  inflating: FLIS/.git/hooks/pre-rebase.sample  \n",
            "  inflating: FLIS/.git/hooks/pre-receive.sample  \n",
            "  inflating: FLIS/.git/hooks/prepare-commit-msg.sample  \n",
            "  inflating: FLIS/.git/hooks/push-to-checkout.sample  \n",
            "  inflating: FLIS/.git/hooks/update.sample  \n",
            "  inflating: FLIS/.git/index         \n",
            "   creating: FLIS/.git/info/\n",
            "  inflating: FLIS/.git/info/exclude  \n",
            "   creating: FLIS/.git/logs/\n",
            "  inflating: FLIS/.git/logs/HEAD     \n",
            "   creating: FLIS/.git/logs/refs/\n",
            "   creating: FLIS/.git/logs/refs/heads/\n",
            "  inflating: FLIS/.git/logs/refs/heads/main  \n",
            "   creating: FLIS/.git/logs/refs/remotes/\n",
            "   creating: FLIS/.git/logs/refs/remotes/origin/\n",
            "  inflating: FLIS/.git/logs/refs/remotes/origin/HEAD  \n",
            "   creating: FLIS/.git/objects/\n",
            "   creating: FLIS/.git/objects/info/\n",
            "   creating: FLIS/.git/objects/pack/\n",
            "  inflating: FLIS/.git/objects/pack/pack-3b06b7c0df2ce7975f8f87344c3c13545aa9f77e.idx  \n",
            "  inflating: FLIS/.git/objects/pack/pack-3b06b7c0df2ce7975f8f87344c3c13545aa9f77e.pack  \n",
            "  inflating: FLIS/.git/packed-refs   \n",
            "   creating: FLIS/.git/refs/\n",
            "   creating: FLIS/.git/refs/heads/\n",
            "  inflating: FLIS/.git/refs/heads/main  \n",
            "   creating: FLIS/.git/refs/remotes/\n",
            "   creating: FLIS/.git/refs/remotes/origin/\n",
            "  inflating: FLIS/.git/refs/remotes/origin/HEAD  \n",
            "   creating: FLIS/.git/refs/tags/\n",
            "  inflating: FLIS/.gitignore         \n",
            "  inflating: FLIS/LICENSE            \n",
            "  inflating: FLIS/main_FLIS_DC.py    \n",
            "  inflating: FLIS/main_FLIS_HC.py    \n",
            "  inflating: FLIS/README.md          \n",
            "   creating: FLIS/scripts/\n",
            "  inflating: FLIS/scripts/flis_dc.sh  \n",
            "  inflating: FLIS/scripts/flis_hc.sh  \n",
            "   creating: FLIS/src/\n",
            "   creating: FLIS/src/client/\n",
            "  inflating: FLIS/src/client/client_fedavg.py  \n",
            "  inflating: FLIS/src/client/client_FLIS.py  \n",
            "  inflating: FLIS/src/client/__init__.py  \n",
            "   creating: FLIS/src/clustering/\n",
            "  inflating: FLIS/src/clustering/hierarchical_clustering.py  \n",
            "  inflating: FLIS/src/clustering/utils_clustering.py  \n",
            "  inflating: FLIS/src/clustering/__init__.py  \n",
            "   creating: FLIS/src/data/\n",
            "  inflating: FLIS/src/data/data.py   \n",
            "  inflating: FLIS/src/data/__init__.py  \n",
            "   creating: FLIS/src/fedavg/\n",
            "  inflating: FLIS/src/fedavg/fedavg.py  \n",
            "  inflating: FLIS/src/fedavg/__init__.py  \n",
            "   creating: FLIS/src/models/\n",
            "  inflating: FLIS/src/models/model.py  \n",
            "  inflating: FLIS/src/models/models.py  \n",
            "  inflating: FLIS/src/models/resnet.py  \n",
            "  inflating: FLIS/src/models/resnet9.py  \n",
            "  inflating: FLIS/src/models/resnetcifar.py  \n",
            "  inflating: FLIS/src/models/vgg.py  \n",
            "  inflating: FLIS/src/models/vggmodel.py  \n",
            "  inflating: FLIS/src/models/__init__.py  \n",
            "   creating: FLIS/src/utils/\n",
            "  inflating: FLIS/src/utils/datasets.py  \n",
            "  inflating: FLIS/src/utils/options_cluster.py  \n",
            "  inflating: FLIS/src/utils/utils.py  \n",
            "  inflating: FLIS/src/utils/__init__.py  \n",
            "/content/FLIS/scripts\n"
          ]
        }
      ],
      "source": [
        "! unzip FLIS.zip\n",
        "%cd /content/FLIS/scripts/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht5hhsm_a85w"
      },
      "source": [
        "#### MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCuNy7tv2Zx2",
        "outputId": "b8864f2b-b5f3-4318-862b-680a512e61b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../../data/MNIST/raw/train-images-idx3-ubyte.gz\n",
            "100%|██████████| 9912422/9912422 [00:01<00:00, 6048759.26it/s] \n",
            "Extracting ../../data/MNIST/raw/train-images-idx3-ubyte.gz to ../../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../../data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "100%|██████████| 28881/28881 [00:00<00:00, 159087.64it/s]\n",
            "Extracting ../../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "100%|██████████| 1648877/1648877 [00:01<00:00, 1515393.98it/s]\n",
            "Extracting ../../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "100%|██████████| 4542/4542 [00:00<00:00, 12492150.01it/s]\n",
            "Extracting ../../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../../data/MNIST/raw\n",
            "\n",
            "partition: noniid-labeldir\n",
            "Data statistics Train:\n",
            " {0: {0: 556, 1: 2527, 2: 378, 3: 1554, 4: 230, 5: 18, 6: 673, 7: 246}, 1: {0: 2, 1: 1737, 2: 617, 3: 682, 4: 913, 5: 1104, 6: 17, 7: 259, 8: 705}, 2: {0: 710, 1: 4, 2: 27, 3: 160, 4: 997, 5: 2791, 6: 2287}, 3: {0: 237, 1: 350, 2: 1276, 3: 65, 4: 1860, 5: 17, 6: 1076, 7: 283, 8: 2215}, 4: {0: 153, 1: 20, 2: 18, 3: 57, 4: 9, 5: 2, 6: 480, 7: 1250, 8: 80, 9: 637}, 5: {0: 165, 1: 2, 2: 576, 3: 385, 4: 190, 5: 635, 6: 840, 7: 1194, 8: 23, 9: 1579}, 6: {0: 1457, 1: 7, 2: 121, 3: 1599, 4: 223, 5: 31, 6: 79, 7: 20, 8: 4, 9: 1703}, 7: {0: 79, 1: 1831, 2: 179, 3: 1606, 4: 794, 5: 557, 6: 69, 7: 6, 8: 2039}, 8: {0: 471, 1: 14, 2: 1719, 3: 22, 4: 597, 5: 33, 6: 156, 7: 688, 8: 785, 9: 2030}, 9: {0: 2093, 1: 250, 2: 1047, 3: 1, 4: 29, 5: 233, 6: 241, 7: 2319}} \n",
            "\n",
            "Data statistics Test:\n",
            " {0: {0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958, 7: 1028}, 1: {0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958, 7: 1028, 8: 974}, 2: {0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958}, 3: {0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958, 7: 1028, 8: 974}, 4: {0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958, 7: 1028, 8: 974, 9: 1009}, 5: {0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958, 7: 1028, 8: 974, 9: 1009}, 6: {0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958, 7: 1028, 8: 974, 9: 1009}, 7: {0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958, 7: 1028, 8: 974}, 8: {0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958, 7: 1028, 8: 974, 9: 1009}, 9: {0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958, 7: 1028}} \n",
            "\n",
            "len train_ds_global: 60000\n",
            "len test_ds_global: 10000\n",
            "Shared data has label: 0, 250 samples\n",
            "Shared data has label: 1, 250 samples\n",
            "Shared data has label: 2, 250 samples\n",
            "Shared data has label: 3, 250 samples\n",
            "Shared data has label: 4, 250 samples\n",
            "Shared data has label: 5, 250 samples\n",
            "Shared data has label: 6, 250 samples\n",
            "Shared data has label: 7, 250 samples\n",
            "Shared data has label: 8, 250 samples\n",
            "Shared data has label: 9, 250 samples\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "MODEL: simple-cnn, Dataset: mnist\n",
            "SimpleCNNMNIST(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "conv1.weight torch.Size([6, 1, 5, 5])\n",
            "conv1.bias torch.Size([6])\n",
            "conv2.weight torch.Size([16, 6, 5, 5])\n",
            "conv2.bias torch.Size([16])\n",
            "fc1.weight torch.Size([120, 256])\n",
            "fc1.bias torch.Size([120])\n",
            "fc2.weight torch.Size([84, 120])\n",
            "fc2.bias torch.Size([84])\n",
            "fc3.weight torch.Size([10, 84])\n",
            "fc3.bias torch.Size([10])\n",
            "44426\n",
            "###### ROUND 1 ######\n",
            "Clients [2 5 8 1 3 6 7 0 9 4]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.121\n",
            "AVG Init Test Loss: 2.319, AVG Init Test Acc: 10.757\n",
            "AVG Final Test Loss: 0.503, AVG Final Test Acc: 88.092\n",
            "Round 1 Upload Cost: 1.777 MB\n",
            "Round 1 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 2, Count: 0, Labels: {0: 710, 1: 4, 2: 27, 3: 160, 4: 997, 5: 2791, 6: 2287}\n",
            "Client 5, Count: 0, Labels: {0: 165, 1: 2, 2: 576, 3: 385, 4: 190, 5: 635, 6: 840, 7: 1194, 8: 23, 9: 1579}\n",
            "Client 8, Count: 0, Labels: {0: 471, 1: 14, 2: 1719, 3: 22, 4: 597, 5: 33, 6: 156, 7: 688, 8: 785, 9: 2030}\n",
            "Client 1, Count: 0, Labels: {0: 2, 1: 1737, 2: 617, 3: 682, 4: 913, 5: 1104, 6: 17, 7: 259, 8: 705}\n",
            "Client 3, Count: 0, Labels: {0: 237, 1: 350, 2: 1276, 3: 65, 4: 1860, 5: 17, 6: 1076, 7: 283, 8: 2215}\n",
            "Client 6, Count: 0, Labels: {0: 1457, 1: 7, 2: 121, 3: 1599, 4: 223, 5: 31, 6: 79, 7: 20, 8: 4, 9: 1703}\n",
            "Client 7, Count: 0, Labels: {0: 79, 1: 1831, 2: 179, 3: 1606, 4: 794, 5: 557, 6: 69, 7: 6, 8: 2039}\n",
            "Client 0, Count: 0, Labels: {0: 556, 1: 2527, 2: 378, 3: 1554, 4: 230, 5: 18, 6: 673, 7: 246}\n",
            "Client 9, Count: 0, Labels: {0: 2093, 1: 250, 2: 1047, 3: 1, 4: 29, 5: 233, 6: 241, 7: 2319}\n",
            "Client 4, Count: 0, Labels: {0: 153, 1: 20, 2: 18, 3: 57, 4: 9, 5: 2, 6: 480, 7: 1250, 8: 80, 9: 637}\n",
            "\n",
            "Client 2, Correct_pred_per_label: {0: 250, 1: 217, 2: 223, 3: 229, 4: 249, 5: 250, 6: 245, 7: 0, 8: 0, 9: 0}\n",
            "Client 5, Correct_pred_per_label: {0: 241, 1: 40, 2: 244, 3: 246, 4: 228, 5: 246, 6: 246, 7: 247, 8: 164, 9: 243}\n",
            "Client 8, Correct_pred_per_label: {0: 245, 1: 217, 2: 245, 3: 181, 4: 242, 5: 168, 6: 231, 7: 247, 8: 241, 9: 248}\n",
            "Client 1, Correct_pred_per_label: {0: 50, 1: 249, 2: 248, 3: 247, 4: 247, 5: 248, 6: 149, 7: 236, 8: 249, 9: 0}\n",
            "Client 3, Correct_pred_per_label: {0: 244, 1: 242, 2: 248, 3: 223, 4: 247, 5: 175, 6: 245, 7: 247, 8: 248, 9: 0}\n",
            "Client 6, Correct_pred_per_label: {0: 249, 1: 230, 2: 239, 3: 245, 4: 241, 5: 186, 6: 236, 7: 190, 8: 6, 9: 250}\n",
            "Client 7, Correct_pred_per_label: {0: 240, 1: 249, 2: 221, 3: 247, 4: 249, 5: 248, 6: 218, 7: 148, 8: 249, 9: 0}\n",
            "Client 0, Correct_pred_per_label: {0: 247, 1: 247, 2: 248, 3: 249, 4: 246, 5: 181, 6: 244, 7: 237, 8: 0, 9: 0}\n",
            "Client 9, Correct_pred_per_label: {0: 248, 1: 247, 2: 230, 3: 0, 4: 232, 5: 245, 6: 237, 7: 250, 8: 0, 9: 0}\n",
            "Client 4, Correct_pred_per_label: {0: 239, 1: 237, 2: 208, 3: 229, 4: 59, 5: 0, 6: 244, 7: 247, 8: 237, 9: 244}\n",
            "\n",
            "Similarity Matrix: \n",
            " [[1.0000 0.6932 0.7148 0.7892 0.5992 0.6820 0.7568 0.7384 0.6992 0.7312]\n",
            " [0.6932 1.0000 0.6040 0.7840 0.5648 0.6436 0.6136 0.7964 0.6932 0.5916]\n",
            " [0.7148 0.6040 1.0000 0.6764 0.4716 0.6036 0.6756 0.7348 0.5948 0.6436]\n",
            " [0.7892 0.7840 0.6764 1.0000 0.7024 0.7260 0.6988 0.8428 0.8100 0.6920]\n",
            " [0.5992 0.5648 0.4716 0.7024 1.0000 0.6844 0.6736 0.6248 0.7760 0.5032]\n",
            " [0.6820 0.6436 0.6036 0.7260 0.6844 1.0000 0.7612 0.7008 0.8116 0.6040]\n",
            " [0.7568 0.6136 0.6756 0.6988 0.6736 0.7612 1.0000 0.6944 0.7860 0.6432]\n",
            " [0.7384 0.7964 0.7348 0.8428 0.6248 0.7008 0.6944 1.0000 0.7520 0.6376]\n",
            " [0.6992 0.6932 0.5948 0.8100 0.7760 0.8116 0.7860 0.7520 1.0000 0.6400]\n",
            " [0.7312 0.5916 0.6436 0.6920 0.5032 0.6040 0.6432 0.6376 0.6400 1.0000]]\n",
            "\n",
            "Cluster {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5], 6: [6], 7: [7], 8: [8], 9: [9]}\n",
            "###### ROUND 2 ######\n",
            "Clients [5 8 7 4 0 9 1 3 2 6]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.004\n",
            "AVG Init Test Loss: 0.503, AVG Init Test Acc: 88.092\n",
            "AVG Final Test Loss: 0.515, AVG Final Test Acc: 90.042\n",
            "Round 2 Upload Cost: 1.777 MB\n",
            "Round 2 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 5, Count: 0, Labels: {0: 165, 1: 2, 2: 576, 3: 385, 4: 190, 5: 635, 6: 840, 7: 1194, 8: 23, 9: 1579}\n",
            "Client 8, Count: 0, Labels: {0: 471, 1: 14, 2: 1719, 3: 22, 4: 597, 5: 33, 6: 156, 7: 688, 8: 785, 9: 2030}\n",
            "Client 7, Count: 0, Labels: {0: 79, 1: 1831, 2: 179, 3: 1606, 4: 794, 5: 557, 6: 69, 7: 6, 8: 2039}\n",
            "Client 4, Count: 0, Labels: {0: 153, 1: 20, 2: 18, 3: 57, 4: 9, 5: 2, 6: 480, 7: 1250, 8: 80, 9: 637}\n",
            "Client 0, Count: 0, Labels: {0: 556, 1: 2527, 2: 378, 3: 1554, 4: 230, 5: 18, 6: 673, 7: 246}\n",
            "Client 9, Count: 0, Labels: {0: 2093, 1: 250, 2: 1047, 3: 1, 4: 29, 5: 233, 6: 241, 7: 2319}\n",
            "Client 1, Count: 0, Labels: {0: 2, 1: 1737, 2: 617, 3: 682, 4: 913, 5: 1104, 6: 17, 7: 259, 8: 705}\n",
            "Client 3, Count: 0, Labels: {0: 237, 1: 350, 2: 1276, 3: 65, 4: 1860, 5: 17, 6: 1076, 7: 283, 8: 2215}\n",
            "Client 2, Count: 0, Labels: {0: 710, 1: 4, 2: 27, 3: 160, 4: 997, 5: 2791, 6: 2287}\n",
            "Client 6, Count: 0, Labels: {0: 1457, 1: 7, 2: 121, 3: 1599, 4: 223, 5: 31, 6: 79, 7: 20, 8: 4, 9: 1703}\n",
            "\n",
            "Client 5, Correct_pred_per_label: {0: 241, 1: 40, 2: 244, 3: 246, 4: 228, 5: 246, 6: 246, 7: 247, 8: 164, 9: 243}\n",
            "Client 8, Correct_pred_per_label: {0: 245, 1: 217, 2: 245, 3: 181, 4: 242, 5: 168, 6: 231, 7: 247, 8: 241, 9: 248}\n",
            "Client 7, Correct_pred_per_label: {0: 240, 1: 249, 2: 221, 3: 247, 4: 249, 5: 248, 6: 218, 7: 148, 8: 249, 9: 0}\n",
            "Client 4, Correct_pred_per_label: {0: 239, 1: 237, 2: 208, 3: 229, 4: 59, 5: 0, 6: 244, 7: 247, 8: 237, 9: 244}\n",
            "Client 0, Correct_pred_per_label: {0: 247, 1: 247, 2: 248, 3: 249, 4: 246, 5: 181, 6: 244, 7: 237, 8: 0, 9: 0}\n",
            "Client 9, Correct_pred_per_label: {0: 248, 1: 247, 2: 230, 3: 0, 4: 232, 5: 245, 6: 237, 7: 250, 8: 0, 9: 0}\n",
            "Client 1, Correct_pred_per_label: {0: 50, 1: 249, 2: 248, 3: 247, 4: 247, 5: 248, 6: 149, 7: 236, 8: 249, 9: 0}\n",
            "Client 3, Correct_pred_per_label: {0: 244, 1: 242, 2: 248, 3: 223, 4: 247, 5: 175, 6: 245, 7: 247, 8: 248, 9: 0}\n",
            "Client 2, Correct_pred_per_label: {0: 250, 1: 217, 2: 223, 3: 229, 4: 249, 5: 250, 6: 245, 7: 0, 8: 0, 9: 0}\n",
            "Client 6, Correct_pred_per_label: {0: 249, 1: 230, 2: 239, 3: 245, 4: 241, 5: 186, 6: 236, 7: 190, 8: 6, 9: 250}\n",
            "\n",
            "Similarity Matrix: \n",
            " [[1.0000 0.6932 0.7148 0.7892 0.5992 0.6820 0.7568 0.7384 0.6992 0.7312]\n",
            " [0.6932 1.0000 0.6040 0.7840 0.5648 0.6436 0.6136 0.7964 0.6932 0.5916]\n",
            " [0.7148 0.6040 1.0000 0.6764 0.4716 0.6036 0.6756 0.7348 0.5948 0.6436]\n",
            " [0.7892 0.7840 0.6764 1.0000 0.7024 0.7260 0.6988 0.8428 0.8100 0.6920]\n",
            " [0.5992 0.5648 0.4716 0.7024 1.0000 0.6844 0.6736 0.6248 0.7760 0.5032]\n",
            " [0.6820 0.6436 0.6036 0.7260 0.6844 1.0000 0.7612 0.7008 0.8116 0.6040]\n",
            " [0.7568 0.6136 0.6756 0.6988 0.6736 0.7612 1.0000 0.6944 0.7860 0.6432]\n",
            " [0.7384 0.7964 0.7348 0.8428 0.6248 0.7008 0.6944 1.0000 0.7520 0.6376]\n",
            " [0.6992 0.6932 0.5948 0.8100 0.7760 0.8116 0.7860 0.7520 1.0000 0.6400]\n",
            " [0.7312 0.5916 0.6436 0.6920 0.5032 0.6040 0.6432 0.6376 0.6400 1.0000]]\n",
            "\n",
            "Cluster {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5], 6: [6], 7: [7], 8: [8], 9: [9]}\n",
            "###### ROUND 3 ######\n",
            "Clients [5 4 9 3 1 6 2 0 7 8]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.001\n",
            "AVG Init Test Loss: 0.515, AVG Init Test Acc: 90.042\n",
            "AVG Final Test Loss: 0.559, AVG Final Test Acc: 90.191\n",
            "Round 3 Upload Cost: 1.777 MB\n",
            "Round 3 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 5, Count: 0, Labels: {0: 165, 1: 2, 2: 576, 3: 385, 4: 190, 5: 635, 6: 840, 7: 1194, 8: 23, 9: 1579}\n",
            "Client 4, Count: 0, Labels: {0: 153, 1: 20, 2: 18, 3: 57, 4: 9, 5: 2, 6: 480, 7: 1250, 8: 80, 9: 637}\n",
            "Client 9, Count: 0, Labels: {0: 2093, 1: 250, 2: 1047, 3: 1, 4: 29, 5: 233, 6: 241, 7: 2319}\n",
            "Client 3, Count: 0, Labels: {0: 237, 1: 350, 2: 1276, 3: 65, 4: 1860, 5: 17, 6: 1076, 7: 283, 8: 2215}\n",
            "Client 1, Count: 0, Labels: {0: 2, 1: 1737, 2: 617, 3: 682, 4: 913, 5: 1104, 6: 17, 7: 259, 8: 705}\n",
            "Client 6, Count: 0, Labels: {0: 1457, 1: 7, 2: 121, 3: 1599, 4: 223, 5: 31, 6: 79, 7: 20, 8: 4, 9: 1703}\n",
            "Client 2, Count: 0, Labels: {0: 710, 1: 4, 2: 27, 3: 160, 4: 997, 5: 2791, 6: 2287}\n",
            "Client 0, Count: 0, Labels: {0: 556, 1: 2527, 2: 378, 3: 1554, 4: 230, 5: 18, 6: 673, 7: 246}\n",
            "Client 7, Count: 0, Labels: {0: 79, 1: 1831, 2: 179, 3: 1606, 4: 794, 5: 557, 6: 69, 7: 6, 8: 2039}\n",
            "Client 8, Count: 0, Labels: {0: 471, 1: 14, 2: 1719, 3: 22, 4: 597, 5: 33, 6: 156, 7: 688, 8: 785, 9: 2030}\n",
            "\n",
            "Client 5, Correct_pred_per_label: {0: 241, 1: 40, 2: 244, 3: 246, 4: 228, 5: 246, 6: 246, 7: 247, 8: 164, 9: 243}\n",
            "Client 4, Correct_pred_per_label: {0: 239, 1: 237, 2: 208, 3: 229, 4: 59, 5: 0, 6: 244, 7: 247, 8: 237, 9: 244}\n",
            "Client 9, Correct_pred_per_label: {0: 248, 1: 247, 2: 230, 3: 0, 4: 232, 5: 245, 6: 237, 7: 250, 8: 0, 9: 0}\n",
            "Client 3, Correct_pred_per_label: {0: 244, 1: 242, 2: 248, 3: 223, 4: 247, 5: 175, 6: 245, 7: 247, 8: 248, 9: 0}\n",
            "Client 1, Correct_pred_per_label: {0: 50, 1: 249, 2: 248, 3: 247, 4: 247, 5: 248, 6: 149, 7: 236, 8: 249, 9: 0}\n",
            "Client 6, Correct_pred_per_label: {0: 249, 1: 230, 2: 239, 3: 245, 4: 241, 5: 186, 6: 236, 7: 190, 8: 6, 9: 250}\n",
            "Client 2, Correct_pred_per_label: {0: 250, 1: 217, 2: 223, 3: 229, 4: 249, 5: 250, 6: 245, 7: 0, 8: 0, 9: 0}\n",
            "Client 0, Correct_pred_per_label: {0: 247, 1: 247, 2: 248, 3: 249, 4: 246, 5: 181, 6: 244, 7: 237, 8: 0, 9: 0}\n",
            "Client 7, Correct_pred_per_label: {0: 240, 1: 249, 2: 221, 3: 247, 4: 249, 5: 248, 6: 218, 7: 148, 8: 249, 9: 0}\n",
            "Client 8, Correct_pred_per_label: {0: 245, 1: 217, 2: 245, 3: 181, 4: 242, 5: 168, 6: 231, 7: 247, 8: 241, 9: 248}\n",
            "\n",
            "Similarity Matrix: \n",
            " [[1.0000 0.6932 0.7148 0.7892 0.5992 0.6820 0.7568 0.7384 0.6992 0.7312]\n",
            " [0.6932 1.0000 0.6040 0.7840 0.5648 0.6436 0.6136 0.7964 0.6932 0.5916]\n",
            " [0.7148 0.6040 1.0000 0.6764 0.4716 0.6036 0.6756 0.7348 0.5948 0.6436]\n",
            " [0.7892 0.7840 0.6764 1.0000 0.7024 0.7260 0.6988 0.8428 0.8100 0.6920]\n",
            " [0.5992 0.5648 0.4716 0.7024 1.0000 0.6844 0.6736 0.6248 0.7760 0.5032]\n",
            " [0.6820 0.6436 0.6036 0.7260 0.6844 1.0000 0.7612 0.7008 0.8116 0.6040]\n",
            " [0.7568 0.6136 0.6756 0.6988 0.6736 0.7612 1.0000 0.6944 0.7860 0.6432]\n",
            " [0.7384 0.7964 0.7348 0.8428 0.6248 0.7008 0.6944 1.0000 0.7520 0.6376]\n",
            " [0.6992 0.6932 0.5948 0.8100 0.7760 0.8116 0.7860 0.7520 1.0000 0.6400]\n",
            " [0.7312 0.5916 0.6436 0.6920 0.5032 0.6040 0.6432 0.6376 0.6400 1.0000]]\n",
            "\n",
            "Cluster {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5], 6: [6], 7: [7], 8: [8], 9: [9]}\n",
            "###### ROUND 4 ######\n",
            "Clients [0 4 5 3 8 6 9 7 1 2]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.000\n",
            "AVG Init Test Loss: 0.559, AVG Init Test Acc: 90.191\n",
            "AVG Final Test Loss: 0.589, AVG Final Test Acc: 90.202\n",
            "Round 4 Upload Cost: 1.777 MB\n",
            "Round 4 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 0, Count: 0, Labels: {0: 556, 1: 2527, 2: 378, 3: 1554, 4: 230, 5: 18, 6: 673, 7: 246}\n",
            "Client 4, Count: 0, Labels: {0: 153, 1: 20, 2: 18, 3: 57, 4: 9, 5: 2, 6: 480, 7: 1250, 8: 80, 9: 637}\n",
            "Client 5, Count: 0, Labels: {0: 165, 1: 2, 2: 576, 3: 385, 4: 190, 5: 635, 6: 840, 7: 1194, 8: 23, 9: 1579}\n",
            "Client 3, Count: 0, Labels: {0: 237, 1: 350, 2: 1276, 3: 65, 4: 1860, 5: 17, 6: 1076, 7: 283, 8: 2215}\n",
            "Client 8, Count: 0, Labels: {0: 471, 1: 14, 2: 1719, 3: 22, 4: 597, 5: 33, 6: 156, 7: 688, 8: 785, 9: 2030}\n",
            "Client 6, Count: 0, Labels: {0: 1457, 1: 7, 2: 121, 3: 1599, 4: 223, 5: 31, 6: 79, 7: 20, 8: 4, 9: 1703}\n",
            "Client 9, Count: 0, Labels: {0: 2093, 1: 250, 2: 1047, 3: 1, 4: 29, 5: 233, 6: 241, 7: 2319}\n",
            "Client 7, Count: 0, Labels: {0: 79, 1: 1831, 2: 179, 3: 1606, 4: 794, 5: 557, 6: 69, 7: 6, 8: 2039}\n",
            "Client 1, Count: 0, Labels: {0: 2, 1: 1737, 2: 617, 3: 682, 4: 913, 5: 1104, 6: 17, 7: 259, 8: 705}\n",
            "Client 2, Count: 0, Labels: {0: 710, 1: 4, 2: 27, 3: 160, 4: 997, 5: 2791, 6: 2287}\n",
            "\n",
            "Client 0, Correct_pred_per_label: {0: 247, 1: 247, 2: 248, 3: 249, 4: 246, 5: 181, 6: 244, 7: 237, 8: 0, 9: 0}\n",
            "Client 4, Correct_pred_per_label: {0: 239, 1: 237, 2: 208, 3: 229, 4: 59, 5: 0, 6: 244, 7: 247, 8: 237, 9: 244}\n",
            "Client 5, Correct_pred_per_label: {0: 241, 1: 40, 2: 244, 3: 246, 4: 228, 5: 246, 6: 246, 7: 247, 8: 164, 9: 243}\n",
            "Client 3, Correct_pred_per_label: {0: 244, 1: 242, 2: 248, 3: 223, 4: 247, 5: 175, 6: 245, 7: 247, 8: 248, 9: 0}\n",
            "Client 8, Correct_pred_per_label: {0: 245, 1: 217, 2: 245, 3: 181, 4: 242, 5: 168, 6: 231, 7: 247, 8: 241, 9: 248}\n",
            "Client 6, Correct_pred_per_label: {0: 249, 1: 230, 2: 239, 3: 245, 4: 241, 5: 186, 6: 236, 7: 190, 8: 6, 9: 250}\n",
            "Client 9, Correct_pred_per_label: {0: 248, 1: 247, 2: 230, 3: 0, 4: 232, 5: 245, 6: 237, 7: 250, 8: 0, 9: 0}\n",
            "Client 7, Correct_pred_per_label: {0: 240, 1: 249, 2: 221, 3: 247, 4: 249, 5: 248, 6: 218, 7: 148, 8: 249, 9: 0}\n",
            "Client 1, Correct_pred_per_label: {0: 50, 1: 249, 2: 248, 3: 247, 4: 247, 5: 248, 6: 149, 7: 236, 8: 249, 9: 0}\n",
            "Client 2, Correct_pred_per_label: {0: 250, 1: 217, 2: 223, 3: 229, 4: 249, 5: 250, 6: 245, 7: 0, 8: 0, 9: 0}\n",
            "\n",
            "Similarity Matrix: \n",
            " [[1.0000 0.6932 0.7148 0.7892 0.5992 0.6820 0.7568 0.7384 0.6992 0.7312]\n",
            " [0.6932 1.0000 0.6040 0.7840 0.5648 0.6436 0.6136 0.7964 0.6932 0.5916]\n",
            " [0.7148 0.6040 1.0000 0.6764 0.4716 0.6036 0.6756 0.7348 0.5948 0.6436]\n",
            " [0.7892 0.7840 0.6764 1.0000 0.7024 0.7260 0.6988 0.8428 0.8100 0.6920]\n",
            " [0.5992 0.5648 0.4716 0.7024 1.0000 0.6844 0.6736 0.6248 0.7760 0.5032]\n",
            " [0.6820 0.6436 0.6036 0.7260 0.6844 1.0000 0.7612 0.7008 0.8116 0.6040]\n",
            " [0.7568 0.6136 0.6756 0.6988 0.6736 0.7612 1.0000 0.6944 0.7860 0.6432]\n",
            " [0.7384 0.7964 0.7348 0.8428 0.6248 0.7008 0.6944 1.0000 0.7520 0.6376]\n",
            " [0.6992 0.6932 0.5948 0.8100 0.7760 0.8116 0.7860 0.7520 1.0000 0.6400]\n",
            " [0.7312 0.5916 0.6436 0.6920 0.5032 0.6040 0.6432 0.6376 0.6400 1.0000]]\n",
            "\n",
            "Cluster {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5], 6: [6], 7: [7], 8: [8], 9: [9]}\n",
            "###### ROUND 5 ######\n",
            "Clients [7 1 8 9 6 3 4 5 2 0]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.000\n",
            "AVG Init Test Loss: 0.589, AVG Init Test Acc: 90.202\n",
            "AVG Final Test Loss: 0.609, AVG Final Test Acc: 90.209\n",
            "Round 5 Upload Cost: 1.777 MB\n",
            "Round 5 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 7, Count: 0, Labels: {0: 79, 1: 1831, 2: 179, 3: 1606, 4: 794, 5: 557, 6: 69, 7: 6, 8: 2039}\n",
            "Client 1, Count: 0, Labels: {0: 2, 1: 1737, 2: 617, 3: 682, 4: 913, 5: 1104, 6: 17, 7: 259, 8: 705}\n",
            "Client 8, Count: 0, Labels: {0: 471, 1: 14, 2: 1719, 3: 22, 4: 597, 5: 33, 6: 156, 7: 688, 8: 785, 9: 2030}\n",
            "Client 9, Count: 0, Labels: {0: 2093, 1: 250, 2: 1047, 3: 1, 4: 29, 5: 233, 6: 241, 7: 2319}\n",
            "Client 6, Count: 0, Labels: {0: 1457, 1: 7, 2: 121, 3: 1599, 4: 223, 5: 31, 6: 79, 7: 20, 8: 4, 9: 1703}\n",
            "Client 3, Count: 0, Labels: {0: 237, 1: 350, 2: 1276, 3: 65, 4: 1860, 5: 17, 6: 1076, 7: 283, 8: 2215}\n",
            "Client 4, Count: 0, Labels: {0: 153, 1: 20, 2: 18, 3: 57, 4: 9, 5: 2, 6: 480, 7: 1250, 8: 80, 9: 637}\n",
            "Client 5, Count: 0, Labels: {0: 165, 1: 2, 2: 576, 3: 385, 4: 190, 5: 635, 6: 840, 7: 1194, 8: 23, 9: 1579}\n",
            "Client 2, Count: 0, Labels: {0: 710, 1: 4, 2: 27, 3: 160, 4: 997, 5: 2791, 6: 2287}\n",
            "Client 0, Count: 0, Labels: {0: 556, 1: 2527, 2: 378, 3: 1554, 4: 230, 5: 18, 6: 673, 7: 246}\n",
            "\n",
            "Client 7, Correct_pred_per_label: {0: 240, 1: 249, 2: 221, 3: 247, 4: 249, 5: 248, 6: 218, 7: 148, 8: 249, 9: 0}\n",
            "Client 1, Correct_pred_per_label: {0: 50, 1: 249, 2: 248, 3: 247, 4: 247, 5: 248, 6: 149, 7: 236, 8: 249, 9: 0}\n",
            "Client 8, Correct_pred_per_label: {0: 245, 1: 217, 2: 245, 3: 181, 4: 242, 5: 168, 6: 231, 7: 247, 8: 241, 9: 248}\n",
            "Client 9, Correct_pred_per_label: {0: 248, 1: 247, 2: 230, 3: 0, 4: 232, 5: 245, 6: 237, 7: 250, 8: 0, 9: 0}\n",
            "Client 6, Correct_pred_per_label: {0: 249, 1: 230, 2: 239, 3: 245, 4: 241, 5: 186, 6: 236, 7: 190, 8: 6, 9: 250}\n",
            "Client 3, Correct_pred_per_label: {0: 244, 1: 242, 2: 248, 3: 223, 4: 247, 5: 175, 6: 245, 7: 247, 8: 248, 9: 0}\n",
            "Client 4, Correct_pred_per_label: {0: 239, 1: 237, 2: 208, 3: 229, 4: 59, 5: 0, 6: 244, 7: 247, 8: 237, 9: 244}\n",
            "Client 5, Correct_pred_per_label: {0: 241, 1: 40, 2: 244, 3: 246, 4: 228, 5: 246, 6: 246, 7: 247, 8: 164, 9: 243}\n",
            "Client 2, Correct_pred_per_label: {0: 250, 1: 217, 2: 223, 3: 229, 4: 249, 5: 250, 6: 245, 7: 0, 8: 0, 9: 0}\n",
            "Client 0, Correct_pred_per_label: {0: 247, 1: 247, 2: 248, 3: 249, 4: 246, 5: 181, 6: 244, 7: 237, 8: 0, 9: 0}\n",
            "\n",
            "Similarity Matrix: \n",
            " [[1.0000 0.6932 0.7148 0.7892 0.5992 0.6820 0.7568 0.7384 0.6992 0.7312]\n",
            " [0.6932 1.0000 0.6040 0.7840 0.5648 0.6436 0.6136 0.7964 0.6932 0.5916]\n",
            " [0.7148 0.6040 1.0000 0.6764 0.4716 0.6036 0.6756 0.7348 0.5948 0.6436]\n",
            " [0.7892 0.7840 0.6764 1.0000 0.7024 0.7260 0.6988 0.8428 0.8100 0.6920]\n",
            " [0.5992 0.5648 0.4716 0.7024 1.0000 0.6844 0.6736 0.6248 0.7760 0.5032]\n",
            " [0.6820 0.6436 0.6036 0.7260 0.6844 1.0000 0.7612 0.7008 0.8116 0.6040]\n",
            " [0.7568 0.6136 0.6756 0.6988 0.6736 0.7612 1.0000 0.6944 0.7860 0.6432]\n",
            " [0.7384 0.7964 0.7348 0.8428 0.6248 0.7008 0.6944 1.0000 0.7520 0.6376]\n",
            " [0.6992 0.6932 0.5948 0.8100 0.7760 0.8116 0.7860 0.7520 1.0000 0.6400]\n",
            " [0.7312 0.5916 0.6436 0.6920 0.5032 0.6040 0.6432 0.6376 0.6400 1.0000]]\n",
            "\n",
            "Cluster {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5], 6: [6], 7: [7], 8: [8], 9: [9]}\n",
            "###### ROUND 6 ######\n",
            "Clients [2 4 8 1 9 6 7 5 3 0]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.000\n",
            "AVG Init Test Loss: 0.609, AVG Init Test Acc: 90.209\n",
            "AVG Final Test Loss: 0.626, AVG Final Test Acc: 90.211\n",
            "Round 6 Upload Cost: 1.777 MB\n",
            "Round 6 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 2, Count: 0, Labels: {0: 710, 1: 4, 2: 27, 3: 160, 4: 997, 5: 2791, 6: 2287}\n",
            "Client 4, Count: 0, Labels: {0: 153, 1: 20, 2: 18, 3: 57, 4: 9, 5: 2, 6: 480, 7: 1250, 8: 80, 9: 637}\n",
            "Client 8, Count: 0, Labels: {0: 471, 1: 14, 2: 1719, 3: 22, 4: 597, 5: 33, 6: 156, 7: 688, 8: 785, 9: 2030}\n",
            "Client 1, Count: 0, Labels: {0: 2, 1: 1737, 2: 617, 3: 682, 4: 913, 5: 1104, 6: 17, 7: 259, 8: 705}\n",
            "Client 9, Count: 0, Labels: {0: 2093, 1: 250, 2: 1047, 3: 1, 4: 29, 5: 233, 6: 241, 7: 2319}\n",
            "Client 6, Count: 0, Labels: {0: 1457, 1: 7, 2: 121, 3: 1599, 4: 223, 5: 31, 6: 79, 7: 20, 8: 4, 9: 1703}\n",
            "Client 7, Count: 0, Labels: {0: 79, 1: 1831, 2: 179, 3: 1606, 4: 794, 5: 557, 6: 69, 7: 6, 8: 2039}\n",
            "Client 5, Count: 0, Labels: {0: 165, 1: 2, 2: 576, 3: 385, 4: 190, 5: 635, 6: 840, 7: 1194, 8: 23, 9: 1579}\n",
            "Client 3, Count: 0, Labels: {0: 237, 1: 350, 2: 1276, 3: 65, 4: 1860, 5: 17, 6: 1076, 7: 283, 8: 2215}\n",
            "Client 0, Count: 0, Labels: {0: 556, 1: 2527, 2: 378, 3: 1554, 4: 230, 5: 18, 6: 673, 7: 246}\n",
            "\n",
            "Client 2, Correct_pred_per_label: {0: 250, 1: 217, 2: 223, 3: 229, 4: 249, 5: 250, 6: 245, 7: 0, 8: 0, 9: 0}\n",
            "Client 4, Correct_pred_per_label: {0: 239, 1: 237, 2: 208, 3: 229, 4: 59, 5: 0, 6: 244, 7: 247, 8: 237, 9: 244}\n",
            "Client 8, Correct_pred_per_label: {0: 245, 1: 217, 2: 245, 3: 181, 4: 242, 5: 168, 6: 231, 7: 247, 8: 241, 9: 248}\n",
            "Client 1, Correct_pred_per_label: {0: 50, 1: 249, 2: 248, 3: 247, 4: 247, 5: 248, 6: 149, 7: 236, 8: 249, 9: 0}\n",
            "Client 9, Correct_pred_per_label: {0: 248, 1: 247, 2: 230, 3: 0, 4: 232, 5: 245, 6: 237, 7: 250, 8: 0, 9: 0}\n",
            "Client 6, Correct_pred_per_label: {0: 249, 1: 230, 2: 239, 3: 245, 4: 241, 5: 186, 6: 236, 7: 190, 8: 6, 9: 250}\n",
            "Client 7, Correct_pred_per_label: {0: 240, 1: 249, 2: 221, 3: 247, 4: 249, 5: 248, 6: 218, 7: 148, 8: 249, 9: 0}\n",
            "Client 5, Correct_pred_per_label: {0: 241, 1: 40, 2: 244, 3: 246, 4: 228, 5: 246, 6: 246, 7: 247, 8: 164, 9: 243}\n",
            "Client 3, Correct_pred_per_label: {0: 244, 1: 242, 2: 248, 3: 223, 4: 247, 5: 175, 6: 245, 7: 247, 8: 248, 9: 0}\n",
            "Client 0, Correct_pred_per_label: {0: 247, 1: 247, 2: 248, 3: 249, 4: 246, 5: 181, 6: 244, 7: 237, 8: 0, 9: 0}\n",
            "\n",
            "Similarity Matrix: \n",
            " [[1.0000 0.6932 0.7148 0.7892 0.5992 0.6820 0.7568 0.7384 0.6992 0.7312]\n",
            " [0.6932 1.0000 0.6040 0.7840 0.5648 0.6436 0.6136 0.7964 0.6932 0.5916]\n",
            " [0.7148 0.6040 1.0000 0.6764 0.4716 0.6036 0.6756 0.7348 0.5948 0.6436]\n",
            " [0.7892 0.7840 0.6764 1.0000 0.7024 0.7260 0.6988 0.8428 0.8100 0.6920]\n",
            " [0.5992 0.5648 0.4716 0.7024 1.0000 0.6844 0.6736 0.6248 0.7760 0.5032]\n",
            " [0.6820 0.6436 0.6036 0.7260 0.6844 1.0000 0.7612 0.7008 0.8116 0.6040]\n",
            " [0.7568 0.6136 0.6756 0.6988 0.6736 0.7612 1.0000 0.6944 0.7860 0.6432]\n",
            " [0.7384 0.7964 0.7348 0.8428 0.6248 0.7008 0.6944 1.0000 0.7520 0.6376]\n",
            " [0.6992 0.6932 0.5948 0.8100 0.7760 0.8116 0.7860 0.7520 1.0000 0.6400]\n",
            " [0.7312 0.5916 0.6436 0.6920 0.5032 0.6040 0.6432 0.6376 0.6400 1.0000]]\n",
            "\n",
            "Cluster {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5], 6: [6], 7: [7], 8: [8], 9: [9]}\n",
            "###### ROUND 7 ######\n",
            "Clients [5 9 7 2 1 0 4 3 6 8]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.000\n",
            "AVG Init Test Loss: 0.626, AVG Init Test Acc: 90.211\n",
            "AVG Final Test Loss: 0.638, AVG Final Test Acc: 90.242\n",
            "Round 7 Upload Cost: 1.777 MB\n",
            "Round 7 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 5, Count: 0, Labels: {0: 165, 1: 2, 2: 576, 3: 385, 4: 190, 5: 635, 6: 840, 7: 1194, 8: 23, 9: 1579}\n",
            "Client 9, Count: 0, Labels: {0: 2093, 1: 250, 2: 1047, 3: 1, 4: 29, 5: 233, 6: 241, 7: 2319}\n",
            "Client 7, Count: 0, Labels: {0: 79, 1: 1831, 2: 179, 3: 1606, 4: 794, 5: 557, 6: 69, 7: 6, 8: 2039}\n",
            "Client 2, Count: 0, Labels: {0: 710, 1: 4, 2: 27, 3: 160, 4: 997, 5: 2791, 6: 2287}\n",
            "Client 1, Count: 0, Labels: {0: 2, 1: 1737, 2: 617, 3: 682, 4: 913, 5: 1104, 6: 17, 7: 259, 8: 705}\n",
            "Client 0, Count: 0, Labels: {0: 556, 1: 2527, 2: 378, 3: 1554, 4: 230, 5: 18, 6: 673, 7: 246}\n",
            "Client 4, Count: 0, Labels: {0: 153, 1: 20, 2: 18, 3: 57, 4: 9, 5: 2, 6: 480, 7: 1250, 8: 80, 9: 637}\n",
            "Client 3, Count: 0, Labels: {0: 237, 1: 350, 2: 1276, 3: 65, 4: 1860, 5: 17, 6: 1076, 7: 283, 8: 2215}\n",
            "Client 6, Count: 0, Labels: {0: 1457, 1: 7, 2: 121, 3: 1599, 4: 223, 5: 31, 6: 79, 7: 20, 8: 4, 9: 1703}\n",
            "Client 8, Count: 0, Labels: {0: 471, 1: 14, 2: 1719, 3: 22, 4: 597, 5: 33, 6: 156, 7: 688, 8: 785, 9: 2030}\n",
            "\n",
            "Client 5, Correct_pred_per_label: {0: 241, 1: 40, 2: 244, 3: 246, 4: 228, 5: 246, 6: 246, 7: 247, 8: 164, 9: 243}\n",
            "Client 9, Correct_pred_per_label: {0: 248, 1: 247, 2: 230, 3: 0, 4: 232, 5: 245, 6: 237, 7: 250, 8: 0, 9: 0}\n",
            "Client 7, Correct_pred_per_label: {0: 240, 1: 249, 2: 221, 3: 247, 4: 249, 5: 248, 6: 218, 7: 148, 8: 249, 9: 0}\n",
            "Client 2, Correct_pred_per_label: {0: 250, 1: 217, 2: 223, 3: 229, 4: 249, 5: 250, 6: 245, 7: 0, 8: 0, 9: 0}\n",
            "Client 1, Correct_pred_per_label: {0: 50, 1: 249, 2: 248, 3: 247, 4: 247, 5: 248, 6: 149, 7: 236, 8: 249, 9: 0}\n",
            "Client 0, Correct_pred_per_label: {0: 247, 1: 247, 2: 248, 3: 249, 4: 246, 5: 181, 6: 244, 7: 237, 8: 0, 9: 0}\n",
            "Client 4, Correct_pred_per_label: {0: 239, 1: 237, 2: 208, 3: 229, 4: 59, 5: 0, 6: 244, 7: 247, 8: 237, 9: 244}\n",
            "Client 3, Correct_pred_per_label: {0: 244, 1: 242, 2: 248, 3: 223, 4: 247, 5: 175, 6: 245, 7: 247, 8: 248, 9: 0}\n",
            "Client 6, Correct_pred_per_label: {0: 249, 1: 230, 2: 239, 3: 245, 4: 241, 5: 186, 6: 236, 7: 190, 8: 6, 9: 250}\n",
            "Client 8, Correct_pred_per_label: {0: 245, 1: 217, 2: 245, 3: 181, 4: 242, 5: 168, 6: 231, 7: 247, 8: 241, 9: 248}\n",
            "\n",
            "Similarity Matrix: \n",
            " [[1.0000 0.6932 0.7148 0.7892 0.5992 0.6820 0.7568 0.7384 0.6992 0.7312]\n",
            " [0.6932 1.0000 0.6040 0.7840 0.5648 0.6436 0.6136 0.7964 0.6932 0.5916]\n",
            " [0.7148 0.6040 1.0000 0.6764 0.4716 0.6036 0.6756 0.7348 0.5948 0.6436]\n",
            " [0.7892 0.7840 0.6764 1.0000 0.7024 0.7260 0.6988 0.8428 0.8100 0.6920]\n",
            " [0.5992 0.5648 0.4716 0.7024 1.0000 0.6844 0.6736 0.6248 0.7760 0.5032]\n",
            " [0.6820 0.6436 0.6036 0.7260 0.6844 1.0000 0.7612 0.7008 0.8116 0.6040]\n",
            " [0.7568 0.6136 0.6756 0.6988 0.6736 0.7612 1.0000 0.6944 0.7860 0.6432]\n",
            " [0.7384 0.7964 0.7348 0.8428 0.6248 0.7008 0.6944 1.0000 0.7520 0.6376]\n",
            " [0.6992 0.6932 0.5948 0.8100 0.7760 0.8116 0.7860 0.7520 1.0000 0.6400]\n",
            " [0.7312 0.5916 0.6436 0.6920 0.5032 0.6040 0.6432 0.6376 0.6400 1.0000]]\n",
            "\n",
            "Cluster {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5], 6: [6], 7: [7], 8: [8], 9: [9]}\n",
            "###### ROUND 8 ######\n",
            "Clients [7 2 6 0 8 5 1 3 9 4]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.000\n",
            "AVG Init Test Loss: 0.638, AVG Init Test Acc: 90.242\n",
            "AVG Final Test Loss: 0.649, AVG Final Test Acc: 90.241\n",
            "Round 8 Upload Cost: 1.777 MB\n",
            "Round 8 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 7, Count: 0, Labels: {0: 79, 1: 1831, 2: 179, 3: 1606, 4: 794, 5: 557, 6: 69, 7: 6, 8: 2039}\n",
            "Client 2, Count: 0, Labels: {0: 710, 1: 4, 2: 27, 3: 160, 4: 997, 5: 2791, 6: 2287}\n",
            "Client 6, Count: 0, Labels: {0: 1457, 1: 7, 2: 121, 3: 1599, 4: 223, 5: 31, 6: 79, 7: 20, 8: 4, 9: 1703}\n",
            "Client 0, Count: 0, Labels: {0: 556, 1: 2527, 2: 378, 3: 1554, 4: 230, 5: 18, 6: 673, 7: 246}\n",
            "Client 8, Count: 0, Labels: {0: 471, 1: 14, 2: 1719, 3: 22, 4: 597, 5: 33, 6: 156, 7: 688, 8: 785, 9: 2030}\n",
            "Client 5, Count: 0, Labels: {0: 165, 1: 2, 2: 576, 3: 385, 4: 190, 5: 635, 6: 840, 7: 1194, 8: 23, 9: 1579}\n",
            "Client 1, Count: 0, Labels: {0: 2, 1: 1737, 2: 617, 3: 682, 4: 913, 5: 1104, 6: 17, 7: 259, 8: 705}\n",
            "Client 3, Count: 0, Labels: {0: 237, 1: 350, 2: 1276, 3: 65, 4: 1860, 5: 17, 6: 1076, 7: 283, 8: 2215}\n",
            "Client 9, Count: 0, Labels: {0: 2093, 1: 250, 2: 1047, 3: 1, 4: 29, 5: 233, 6: 241, 7: 2319}\n",
            "Client 4, Count: 0, Labels: {0: 153, 1: 20, 2: 18, 3: 57, 4: 9, 5: 2, 6: 480, 7: 1250, 8: 80, 9: 637}\n",
            "\n",
            "Client 7, Correct_pred_per_label: {0: 240, 1: 249, 2: 221, 3: 247, 4: 249, 5: 248, 6: 218, 7: 148, 8: 249, 9: 0}\n",
            "Client 2, Correct_pred_per_label: {0: 250, 1: 217, 2: 223, 3: 229, 4: 249, 5: 250, 6: 245, 7: 0, 8: 0, 9: 0}\n",
            "Client 6, Correct_pred_per_label: {0: 249, 1: 230, 2: 239, 3: 245, 4: 241, 5: 186, 6: 236, 7: 190, 8: 6, 9: 250}\n",
            "Client 0, Correct_pred_per_label: {0: 247, 1: 247, 2: 248, 3: 249, 4: 246, 5: 181, 6: 244, 7: 237, 8: 0, 9: 0}\n",
            "Client 8, Correct_pred_per_label: {0: 245, 1: 217, 2: 245, 3: 181, 4: 242, 5: 168, 6: 231, 7: 247, 8: 241, 9: 248}\n",
            "Client 5, Correct_pred_per_label: {0: 241, 1: 40, 2: 244, 3: 246, 4: 228, 5: 246, 6: 246, 7: 247, 8: 164, 9: 243}\n",
            "Client 1, Correct_pred_per_label: {0: 50, 1: 249, 2: 248, 3: 247, 4: 247, 5: 248, 6: 149, 7: 236, 8: 249, 9: 0}\n",
            "Client 3, Correct_pred_per_label: {0: 244, 1: 242, 2: 248, 3: 223, 4: 247, 5: 175, 6: 245, 7: 247, 8: 248, 9: 0}\n",
            "Client 9, Correct_pred_per_label: {0: 248, 1: 247, 2: 230, 3: 0, 4: 232, 5: 245, 6: 237, 7: 250, 8: 0, 9: 0}\n",
            "Client 4, Correct_pred_per_label: {0: 239, 1: 237, 2: 208, 3: 229, 4: 59, 5: 0, 6: 244, 7: 247, 8: 237, 9: 244}\n",
            "\n",
            "Similarity Matrix: \n",
            " [[1.0000 0.6932 0.7148 0.7892 0.5992 0.6820 0.7568 0.7384 0.6992 0.7312]\n",
            " [0.6932 1.0000 0.6040 0.7840 0.5648 0.6436 0.6136 0.7964 0.6932 0.5916]\n",
            " [0.7148 0.6040 1.0000 0.6764 0.4716 0.6036 0.6756 0.7348 0.5948 0.6436]\n",
            " [0.7892 0.7840 0.6764 1.0000 0.7024 0.7260 0.6988 0.8428 0.8100 0.6920]\n",
            " [0.5992 0.5648 0.4716 0.7024 1.0000 0.6844 0.6736 0.6248 0.7760 0.5032]\n",
            " [0.6820 0.6436 0.6036 0.7260 0.6844 1.0000 0.7612 0.7008 0.8116 0.6040]\n",
            " [0.7568 0.6136 0.6756 0.6988 0.6736 0.7612 1.0000 0.6944 0.7860 0.6432]\n",
            " [0.7384 0.7964 0.7348 0.8428 0.6248 0.7008 0.6944 1.0000 0.7520 0.6376]\n",
            " [0.6992 0.6932 0.5948 0.8100 0.7760 0.8116 0.7860 0.7520 1.0000 0.6400]\n",
            " [0.7312 0.5916 0.6436 0.6920 0.5032 0.6040 0.6432 0.6376 0.6400 1.0000]]\n",
            "\n",
            "Cluster {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5], 6: [6], 7: [7], 8: [8], 9: [9]}\n",
            "###### ROUND 9 ######\n",
            "Clients [5 2 4 0 6 9 3 8 1 7]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.000\n",
            "AVG Init Test Loss: 0.649, AVG Init Test Acc: 90.241\n",
            "AVG Final Test Loss: 0.659, AVG Final Test Acc: 90.243\n",
            "Round 9 Upload Cost: 1.777 MB\n",
            "Round 9 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 5, Count: 0, Labels: {0: 165, 1: 2, 2: 576, 3: 385, 4: 190, 5: 635, 6: 840, 7: 1194, 8: 23, 9: 1579}\n",
            "Client 2, Count: 0, Labels: {0: 710, 1: 4, 2: 27, 3: 160, 4: 997, 5: 2791, 6: 2287}\n",
            "Client 4, Count: 0, Labels: {0: 153, 1: 20, 2: 18, 3: 57, 4: 9, 5: 2, 6: 480, 7: 1250, 8: 80, 9: 637}\n",
            "Client 0, Count: 0, Labels: {0: 556, 1: 2527, 2: 378, 3: 1554, 4: 230, 5: 18, 6: 673, 7: 246}\n",
            "Client 6, Count: 0, Labels: {0: 1457, 1: 7, 2: 121, 3: 1599, 4: 223, 5: 31, 6: 79, 7: 20, 8: 4, 9: 1703}\n",
            "Client 9, Count: 0, Labels: {0: 2093, 1: 250, 2: 1047, 3: 1, 4: 29, 5: 233, 6: 241, 7: 2319}\n",
            "Client 3, Count: 0, Labels: {0: 237, 1: 350, 2: 1276, 3: 65, 4: 1860, 5: 17, 6: 1076, 7: 283, 8: 2215}\n",
            "Client 8, Count: 0, Labels: {0: 471, 1: 14, 2: 1719, 3: 22, 4: 597, 5: 33, 6: 156, 7: 688, 8: 785, 9: 2030}\n",
            "Client 1, Count: 0, Labels: {0: 2, 1: 1737, 2: 617, 3: 682, 4: 913, 5: 1104, 6: 17, 7: 259, 8: 705}\n",
            "Client 7, Count: 0, Labels: {0: 79, 1: 1831, 2: 179, 3: 1606, 4: 794, 5: 557, 6: 69, 7: 6, 8: 2039}\n",
            "\n",
            "Client 5, Correct_pred_per_label: {0: 241, 1: 40, 2: 244, 3: 246, 4: 228, 5: 246, 6: 246, 7: 247, 8: 164, 9: 243}\n",
            "Client 2, Correct_pred_per_label: {0: 250, 1: 217, 2: 223, 3: 229, 4: 249, 5: 250, 6: 245, 7: 0, 8: 0, 9: 0}\n",
            "Client 4, Correct_pred_per_label: {0: 239, 1: 237, 2: 208, 3: 229, 4: 59, 5: 0, 6: 244, 7: 247, 8: 237, 9: 244}\n",
            "Client 0, Correct_pred_per_label: {0: 247, 1: 247, 2: 248, 3: 249, 4: 246, 5: 181, 6: 244, 7: 237, 8: 0, 9: 0}\n",
            "Client 6, Correct_pred_per_label: {0: 249, 1: 230, 2: 239, 3: 245, 4: 241, 5: 186, 6: 236, 7: 190, 8: 6, 9: 250}\n",
            "Client 9, Correct_pred_per_label: {0: 248, 1: 247, 2: 230, 3: 0, 4: 232, 5: 245, 6: 237, 7: 250, 8: 0, 9: 0}\n",
            "Client 3, Correct_pred_per_label: {0: 244, 1: 242, 2: 248, 3: 223, 4: 247, 5: 175, 6: 245, 7: 247, 8: 248, 9: 0}\n",
            "Client 8, Correct_pred_per_label: {0: 245, 1: 217, 2: 245, 3: 181, 4: 242, 5: 168, 6: 231, 7: 247, 8: 241, 9: 248}\n",
            "Client 1, Correct_pred_per_label: {0: 50, 1: 249, 2: 248, 3: 247, 4: 247, 5: 248, 6: 149, 7: 236, 8: 249, 9: 0}\n",
            "Client 7, Correct_pred_per_label: {0: 240, 1: 249, 2: 221, 3: 247, 4: 249, 5: 248, 6: 218, 7: 148, 8: 249, 9: 0}\n",
            "\n",
            "Similarity Matrix: \n",
            " [[1.0000 0.6932 0.7148 0.7892 0.5992 0.6820 0.7568 0.7384 0.6992 0.7312]\n",
            " [0.6932 1.0000 0.6040 0.7840 0.5648 0.6436 0.6136 0.7964 0.6932 0.5916]\n",
            " [0.7148 0.6040 1.0000 0.6764 0.4716 0.6036 0.6756 0.7348 0.5948 0.6436]\n",
            " [0.7892 0.7840 0.6764 1.0000 0.7024 0.7260 0.6988 0.8428 0.8100 0.6920]\n",
            " [0.5992 0.5648 0.4716 0.7024 1.0000 0.6844 0.6736 0.6248 0.7760 0.5032]\n",
            " [0.6820 0.6436 0.6036 0.7260 0.6844 1.0000 0.7612 0.7008 0.8116 0.6040]\n",
            " [0.7568 0.6136 0.6756 0.6988 0.6736 0.7612 1.0000 0.6944 0.7860 0.6432]\n",
            " [0.7384 0.7964 0.7348 0.8428 0.6248 0.7008 0.6944 1.0000 0.7520 0.6376]\n",
            " [0.6992 0.6932 0.5948 0.8100 0.7760 0.8116 0.7860 0.7520 1.0000 0.6400]\n",
            " [0.7312 0.5916 0.6436 0.6920 0.5032 0.6040 0.6432 0.6376 0.6400 1.0000]]\n",
            "\n",
            "Cluster {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5], 6: [6], 7: [7], 8: [8], 9: [9]}\n",
            "###### ROUND 10 ######\n",
            "Clients [9 2 0 5 1 7 8 6 4 3]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.000\n",
            "AVG Init Test Loss: 0.659, AVG Init Test Acc: 90.243\n",
            "AVG Final Test Loss: 0.667, AVG Final Test Acc: 90.240\n",
            "Round 10 Upload Cost: 1.777 MB\n",
            "Round 10 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 9, Count: 0, Labels: {0: 2093, 1: 250, 2: 1047, 3: 1, 4: 29, 5: 233, 6: 241, 7: 2319}\n",
            "Client 2, Count: 0, Labels: {0: 710, 1: 4, 2: 27, 3: 160, 4: 997, 5: 2791, 6: 2287}\n",
            "Client 0, Count: 0, Labels: {0: 556, 1: 2527, 2: 378, 3: 1554, 4: 230, 5: 18, 6: 673, 7: 246}\n",
            "Client 5, Count: 0, Labels: {0: 165, 1: 2, 2: 576, 3: 385, 4: 190, 5: 635, 6: 840, 7: 1194, 8: 23, 9: 1579}\n",
            "Client 1, Count: 0, Labels: {0: 2, 1: 1737, 2: 617, 3: 682, 4: 913, 5: 1104, 6: 17, 7: 259, 8: 705}\n",
            "Client 7, Count: 0, Labels: {0: 79, 1: 1831, 2: 179, 3: 1606, 4: 794, 5: 557, 6: 69, 7: 6, 8: 2039}\n",
            "Client 8, Count: 0, Labels: {0: 471, 1: 14, 2: 1719, 3: 22, 4: 597, 5: 33, 6: 156, 7: 688, 8: 785, 9: 2030}\n",
            "Client 6, Count: 0, Labels: {0: 1457, 1: 7, 2: 121, 3: 1599, 4: 223, 5: 31, 6: 79, 7: 20, 8: 4, 9: 1703}\n",
            "Client 4, Count: 0, Labels: {0: 153, 1: 20, 2: 18, 3: 57, 4: 9, 5: 2, 6: 480, 7: 1250, 8: 80, 9: 637}\n",
            "Client 3, Count: 0, Labels: {0: 237, 1: 350, 2: 1276, 3: 65, 4: 1860, 5: 17, 6: 1076, 7: 283, 8: 2215}\n",
            "\n",
            "Client 9, Correct_pred_per_label: {0: 248, 1: 247, 2: 230, 3: 0, 4: 232, 5: 245, 6: 237, 7: 250, 8: 0, 9: 0}\n",
            "Client 2, Correct_pred_per_label: {0: 250, 1: 217, 2: 223, 3: 229, 4: 249, 5: 250, 6: 245, 7: 0, 8: 0, 9: 0}\n",
            "Client 0, Correct_pred_per_label: {0: 247, 1: 247, 2: 248, 3: 249, 4: 246, 5: 181, 6: 244, 7: 237, 8: 0, 9: 0}\n",
            "Client 5, Correct_pred_per_label: {0: 241, 1: 40, 2: 244, 3: 246, 4: 228, 5: 246, 6: 246, 7: 247, 8: 164, 9: 243}\n",
            "Client 1, Correct_pred_per_label: {0: 50, 1: 249, 2: 248, 3: 247, 4: 247, 5: 248, 6: 149, 7: 236, 8: 249, 9: 0}\n",
            "Client 7, Correct_pred_per_label: {0: 240, 1: 249, 2: 221, 3: 247, 4: 249, 5: 248, 6: 218, 7: 148, 8: 249, 9: 0}\n",
            "Client 8, Correct_pred_per_label: {0: 245, 1: 217, 2: 245, 3: 181, 4: 242, 5: 168, 6: 231, 7: 247, 8: 241, 9: 248}\n",
            "Client 6, Correct_pred_per_label: {0: 249, 1: 230, 2: 239, 3: 245, 4: 241, 5: 186, 6: 236, 7: 190, 8: 6, 9: 250}\n",
            "Client 4, Correct_pred_per_label: {0: 239, 1: 237, 2: 208, 3: 229, 4: 59, 5: 0, 6: 244, 7: 247, 8: 237, 9: 244}\n",
            "Client 3, Correct_pred_per_label: {0: 244, 1: 242, 2: 248, 3: 223, 4: 247, 5: 175, 6: 245, 7: 247, 8: 248, 9: 0}\n",
            "\n",
            "Similarity Matrix: \n",
            " [[1.0000 0.6932 0.7148 0.7892 0.5992 0.6820 0.7568 0.7384 0.6992 0.7312]\n",
            " [0.6932 1.0000 0.6040 0.7840 0.5648 0.6436 0.6136 0.7964 0.6932 0.5916]\n",
            " [0.7148 0.6040 1.0000 0.6764 0.4716 0.6036 0.6756 0.7348 0.5948 0.6436]\n",
            " [0.7892 0.7840 0.6764 1.0000 0.7024 0.7260 0.6988 0.8428 0.8100 0.6920]\n",
            " [0.5992 0.5648 0.4716 0.7024 1.0000 0.6844 0.6736 0.6248 0.7760 0.5032]\n",
            " [0.6820 0.6436 0.6036 0.7260 0.6844 1.0000 0.7612 0.7008 0.8116 0.6040]\n",
            " [0.7568 0.6136 0.6756 0.6988 0.6736 0.7612 1.0000 0.6944 0.7860 0.6432]\n",
            " [0.7384 0.7964 0.7348 0.8428 0.6248 0.7008 0.6944 1.0000 0.7520 0.6376]\n",
            " [0.6992 0.6932 0.5948 0.8100 0.7760 0.8116 0.7860 0.7520 1.0000 0.6400]\n",
            " [0.7312 0.5916 0.6436 0.6920 0.5032 0.6040 0.6432 0.6376 0.6400 1.0000]]\n",
            "\n",
            "Cluster {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5], 6: [6], 7: [7], 8: [8], 9: [9]}\n",
            "Training Loss: 0.000, Training Accuracy: 100.000\n",
            "Testing Loss: 0.667, Testing Accuracy: 90.240\n"
          ]
        }
      ],
      "source": [
        "! bash flis_hc.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uM9TjyMzAgHE",
        "outputId": "f1a681c5-566f-464b-817a-cbcf8dfdcac5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "partition: noniid-labeldir\n",
            "Data statistics Train:\n",
            " {0: {0: 433, 1: 402, 2: 694, 3: 641, 4: 1746, 5: 1340, 6: 259, 7: 301, 8: 846}, 1: {0: 11, 1: 906, 2: 1645, 3: 94, 4: 54, 5: 213, 6: 838, 7: 3, 8: 964, 9: 183}, 2: {0: 103, 1: 1494, 2: 382, 3: 2, 4: 13, 5: 404, 6: 464, 7: 899, 8: 20, 9: 776}, 3: {0: 1250, 1: 125, 2: 1374, 3: 22, 4: 928, 5: 111, 6: 331, 7: 1602, 8: 468}, 4: {0: 758, 1: 3, 2: 298, 4: 510, 5: 86, 6: 568, 7: 114, 8: 3176, 9: 213}, 5: {0: 218, 1: 1354, 2: 183, 3: 871, 4: 387, 5: 1466, 6: 44, 7: 14, 8: 128, 9: 2061}, 6: {0: 2271, 1: 42, 2: 1022, 3: 158, 4: 1480, 5: 21, 6: 283, 7: 2024}, 7: {0: 114, 1: 92, 2: 274, 3: 2680, 4: 3, 5: 1117, 6: 2033}, 8: {0: 558, 1: 1832, 2: 77, 3: 396, 4: 39, 5: 42, 6: 1007, 7: 746, 8: 237, 9: 1275}, 9: {0: 207, 1: 492, 2: 9, 3: 1267, 4: 682, 5: 621, 6: 91, 7: 562, 8: 12, 9: 1441}} \n",
            "\n",
            "Data statistics Test:\n",
            " {0: {0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958, 7: 1028, 8: 974}, 1: {0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958, 7: 1028, 8: 974, 9: 1009}, 2: {0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958, 7: 1028, 8: 974, 9: 1009}, 3: {0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958, 7: 1028, 8: 974}, 4: {0: 980, 1: 1135, 2: 1032, 4: 982, 5: 892, 6: 958, 7: 1028, 8: 974, 9: 1009}, 5: {0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958, 7: 1028, 8: 974, 9: 1009}, 6: {0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958, 7: 1028}, 7: {0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958}, 8: {0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958, 7: 1028, 8: 974, 9: 1009}, 9: {0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958, 7: 1028, 8: 974, 9: 1009}} \n",
            "\n",
            "len train_ds_global: 60000\n",
            "len test_ds_global: 10000\n",
            "Shared data has label: 0, 250 samples\n",
            "Shared data has label: 1, 250 samples\n",
            "Shared data has label: 2, 250 samples\n",
            "Shared data has label: 3, 250 samples\n",
            "Shared data has label: 4, 250 samples\n",
            "Shared data has label: 5, 250 samples\n",
            "Shared data has label: 6, 250 samples\n",
            "Shared data has label: 7, 250 samples\n",
            "Shared data has label: 8, 250 samples\n",
            "Shared data has label: 9, 250 samples\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "MODEL: simple-cnn, Dataset: mnist\n",
            "SimpleCNNMNIST(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "conv1.weight torch.Size([6, 1, 5, 5])\n",
            "conv1.bias torch.Size([6])\n",
            "conv2.weight torch.Size([16, 6, 5, 5])\n",
            "conv2.bias torch.Size([16])\n",
            "fc1.weight torch.Size([120, 256])\n",
            "fc1.bias torch.Size([120])\n",
            "fc2.weight torch.Size([84, 120])\n",
            "fc2.bias torch.Size([84])\n",
            "fc3.weight torch.Size([10, 84])\n",
            "fc3.bias torch.Size([10])\n",
            "44426\n",
            "###### ROUND 1 ######\n",
            "Clients [3 6 2 7 9 5 4 1 0 8]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.131\n",
            "AVG Init Test Loss: 2.308, AVG Init Test Acc: 13.164\n",
            "AVG Final Test Loss: 0.459, AVG Final Test Acc: 88.073\n",
            "Round 1 Upload Cost: 1.777 MB\n",
            "Round 1 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 3, Count: 0, Labels: {0: 1250, 1: 125, 2: 1374, 3: 22, 4: 928, 5: 111, 6: 331, 7: 1602, 8: 468}\n",
            "Client 6, Count: 0, Labels: {0: 2271, 1: 42, 2: 1022, 3: 158, 4: 1480, 5: 21, 6: 283, 7: 2024}\n",
            "Client 2, Count: 0, Labels: {0: 103, 1: 1494, 2: 382, 3: 2, 4: 13, 5: 404, 6: 464, 7: 899, 8: 20, 9: 776}\n",
            "Client 7, Count: 0, Labels: {0: 114, 1: 92, 2: 274, 3: 2680, 4: 3, 5: 1117, 6: 2033}\n",
            "Client 9, Count: 0, Labels: {0: 207, 1: 492, 2: 9, 3: 1267, 4: 682, 5: 621, 6: 91, 7: 562, 8: 12, 9: 1441}\n",
            "Client 5, Count: 0, Labels: {0: 218, 1: 1354, 2: 183, 3: 871, 4: 387, 5: 1466, 6: 44, 7: 14, 8: 128, 9: 2061}\n",
            "Client 4, Count: 0, Labels: {0: 758, 1: 3, 2: 298, 4: 510, 5: 86, 6: 568, 7: 114, 8: 3176, 9: 213}\n",
            "Client 1, Count: 0, Labels: {0: 11, 1: 906, 2: 1645, 3: 94, 4: 54, 5: 213, 6: 838, 7: 3, 8: 964, 9: 183}\n",
            "Client 0, Count: 0, Labels: {0: 433, 1: 402, 2: 694, 3: 641, 4: 1746, 5: 1340, 6: 259, 7: 301, 8: 846}\n",
            "Client 8, Count: 0, Labels: {0: 558, 1: 1832, 2: 77, 3: 396, 4: 39, 5: 42, 6: 1007, 7: 746, 8: 237, 9: 1275}\n",
            "\n",
            "Client 3, Correct_pred_per_label: {0: 249, 1: 246, 2: 247, 3: 185, 4: 247, 5: 237, 6: 239, 7: 248, 8: 238, 9: 0}\n",
            "Client 6, Correct_pred_per_label: {0: 249, 1: 233, 2: 246, 3: 230, 4: 250, 5: 163, 6: 240, 7: 249, 8: 0, 9: 0}\n",
            "Client 2, Correct_pred_per_label: {0: 234, 1: 249, 2: 243, 3: 1, 4: 144, 5: 242, 6: 245, 7: 247, 8: 169, 9: 245}\n",
            "Client 7, Correct_pred_per_label: {0: 233, 1: 242, 2: 236, 3: 248, 4: 42, 5: 246, 6: 247, 7: 0, 8: 0, 9: 0}\n",
            "Client 9, Correct_pred_per_label: {0: 247, 1: 250, 2: 111, 3: 250, 4: 245, 5: 241, 6: 216, 7: 245, 8: 40, 9: 248}\n",
            "Client 5, Correct_pred_per_label: {0: 236, 1: 250, 2: 236, 3: 248, 4: 247, 5: 248, 6: 197, 7: 131, 8: 190, 9: 248}\n",
            "Client 4, Correct_pred_per_label: {0: 245, 1: 18, 2: 229, 3: 0, 4: 243, 5: 221, 6: 228, 7: 231, 8: 250, 9: 227}\n",
            "Client 1, Correct_pred_per_label: {0: 174, 1: 245, 2: 245, 3: 238, 4: 228, 5: 234, 6: 245, 7: 38, 8: 249, 9: 207}\n",
            "Client 0, Correct_pred_per_label: {0: 243, 1: 249, 2: 246, 3: 245, 4: 249, 5: 248, 6: 209, 7: 238, 8: 237, 9: 0}\n",
            "Client 8, Correct_pred_per_label: {0: 247, 1: 249, 2: 222, 3: 245, 4: 191, 5: 184, 6: 241, 7: 246, 8: 223, 9: 244}\n",
            "Clusters Glob Acc: [tensor(82.9300), tensor(82.9300), tensor(82.9300), tensor(81.4700), tensor(82.9300), tensor(82.9300), tensor(84.4400), tensor(82.9300), tensor(82.9300), tensor(82.9300)]\n",
            "###### ROUND 2 ######\n",
            "Clients [8 3 0 2 6 5 7 4 9 1]\n",
            "Client 8, Select Cluster: 6\n",
            "acc clusters: [tensor(82.9300), tensor(82.9300), tensor(82.9300), tensor(81.4700), tensor(82.9300), tensor(82.9300), tensor(84.4400), tensor(82.9300), tensor(82.9300), tensor(82.9300)]\n",
            "Client 3, Select Cluster: 6\n",
            "acc clusters: [tensor(91.5360), tensor(91.5360), tensor(91.5360), tensor(90.2124), tensor(91.5360), tensor(91.5360), tensor(93.1932), tensor(91.5360), tensor(91.5360), tensor(91.5360)]\n",
            "Client 0, Select Cluster: 6\n",
            "acc clusters: [tensor(91.5360), tensor(91.5360), tensor(91.5360), tensor(90.2124), tensor(91.5360), tensor(91.5360), tensor(93.1932), tensor(91.5360), tensor(91.5360), tensor(91.5360)]\n",
            "Client 2, Select Cluster: 6\n",
            "acc clusters: [tensor(82.9300), tensor(82.9300), tensor(82.9300), tensor(81.4700), tensor(82.9300), tensor(82.9300), tensor(84.4400), tensor(82.9300), tensor(82.9300), tensor(82.9300)]\n",
            "Client 6, Select Cluster: 3\n",
            "acc clusters: [tensor(97.1061), tensor(97.1061), tensor(97.1061), tensor(97.6675), tensor(97.1061), tensor(97.1061), tensor(96.6571), tensor(97.1061), tensor(97.1061), tensor(97.1061)]\n",
            "Client 5, Select Cluster: 6\n",
            "acc clusters: [tensor(82.9300), tensor(82.9300), tensor(82.9300), tensor(81.4700), tensor(82.9300), tensor(82.9300), tensor(84.4400), tensor(82.9300), tensor(82.9300), tensor(82.9300)]\n",
            "Client 7, Select Cluster: 3\n",
            "acc clusters: [tensor(97.1097), tensor(97.1097), tensor(97.1097), tensor(97.7822), tensor(97.1097), tensor(97.1097), tensor(96.4373), tensor(97.1097), tensor(97.1097), tensor(97.1097)]\n",
            "Client 4, Select Cluster: 6\n",
            "acc clusters: [tensor(82.0134), tensor(82.0134), tensor(82.0134), tensor(79.7553), tensor(82.0134), tensor(82.0134), tensor(84.1157), tensor(82.0134), tensor(82.0134), tensor(82.0134)]\n",
            "Client 9, Select Cluster: 6\n",
            "acc clusters: [tensor(82.9300), tensor(82.9300), tensor(82.9300), tensor(81.4700), tensor(82.9300), tensor(82.9300), tensor(84.4400), tensor(82.9300), tensor(82.9300), tensor(82.9300)]\n",
            "Client 1, Select Cluster: 6\n",
            "acc clusters: [tensor(82.9300), tensor(82.9300), tensor(82.9300), tensor(81.4700), tensor(82.9300), tensor(82.9300), tensor(84.4400), tensor(82.9300), tensor(82.9300), tensor(82.9300)]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.030\n",
            "AVG Init Test Loss: 0.300, AVG Init Test Acc: 88.815\n",
            "AVG Final Test Loss: 0.246, AVG Final Test Acc: 93.831\n",
            "Round 2 Upload Cost: 1.777 MB\n",
            "Round 2 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 8, Count: 0, Labels: {0: 558, 1: 1832, 2: 77, 3: 396, 4: 39, 5: 42, 6: 1007, 7: 746, 8: 237, 9: 1275}\n",
            "Client 3, Count: 0, Labels: {0: 1250, 1: 125, 2: 1374, 3: 22, 4: 928, 5: 111, 6: 331, 7: 1602, 8: 468}\n",
            "Client 0, Count: 0, Labels: {0: 433, 1: 402, 2: 694, 3: 641, 4: 1746, 5: 1340, 6: 259, 7: 301, 8: 846}\n",
            "Client 2, Count: 0, Labels: {0: 103, 1: 1494, 2: 382, 3: 2, 4: 13, 5: 404, 6: 464, 7: 899, 8: 20, 9: 776}\n",
            "Client 6, Count: 0, Labels: {0: 2271, 1: 42, 2: 1022, 3: 158, 4: 1480, 5: 21, 6: 283, 7: 2024}\n",
            "Client 5, Count: 0, Labels: {0: 218, 1: 1354, 2: 183, 3: 871, 4: 387, 5: 1466, 6: 44, 7: 14, 8: 128, 9: 2061}\n",
            "Client 7, Count: 0, Labels: {0: 114, 1: 92, 2: 274, 3: 2680, 4: 3, 5: 1117, 6: 2033}\n",
            "Client 4, Count: 0, Labels: {0: 758, 1: 3, 2: 298, 4: 510, 5: 86, 6: 568, 7: 114, 8: 3176, 9: 213}\n",
            "Client 9, Count: 0, Labels: {0: 207, 1: 492, 2: 9, 3: 1267, 4: 682, 5: 621, 6: 91, 7: 562, 8: 12, 9: 1441}\n",
            "Client 1, Count: 0, Labels: {0: 11, 1: 906, 2: 1645, 3: 94, 4: 54, 5: 213, 6: 838, 7: 3, 8: 964, 9: 183}\n",
            "\n",
            "Client 8, Correct_pred_per_label: {0: 247, 1: 249, 2: 226, 3: 247, 4: 212, 5: 203, 6: 243, 7: 248, 8: 232, 9: 244}\n",
            "Client 3, Correct_pred_per_label: {0: 248, 1: 242, 2: 248, 3: 201, 4: 247, 5: 233, 6: 239, 7: 250, 8: 239, 9: 0}\n",
            "Client 0, Correct_pred_per_label: {0: 248, 1: 247, 2: 248, 3: 248, 4: 250, 5: 249, 6: 239, 7: 245, 8: 240, 9: 0}\n",
            "Client 2, Correct_pred_per_label: {0: 233, 1: 248, 2: 245, 3: 92, 4: 202, 5: 248, 6: 247, 7: 247, 8: 192, 9: 244}\n",
            "Client 6, Correct_pred_per_label: {0: 246, 1: 242, 2: 243, 3: 239, 4: 250, 5: 215, 6: 242, 7: 250, 8: 0, 9: 0}\n",
            "Client 5, Correct_pred_per_label: {0: 243, 1: 249, 2: 240, 3: 247, 4: 246, 5: 248, 6: 221, 7: 199, 8: 225, 9: 246}\n",
            "Client 7, Correct_pred_per_label: {0: 236, 1: 245, 2: 241, 3: 248, 4: 195, 5: 249, 6: 249, 7: 18, 8: 5, 9: 0}\n",
            "Client 4, Correct_pred_per_label: {0: 248, 1: 189, 2: 242, 3: 3, 4: 247, 5: 242, 6: 245, 7: 224, 8: 249, 9: 238}\n",
            "Client 9, Correct_pred_per_label: {0: 246, 1: 250, 2: 149, 3: 250, 4: 248, 5: 246, 6: 225, 7: 238, 8: 169, 9: 247}\n",
            "Client 1, Correct_pred_per_label: {0: 220, 1: 249, 2: 250, 3: 239, 4: 235, 5: 240, 6: 246, 7: 148, 8: 247, 9: 237}\n",
            "Clusters Glob Acc: [tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500)]\n",
            "###### ROUND 3 ######\n",
            "Clients [8 1 0 3 4 9 6 5 7 2]\n",
            "Client 8, Select Cluster: 0\n",
            "acc clusters: [tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500)]\n",
            "Client 1, Select Cluster: 0\n",
            "acc clusters: [tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500)]\n",
            "Client 0, Select Cluster: 0\n",
            "acc clusters: [tensor(97.6532), tensor(97.6532), tensor(97.6532), tensor(97.6532), tensor(97.6532), tensor(97.6532), tensor(97.6532), tensor(97.6532), tensor(97.6532), tensor(97.6532)]\n",
            "Client 3, Select Cluster: 0\n",
            "acc clusters: [tensor(97.6532), tensor(97.6532), tensor(97.6532), tensor(97.6532), tensor(97.6532), tensor(97.6532), tensor(97.6532), tensor(97.6532), tensor(97.6532), tensor(97.6532)]\n",
            "Client 4, Select Cluster: 0\n",
            "acc clusters: [tensor(95.9399), tensor(95.9399), tensor(95.9399), tensor(95.9399), tensor(95.9399), tensor(95.9399), tensor(95.9399), tensor(95.9399), tensor(95.9399), tensor(95.9399)]\n",
            "Client 9, Select Cluster: 0\n",
            "acc clusters: [tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500)]\n",
            "Client 6, Select Cluster: 0\n",
            "acc clusters: [tensor(98.5281), tensor(98.5281), tensor(98.5281), tensor(98.5281), tensor(98.5281), tensor(98.5281), tensor(98.5281), tensor(98.5281), tensor(98.5281), tensor(98.5281)]\n",
            "Client 5, Select Cluster: 0\n",
            "acc clusters: [tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500)]\n",
            "Client 7, Select Cluster: 0\n",
            "acc clusters: [tensor(98.6980), tensor(98.6980), tensor(98.6980), tensor(98.6980), tensor(98.6980), tensor(98.6980), tensor(98.6980), tensor(98.6980), tensor(98.6980), tensor(98.6980)]\n",
            "Client 2, Select Cluster: 0\n",
            "acc clusters: [tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500), tensor(96.0500)]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.017\n",
            "AVG Init Test Loss: 0.103, AVG Init Test Acc: 96.872\n",
            "AVG Final Test Loss: 0.169, AVG Final Test Acc: 95.765\n",
            "Round 3 Upload Cost: 1.777 MB\n",
            "Round 3 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 8, Count: 0, Labels: {0: 558, 1: 1832, 2: 77, 3: 396, 4: 39, 5: 42, 6: 1007, 7: 746, 8: 237, 9: 1275}\n",
            "Client 1, Count: 0, Labels: {0: 11, 1: 906, 2: 1645, 3: 94, 4: 54, 5: 213, 6: 838, 7: 3, 8: 964, 9: 183}\n",
            "Client 0, Count: 0, Labels: {0: 433, 1: 402, 2: 694, 3: 641, 4: 1746, 5: 1340, 6: 259, 7: 301, 8: 846}\n",
            "Client 3, Count: 0, Labels: {0: 1250, 1: 125, 2: 1374, 3: 22, 4: 928, 5: 111, 6: 331, 7: 1602, 8: 468}\n",
            "Client 4, Count: 0, Labels: {0: 758, 1: 3, 2: 298, 4: 510, 5: 86, 6: 568, 7: 114, 8: 3176, 9: 213}\n",
            "Client 9, Count: 0, Labels: {0: 207, 1: 492, 2: 9, 3: 1267, 4: 682, 5: 621, 6: 91, 7: 562, 8: 12, 9: 1441}\n",
            "Client 6, Count: 0, Labels: {0: 2271, 1: 42, 2: 1022, 3: 158, 4: 1480, 5: 21, 6: 283, 7: 2024}\n",
            "Client 5, Count: 0, Labels: {0: 218, 1: 1354, 2: 183, 3: 871, 4: 387, 5: 1466, 6: 44, 7: 14, 8: 128, 9: 2061}\n",
            "Client 7, Count: 0, Labels: {0: 114, 1: 92, 2: 274, 3: 2680, 4: 3, 5: 1117, 6: 2033}\n",
            "Client 2, Count: 0, Labels: {0: 103, 1: 1494, 2: 382, 3: 2, 4: 13, 5: 404, 6: 464, 7: 899, 8: 20, 9: 776}\n",
            "\n",
            "Client 8, Correct_pred_per_label: {0: 247, 1: 249, 2: 232, 3: 248, 4: 226, 5: 225, 6: 245, 7: 247, 8: 233, 9: 245}\n",
            "Client 1, Correct_pred_per_label: {0: 234, 1: 250, 2: 250, 3: 241, 4: 239, 5: 245, 6: 247, 7: 173, 8: 248, 9: 241}\n",
            "Client 0, Correct_pred_per_label: {0: 247, 1: 248, 2: 249, 3: 248, 4: 250, 5: 249, 6: 238, 7: 245, 8: 243, 9: 1}\n",
            "Client 3, Correct_pred_per_label: {0: 249, 1: 244, 2: 249, 3: 212, 4: 248, 5: 248, 6: 239, 7: 250, 8: 240, 9: 0}\n",
            "Client 4, Correct_pred_per_label: {0: 247, 1: 219, 2: 247, 3: 127, 4: 249, 5: 242, 6: 245, 7: 241, 8: 248, 9: 243}\n",
            "Client 9, Correct_pred_per_label: {0: 248, 1: 250, 2: 188, 3: 250, 4: 247, 5: 246, 6: 232, 7: 249, 8: 198, 9: 246}\n",
            "Client 6, Correct_pred_per_label: {0: 248, 1: 245, 2: 246, 3: 247, 4: 250, 5: 230, 6: 243, 7: 250, 8: 134, 9: 0}\n",
            "Client 5, Correct_pred_per_label: {0: 245, 1: 249, 2: 243, 3: 246, 4: 250, 5: 248, 6: 224, 7: 199, 8: 221, 9: 243}\n",
            "Client 7, Correct_pred_per_label: {0: 241, 1: 246, 2: 244, 3: 249, 4: 219, 5: 246, 6: 247, 7: 45, 8: 75, 9: 9}\n",
            "Client 2, Correct_pred_per_label: {0: 235, 1: 247, 2: 247, 3: 175, 4: 191, 5: 249, 6: 247, 7: 248, 8: 209, 9: 248}\n",
            "Clusters Glob Acc: [tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400)]\n",
            "###### ROUND 4 ######\n",
            "Clients [2 0 3 5 1 6 4 9 7 8]\n",
            "Client 2, Select Cluster: 0\n",
            "acc clusters: [tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400)]\n",
            "Client 0, Select Cluster: 0\n",
            "acc clusters: [tensor(98.3762), tensor(98.3762), tensor(98.3762), tensor(98.3762), tensor(98.3762), tensor(98.3762), tensor(98.3762), tensor(98.3762), tensor(98.3762), tensor(98.3762)]\n",
            "Client 3, Select Cluster: 0\n",
            "acc clusters: [tensor(98.3762), tensor(98.3762), tensor(98.3762), tensor(98.3762), tensor(98.3762), tensor(98.3762), tensor(98.3762), tensor(98.3762), tensor(98.3762), tensor(98.3762)]\n",
            "Client 5, Select Cluster: 0\n",
            "acc clusters: [tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400)]\n",
            "Client 1, Select Cluster: 0\n",
            "acc clusters: [tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400)]\n",
            "Client 6, Select Cluster: 0\n",
            "acc clusters: [tensor(98.8150), tensor(98.8150), tensor(98.8150), tensor(98.8150), tensor(98.8150), tensor(98.8150), tensor(98.8150), tensor(98.8150), tensor(98.8150), tensor(98.8150)]\n",
            "Client 4, Select Cluster: 0\n",
            "acc clusters: [tensor(97.6974), tensor(97.6974), tensor(97.6974), tensor(97.6974), tensor(97.6974), tensor(97.6974), tensor(97.6974), tensor(97.6974), tensor(97.6974), tensor(97.6974)]\n",
            "Client 9, Select Cluster: 0\n",
            "acc clusters: [tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400)]\n",
            "Client 7, Select Cluster: 0\n",
            "acc clusters: [tensor(98.9841), tensor(98.9841), tensor(98.9841), tensor(98.9841), tensor(98.9841), tensor(98.9841), tensor(98.9841), tensor(98.9841), tensor(98.9841), tensor(98.9841)]\n",
            "Client 8, Select Cluster: 0\n",
            "acc clusters: [tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400), tensor(97.7400)]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.012\n",
            "AVG Init Test Loss: 0.067, AVG Init Test Acc: 98.095\n",
            "AVG Final Test Loss: 0.150, AVG Final Test Acc: 96.238\n",
            "Round 4 Upload Cost: 1.777 MB\n",
            "Round 4 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 2, Count: 0, Labels: {0: 103, 1: 1494, 2: 382, 3: 2, 4: 13, 5: 404, 6: 464, 7: 899, 8: 20, 9: 776}\n",
            "Client 0, Count: 0, Labels: {0: 433, 1: 402, 2: 694, 3: 641, 4: 1746, 5: 1340, 6: 259, 7: 301, 8: 846}\n",
            "Client 3, Count: 0, Labels: {0: 1250, 1: 125, 2: 1374, 3: 22, 4: 928, 5: 111, 6: 331, 7: 1602, 8: 468}\n",
            "Client 5, Count: 0, Labels: {0: 218, 1: 1354, 2: 183, 3: 871, 4: 387, 5: 1466, 6: 44, 7: 14, 8: 128, 9: 2061}\n",
            "Client 1, Count: 0, Labels: {0: 11, 1: 906, 2: 1645, 3: 94, 4: 54, 5: 213, 6: 838, 7: 3, 8: 964, 9: 183}\n",
            "Client 6, Count: 0, Labels: {0: 2271, 1: 42, 2: 1022, 3: 158, 4: 1480, 5: 21, 6: 283, 7: 2024}\n",
            "Client 4, Count: 0, Labels: {0: 758, 1: 3, 2: 298, 4: 510, 5: 86, 6: 568, 7: 114, 8: 3176, 9: 213}\n",
            "Client 9, Count: 0, Labels: {0: 207, 1: 492, 2: 9, 3: 1267, 4: 682, 5: 621, 6: 91, 7: 562, 8: 12, 9: 1441}\n",
            "Client 7, Count: 0, Labels: {0: 114, 1: 92, 2: 274, 3: 2680, 4: 3, 5: 1117, 6: 2033}\n",
            "Client 8, Count: 0, Labels: {0: 558, 1: 1832, 2: 77, 3: 396, 4: 39, 5: 42, 6: 1007, 7: 746, 8: 237, 9: 1275}\n",
            "\n",
            "Client 2, Correct_pred_per_label: {0: 238, 1: 247, 2: 247, 3: 186, 4: 207, 5: 249, 6: 246, 7: 248, 8: 222, 9: 246}\n",
            "Client 0, Correct_pred_per_label: {0: 248, 1: 247, 2: 249, 3: 247, 4: 250, 5: 248, 6: 237, 7: 245, 8: 243, 9: 20}\n",
            "Client 3, Correct_pred_per_label: {0: 249, 1: 245, 2: 248, 3: 214, 4: 248, 5: 249, 6: 241, 7: 250, 8: 243, 9: 18}\n",
            "Client 5, Correct_pred_per_label: {0: 246, 1: 249, 2: 246, 3: 247, 4: 246, 5: 248, 6: 219, 7: 199, 8: 224, 9: 247}\n",
            "Client 1, Correct_pred_per_label: {0: 237, 1: 249, 2: 250, 3: 243, 4: 240, 5: 247, 6: 247, 7: 179, 8: 247, 9: 243}\n",
            "Client 6, Correct_pred_per_label: {0: 248, 1: 245, 2: 248, 3: 247, 4: 250, 5: 233, 6: 244, 7: 250, 8: 161, 9: 3}\n",
            "Client 4, Correct_pred_per_label: {0: 247, 1: 220, 2: 246, 3: 179, 4: 249, 5: 241, 6: 245, 7: 246, 8: 249, 9: 244}\n",
            "Client 9, Correct_pred_per_label: {0: 247, 1: 250, 2: 204, 3: 250, 4: 249, 5: 246, 6: 235, 7: 245, 8: 199, 9: 249}\n",
            "Client 7, Correct_pred_per_label: {0: 242, 1: 247, 2: 243, 3: 249, 4: 219, 5: 247, 6: 247, 7: 137, 8: 113, 9: 87}\n",
            "Client 8, Correct_pred_per_label: {0: 247, 1: 249, 2: 239, 3: 248, 4: 223, 5: 230, 6: 248, 7: 247, 8: 236, 9: 245}\n",
            "Clusters Glob Acc: [tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600)]\n",
            "###### ROUND 5 ######\n",
            "Clients [9 5 4 3 0 8 2 7 1 6]\n",
            "Client 9, Select Cluster: 0\n",
            "acc clusters: [tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600)]\n",
            "Client 5, Select Cluster: 0\n",
            "acc clusters: [tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600)]\n",
            "Client 4, Select Cluster: 0\n",
            "acc clusters: [tensor(98.2647), tensor(98.2647), tensor(98.2647), tensor(98.2647), tensor(98.2647), tensor(98.2647), tensor(98.2647), tensor(98.2647), tensor(98.2647), tensor(98.2647)]\n",
            "Client 3, Select Cluster: 0\n",
            "acc clusters: [tensor(98.5875), tensor(98.5875), tensor(98.5875), tensor(98.5875), tensor(98.5875), tensor(98.5875), tensor(98.5875), tensor(98.5875), tensor(98.5875), tensor(98.5875)]\n",
            "Client 0, Select Cluster: 0\n",
            "acc clusters: [tensor(98.5875), tensor(98.5875), tensor(98.5875), tensor(98.5875), tensor(98.5875), tensor(98.5875), tensor(98.5875), tensor(98.5875), tensor(98.5875), tensor(98.5875)]\n",
            "Client 8, Select Cluster: 0\n",
            "acc clusters: [tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600)]\n",
            "Client 2, Select Cluster: 0\n",
            "acc clusters: [tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600)]\n",
            "Client 7, Select Cluster: 0\n",
            "acc clusters: [tensor(99.0127), tensor(99.0127), tensor(99.0127), tensor(99.0127), tensor(99.0127), tensor(99.0127), tensor(99.0127), tensor(99.0127), tensor(99.0127), tensor(99.0127)]\n",
            "Client 1, Select Cluster: 0\n",
            "acc clusters: [tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600), tensor(98.2600)]\n",
            "Client 6, Select Cluster: 0\n",
            "acc clusters: [tensor(98.8899), tensor(98.8899), tensor(98.8899), tensor(98.8899), tensor(98.8899), tensor(98.8899), tensor(98.8899), tensor(98.8899), tensor(98.8899), tensor(98.8899)]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.010\n",
            "AVG Init Test Loss: 0.050, AVG Init Test Acc: 98.464\n",
            "AVG Final Test Loss: 0.129, AVG Final Test Acc: 96.770\n",
            "Round 5 Upload Cost: 1.777 MB\n",
            "Round 5 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 9, Count: 0, Labels: {0: 207, 1: 492, 2: 9, 3: 1267, 4: 682, 5: 621, 6: 91, 7: 562, 8: 12, 9: 1441}\n",
            "Client 5, Count: 0, Labels: {0: 218, 1: 1354, 2: 183, 3: 871, 4: 387, 5: 1466, 6: 44, 7: 14, 8: 128, 9: 2061}\n",
            "Client 4, Count: 0, Labels: {0: 758, 1: 3, 2: 298, 4: 510, 5: 86, 6: 568, 7: 114, 8: 3176, 9: 213}\n",
            "Client 3, Count: 0, Labels: {0: 1250, 1: 125, 2: 1374, 3: 22, 4: 928, 5: 111, 6: 331, 7: 1602, 8: 468}\n",
            "Client 0, Count: 0, Labels: {0: 433, 1: 402, 2: 694, 3: 641, 4: 1746, 5: 1340, 6: 259, 7: 301, 8: 846}\n",
            "Client 8, Count: 0, Labels: {0: 558, 1: 1832, 2: 77, 3: 396, 4: 39, 5: 42, 6: 1007, 7: 746, 8: 237, 9: 1275}\n",
            "Client 2, Count: 0, Labels: {0: 103, 1: 1494, 2: 382, 3: 2, 4: 13, 5: 404, 6: 464, 7: 899, 8: 20, 9: 776}\n",
            "Client 7, Count: 0, Labels: {0: 114, 1: 92, 2: 274, 3: 2680, 4: 3, 5: 1117, 6: 2033}\n",
            "Client 1, Count: 0, Labels: {0: 11, 1: 906, 2: 1645, 3: 94, 4: 54, 5: 213, 6: 838, 7: 3, 8: 964, 9: 183}\n",
            "Client 6, Count: 0, Labels: {0: 2271, 1: 42, 2: 1022, 3: 158, 4: 1480, 5: 21, 6: 283, 7: 2024}\n",
            "\n",
            "Client 9, Correct_pred_per_label: {0: 245, 1: 250, 2: 204, 3: 250, 4: 250, 5: 246, 6: 236, 7: 246, 8: 215, 9: 247}\n",
            "Client 5, Correct_pred_per_label: {0: 246, 1: 249, 2: 245, 3: 248, 4: 247, 5: 248, 6: 222, 7: 209, 8: 225, 9: 248}\n",
            "Client 4, Correct_pred_per_label: {0: 248, 1: 221, 2: 244, 3: 199, 4: 248, 5: 243, 6: 245, 7: 246, 8: 249, 9: 243}\n",
            "Client 3, Correct_pred_per_label: {0: 249, 1: 246, 2: 248, 3: 217, 4: 249, 5: 248, 6: 239, 7: 250, 8: 243, 9: 45}\n",
            "Client 0, Correct_pred_per_label: {0: 247, 1: 248, 2: 249, 3: 247, 4: 250, 5: 248, 6: 241, 7: 245, 8: 244, 9: 45}\n",
            "Client 8, Correct_pred_per_label: {0: 247, 1: 248, 2: 239, 3: 248, 4: 226, 5: 232, 6: 246, 7: 248, 8: 238, 9: 245}\n",
            "Client 2, Correct_pred_per_label: {0: 238, 1: 247, 2: 247, 3: 213, 4: 213, 5: 249, 6: 246, 7: 248, 8: 224, 9: 248}\n",
            "Client 7, Correct_pred_per_label: {0: 243, 1: 245, 2: 246, 3: 249, 4: 240, 5: 247, 6: 247, 7: 139, 8: 104, 9: 148}\n",
            "Client 1, Correct_pred_per_label: {0: 232, 1: 248, 2: 250, 3: 245, 4: 239, 5: 246, 6: 248, 7: 201, 8: 248, 9: 244}\n",
            "Client 6, Correct_pred_per_label: {0: 248, 1: 244, 2: 249, 3: 247, 4: 250, 5: 239, 6: 243, 7: 250, 8: 167, 9: 29}\n",
            "Clusters Glob Acc: [tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900)]\n",
            "###### ROUND 6 ######\n",
            "Clients [0 7 9 1 5 4 2 8 3 6]\n",
            "Client 0, Select Cluster: 0\n",
            "acc clusters: [tensor(98.7432), tensor(98.7432), tensor(98.7432), tensor(98.7432), tensor(98.7432), tensor(98.7432), tensor(98.7432), tensor(98.7432), tensor(98.7432), tensor(98.7432)]\n",
            "Client 7, Select Cluster: 0\n",
            "acc clusters: [tensor(99.1558), tensor(99.1558), tensor(99.1558), tensor(99.1558), tensor(99.1558), tensor(99.1558), tensor(99.1558), tensor(99.1558), tensor(99.1558), tensor(99.1558)]\n",
            "Client 9, Select Cluster: 0\n",
            "acc clusters: [tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900)]\n",
            "Client 1, Select Cluster: 0\n",
            "acc clusters: [tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900)]\n",
            "Client 5, Select Cluster: 0\n",
            "acc clusters: [tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900)]\n",
            "Client 4, Select Cluster: 0\n",
            "acc clusters: [tensor(98.4650), tensor(98.4650), tensor(98.4650), tensor(98.4650), tensor(98.4650), tensor(98.4650), tensor(98.4650), tensor(98.4650), tensor(98.4650), tensor(98.4650)]\n",
            "Client 2, Select Cluster: 0\n",
            "acc clusters: [tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900)]\n",
            "Client 8, Select Cluster: 0\n",
            "acc clusters: [tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900), tensor(98.4900)]\n",
            "Client 3, Select Cluster: 0\n",
            "acc clusters: [tensor(98.7432), tensor(98.7432), tensor(98.7432), tensor(98.7432), tensor(98.7432), tensor(98.7432), tensor(98.7432), tensor(98.7432), tensor(98.7432), tensor(98.7432)]\n",
            "Client 6, Select Cluster: 0\n",
            "acc clusters: [tensor(99.0271), tensor(99.0271), tensor(99.0271), tensor(99.0271), tensor(99.0271), tensor(99.0271), tensor(99.0271), tensor(99.0271), tensor(99.0271), tensor(99.0271)]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.008\n",
            "AVG Init Test Loss: 0.045, AVG Init Test Acc: 98.658\n",
            "AVG Final Test Loss: 0.122, AVG Final Test Acc: 96.952\n",
            "Round 6 Upload Cost: 1.777 MB\n",
            "Round 6 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 0, Count: 0, Labels: {0: 433, 1: 402, 2: 694, 3: 641, 4: 1746, 5: 1340, 6: 259, 7: 301, 8: 846}\n",
            "Client 7, Count: 0, Labels: {0: 114, 1: 92, 2: 274, 3: 2680, 4: 3, 5: 1117, 6: 2033}\n",
            "Client 9, Count: 0, Labels: {0: 207, 1: 492, 2: 9, 3: 1267, 4: 682, 5: 621, 6: 91, 7: 562, 8: 12, 9: 1441}\n",
            "Client 1, Count: 0, Labels: {0: 11, 1: 906, 2: 1645, 3: 94, 4: 54, 5: 213, 6: 838, 7: 3, 8: 964, 9: 183}\n",
            "Client 5, Count: 0, Labels: {0: 218, 1: 1354, 2: 183, 3: 871, 4: 387, 5: 1466, 6: 44, 7: 14, 8: 128, 9: 2061}\n",
            "Client 4, Count: 0, Labels: {0: 758, 1: 3, 2: 298, 4: 510, 5: 86, 6: 568, 7: 114, 8: 3176, 9: 213}\n",
            "Client 2, Count: 0, Labels: {0: 103, 1: 1494, 2: 382, 3: 2, 4: 13, 5: 404, 6: 464, 7: 899, 8: 20, 9: 776}\n",
            "Client 8, Count: 0, Labels: {0: 558, 1: 1832, 2: 77, 3: 396, 4: 39, 5: 42, 6: 1007, 7: 746, 8: 237, 9: 1275}\n",
            "Client 3, Count: 0, Labels: {0: 1250, 1: 125, 2: 1374, 3: 22, 4: 928, 5: 111, 6: 331, 7: 1602, 8: 468}\n",
            "Client 6, Count: 0, Labels: {0: 2271, 1: 42, 2: 1022, 3: 158, 4: 1480, 5: 21, 6: 283, 7: 2024}\n",
            "\n",
            "Client 0, Correct_pred_per_label: {0: 248, 1: 248, 2: 248, 3: 247, 4: 249, 5: 247, 6: 242, 7: 247, 8: 244, 9: 34}\n",
            "Client 7, Correct_pred_per_label: {0: 244, 1: 245, 2: 244, 3: 249, 4: 234, 5: 246, 6: 248, 7: 211, 8: 131, 9: 151}\n",
            "Client 9, Correct_pred_per_label: {0: 247, 1: 249, 2: 201, 3: 249, 4: 250, 5: 247, 6: 237, 7: 248, 8: 212, 9: 246}\n",
            "Client 1, Correct_pred_per_label: {0: 237, 1: 248, 2: 250, 3: 246, 4: 239, 5: 247, 6: 247, 7: 211, 8: 248, 9: 246}\n",
            "Client 5, Correct_pred_per_label: {0: 246, 1: 248, 2: 244, 3: 247, 4: 246, 5: 248, 6: 225, 7: 221, 8: 228, 9: 248}\n",
            "Client 4, Correct_pred_per_label: {0: 248, 1: 223, 2: 246, 3: 220, 4: 248, 5: 245, 6: 245, 7: 247, 8: 249, 9: 243}\n",
            "Client 2, Correct_pred_per_label: {0: 240, 1: 248, 2: 248, 3: 212, 4: 209, 5: 249, 6: 246, 7: 248, 8: 224, 9: 246}\n",
            "Client 8, Correct_pred_per_label: {0: 247, 1: 248, 2: 242, 3: 249, 4: 226, 5: 236, 6: 248, 7: 248, 8: 238, 9: 245}\n",
            "Client 3, Correct_pred_per_label: {0: 249, 1: 244, 2: 248, 3: 221, 4: 249, 5: 248, 6: 240, 7: 250, 8: 243, 9: 59}\n",
            "Client 6, Correct_pred_per_label: {0: 248, 1: 244, 2: 249, 3: 246, 4: 250, 5: 242, 6: 244, 7: 250, 8: 164, 9: 41}\n",
            "Clusters Glob Acc: [tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000)]\n",
            "###### ROUND 7 ######\n",
            "Clients [4 0 5 3 9 6 1 7 2 8]\n",
            "Client 4, Select Cluster: 0\n",
            "acc clusters: [tensor(98.5873), tensor(98.5873), tensor(98.5873), tensor(98.5873), tensor(98.5873), tensor(98.5873), tensor(98.5873), tensor(98.5873), tensor(98.5873), tensor(98.5873)]\n",
            "Client 0, Select Cluster: 0\n",
            "acc clusters: [tensor(98.8767), tensor(98.8767), tensor(98.8767), tensor(98.8767), tensor(98.8767), tensor(98.8767), tensor(98.8767), tensor(98.8767), tensor(98.8767), tensor(98.8767)]\n",
            "Client 5, Select Cluster: 0\n",
            "acc clusters: [tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000)]\n",
            "Client 3, Select Cluster: 0\n",
            "acc clusters: [tensor(98.8767), tensor(98.8767), tensor(98.8767), tensor(98.8767), tensor(98.8767), tensor(98.8767), tensor(98.8767), tensor(98.8767), tensor(98.8767), tensor(98.8767)]\n",
            "Client 9, Select Cluster: 0\n",
            "acc clusters: [tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000)]\n",
            "Client 6, Select Cluster: 0\n",
            "acc clusters: [tensor(99.1144), tensor(99.1144), tensor(99.1144), tensor(99.1144), tensor(99.1144), tensor(99.1144), tensor(99.1144), tensor(99.1144), tensor(99.1144), tensor(99.1144)]\n",
            "Client 1, Select Cluster: 0\n",
            "acc clusters: [tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000)]\n",
            "Client 7, Select Cluster: 0\n",
            "acc clusters: [tensor(99.1701), tensor(99.1701), tensor(99.1701), tensor(99.1701), tensor(99.1701), tensor(99.1701), tensor(99.1701), tensor(99.1701), tensor(99.1701), tensor(99.1701)]\n",
            "Client 2, Select Cluster: 0\n",
            "acc clusters: [tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000)]\n",
            "Client 8, Select Cluster: 0\n",
            "acc clusters: [tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000), tensor(98.6000)]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.007\n",
            "AVG Init Test Loss: 0.043, AVG Init Test Acc: 98.763\n",
            "AVG Final Test Loss: 0.116, AVG Final Test Acc: 97.147\n",
            "Round 7 Upload Cost: 1.777 MB\n",
            "Round 7 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 4, Count: 0, Labels: {0: 758, 1: 3, 2: 298, 4: 510, 5: 86, 6: 568, 7: 114, 8: 3176, 9: 213}\n",
            "Client 0, Count: 0, Labels: {0: 433, 1: 402, 2: 694, 3: 641, 4: 1746, 5: 1340, 6: 259, 7: 301, 8: 846}\n",
            "Client 5, Count: 0, Labels: {0: 218, 1: 1354, 2: 183, 3: 871, 4: 387, 5: 1466, 6: 44, 7: 14, 8: 128, 9: 2061}\n",
            "Client 3, Count: 0, Labels: {0: 1250, 1: 125, 2: 1374, 3: 22, 4: 928, 5: 111, 6: 331, 7: 1602, 8: 468}\n",
            "Client 9, Count: 0, Labels: {0: 207, 1: 492, 2: 9, 3: 1267, 4: 682, 5: 621, 6: 91, 7: 562, 8: 12, 9: 1441}\n",
            "Client 6, Count: 0, Labels: {0: 2271, 1: 42, 2: 1022, 3: 158, 4: 1480, 5: 21, 6: 283, 7: 2024}\n",
            "Client 1, Count: 0, Labels: {0: 11, 1: 906, 2: 1645, 3: 94, 4: 54, 5: 213, 6: 838, 7: 3, 8: 964, 9: 183}\n",
            "Client 7, Count: 0, Labels: {0: 114, 1: 92, 2: 274, 3: 2680, 4: 3, 5: 1117, 6: 2033}\n",
            "Client 2, Count: 0, Labels: {0: 103, 1: 1494, 2: 382, 3: 2, 4: 13, 5: 404, 6: 464, 7: 899, 8: 20, 9: 776}\n",
            "Client 8, Count: 0, Labels: {0: 558, 1: 1832, 2: 77, 3: 396, 4: 39, 5: 42, 6: 1007, 7: 746, 8: 237, 9: 1275}\n",
            "\n",
            "Client 4, Correct_pred_per_label: {0: 247, 1: 222, 2: 247, 3: 208, 4: 249, 5: 246, 6: 245, 7: 244, 8: 250, 9: 242}\n",
            "Client 0, Correct_pred_per_label: {0: 247, 1: 247, 2: 249, 3: 249, 4: 250, 5: 248, 6: 237, 7: 246, 8: 245, 9: 53}\n",
            "Client 5, Correct_pred_per_label: {0: 245, 1: 249, 2: 244, 3: 247, 4: 246, 5: 248, 6: 226, 7: 211, 8: 228, 9: 249}\n",
            "Client 3, Correct_pred_per_label: {0: 249, 1: 244, 2: 249, 3: 218, 4: 250, 5: 248, 6: 241, 7: 250, 8: 245, 9: 82}\n",
            "Client 9, Correct_pred_per_label: {0: 247, 1: 250, 2: 212, 3: 249, 4: 250, 5: 246, 6: 239, 7: 245, 8: 220, 9: 248}\n",
            "Client 6, Correct_pred_per_label: {0: 249, 1: 244, 2: 249, 3: 246, 4: 250, 5: 241, 6: 243, 7: 250, 8: 186, 9: 72}\n",
            "Client 1, Correct_pred_per_label: {0: 239, 1: 248, 2: 250, 3: 246, 4: 242, 5: 246, 6: 248, 7: 206, 8: 248, 9: 245}\n",
            "Client 7, Correct_pred_per_label: {0: 246, 1: 244, 2: 247, 3: 249, 4: 242, 5: 247, 6: 247, 7: 221, 8: 138, 9: 205}\n",
            "Client 2, Correct_pred_per_label: {0: 239, 1: 247, 2: 248, 3: 226, 4: 216, 5: 249, 6: 246, 7: 248, 8: 228, 9: 246}\n",
            "Client 8, Correct_pred_per_label: {0: 247, 1: 248, 2: 241, 3: 249, 4: 230, 5: 232, 6: 248, 7: 247, 8: 239, 9: 245}\n",
            "Clusters Glob Acc: [tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900)]\n",
            "###### ROUND 8 ######\n",
            "Clients [9 4 8 3 6 5 7 0 1 2]\n",
            "Client 9, Select Cluster: 0\n",
            "acc clusters: [tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900)]\n",
            "Client 4, Select Cluster: 0\n",
            "acc clusters: [tensor(98.6541), tensor(98.6541), tensor(98.6541), tensor(98.6541), tensor(98.6541), tensor(98.6541), tensor(98.6541), tensor(98.6541), tensor(98.6541), tensor(98.6541)]\n",
            "Client 8, Select Cluster: 0\n",
            "acc clusters: [tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900)]\n",
            "Client 3, Select Cluster: 0\n",
            "acc clusters: [tensor(98.9545), tensor(98.9545), tensor(98.9545), tensor(98.9545), tensor(98.9545), tensor(98.9545), tensor(98.9545), tensor(98.9545), tensor(98.9545), tensor(98.9545)]\n",
            "Client 6, Select Cluster: 0\n",
            "acc clusters: [tensor(99.1144), tensor(99.1144), tensor(99.1144), tensor(99.1144), tensor(99.1144), tensor(99.1144), tensor(99.1144), tensor(99.1144), tensor(99.1144), tensor(99.1144)]\n",
            "Client 5, Select Cluster: 0\n",
            "acc clusters: [tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900)]\n",
            "Client 7, Select Cluster: 0\n",
            "acc clusters: [tensor(99.2131), tensor(99.2131), tensor(99.2131), tensor(99.2131), tensor(99.2131), tensor(99.2131), tensor(99.2131), tensor(99.2131), tensor(99.2131), tensor(99.2131)]\n",
            "Client 0, Select Cluster: 0\n",
            "acc clusters: [tensor(98.9545), tensor(98.9545), tensor(98.9545), tensor(98.9545), tensor(98.9545), tensor(98.9545), tensor(98.9545), tensor(98.9545), tensor(98.9545), tensor(98.9545)]\n",
            "Client 1, Select Cluster: 0\n",
            "acc clusters: [tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900)]\n",
            "Client 2, Select Cluster: 0\n",
            "acc clusters: [tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900), tensor(98.6900)]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.006\n",
            "AVG Init Test Loss: 0.040, AVG Init Test Acc: 98.834\n",
            "AVG Final Test Loss: 0.108, AVG Final Test Acc: 97.290\n",
            "Round 8 Upload Cost: 1.777 MB\n",
            "Round 8 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 9, Count: 0, Labels: {0: 207, 1: 492, 2: 9, 3: 1267, 4: 682, 5: 621, 6: 91, 7: 562, 8: 12, 9: 1441}\n",
            "Client 4, Count: 0, Labels: {0: 758, 1: 3, 2: 298, 4: 510, 5: 86, 6: 568, 7: 114, 8: 3176, 9: 213}\n",
            "Client 8, Count: 0, Labels: {0: 558, 1: 1832, 2: 77, 3: 396, 4: 39, 5: 42, 6: 1007, 7: 746, 8: 237, 9: 1275}\n",
            "Client 3, Count: 0, Labels: {0: 1250, 1: 125, 2: 1374, 3: 22, 4: 928, 5: 111, 6: 331, 7: 1602, 8: 468}\n",
            "Client 6, Count: 0, Labels: {0: 2271, 1: 42, 2: 1022, 3: 158, 4: 1480, 5: 21, 6: 283, 7: 2024}\n",
            "Client 5, Count: 0, Labels: {0: 218, 1: 1354, 2: 183, 3: 871, 4: 387, 5: 1466, 6: 44, 7: 14, 8: 128, 9: 2061}\n",
            "Client 7, Count: 0, Labels: {0: 114, 1: 92, 2: 274, 3: 2680, 4: 3, 5: 1117, 6: 2033}\n",
            "Client 0, Count: 0, Labels: {0: 433, 1: 402, 2: 694, 3: 641, 4: 1746, 5: 1340, 6: 259, 7: 301, 8: 846}\n",
            "Client 1, Count: 0, Labels: {0: 11, 1: 906, 2: 1645, 3: 94, 4: 54, 5: 213, 6: 838, 7: 3, 8: 964, 9: 183}\n",
            "Client 2, Count: 0, Labels: {0: 103, 1: 1494, 2: 382, 3: 2, 4: 13, 5: 404, 6: 464, 7: 899, 8: 20, 9: 776}\n",
            "\n",
            "Client 9, Correct_pred_per_label: {0: 246, 1: 249, 2: 210, 3: 249, 4: 250, 5: 245, 6: 237, 7: 246, 8: 222, 9: 246}\n",
            "Client 4, Correct_pred_per_label: {0: 248, 1: 223, 2: 246, 3: 217, 4: 248, 5: 245, 6: 245, 7: 246, 8: 249, 9: 242}\n",
            "Client 8, Correct_pred_per_label: {0: 248, 1: 248, 2: 243, 3: 249, 4: 232, 5: 237, 6: 248, 7: 248, 8: 237, 9: 245}\n",
            "Client 3, Correct_pred_per_label: {0: 249, 1: 244, 2: 248, 3: 224, 4: 250, 5: 249, 6: 240, 7: 249, 8: 244, 9: 126}\n",
            "Client 6, Correct_pred_per_label: {0: 248, 1: 244, 2: 249, 3: 247, 4: 250, 5: 242, 6: 245, 7: 250, 8: 175, 9: 58}\n",
            "Client 5, Correct_pred_per_label: {0: 246, 1: 249, 2: 244, 3: 247, 4: 247, 5: 248, 6: 222, 7: 227, 8: 232, 9: 248}\n",
            "Client 7, Correct_pred_per_label: {0: 245, 1: 245, 2: 245, 3: 249, 4: 231, 5: 247, 6: 247, 7: 220, 8: 143, 9: 201}\n",
            "Client 0, Correct_pred_per_label: {0: 248, 1: 247, 2: 250, 3: 247, 4: 250, 5: 248, 6: 238, 7: 246, 8: 245, 9: 72}\n",
            "Client 1, Correct_pred_per_label: {0: 236, 1: 249, 2: 249, 3: 244, 4: 243, 5: 248, 6: 249, 7: 215, 8: 249, 9: 245}\n",
            "Client 2, Correct_pred_per_label: {0: 238, 1: 247, 2: 248, 3: 234, 4: 213, 5: 247, 6: 246, 7: 249, 8: 228, 9: 247}\n",
            "Clusters Glob Acc: [tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800)]\n",
            "###### ROUND 9 ######\n",
            "Clients [4 9 1 8 5 2 0 6 3 7]\n",
            "Client 4, Select Cluster: 0\n",
            "acc clusters: [tensor(98.7542), tensor(98.7542), tensor(98.7542), tensor(98.7542), tensor(98.7542), tensor(98.7542), tensor(98.7542), tensor(98.7542), tensor(98.7542), tensor(98.7542)]\n",
            "Client 9, Select Cluster: 0\n",
            "acc clusters: [tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800)]\n",
            "Client 1, Select Cluster: 0\n",
            "acc clusters: [tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800)]\n",
            "Client 8, Select Cluster: 0\n",
            "acc clusters: [tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800)]\n",
            "Client 5, Select Cluster: 0\n",
            "acc clusters: [tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800)]\n",
            "Client 2, Select Cluster: 0\n",
            "acc clusters: [tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800), tensor(98.7800)]\n",
            "Client 0, Select Cluster: 0\n",
            "acc clusters: [tensor(99.0212), tensor(99.0212), tensor(99.0212), tensor(99.0212), tensor(99.0212), tensor(99.0212), tensor(99.0212), tensor(99.0212), tensor(99.0212), tensor(99.0212)]\n",
            "Client 6, Select Cluster: 0\n",
            "acc clusters: [tensor(99.1768), tensor(99.1768), tensor(99.1768), tensor(99.1768), tensor(99.1768), tensor(99.1768), tensor(99.1768), tensor(99.1768), tensor(99.1768), tensor(99.1768)]\n",
            "Client 3, Select Cluster: 0\n",
            "acc clusters: [tensor(99.0212), tensor(99.0212), tensor(99.0212), tensor(99.0212), tensor(99.0212), tensor(99.0212), tensor(99.0212), tensor(99.0212), tensor(99.0212), tensor(99.0212)]\n",
            "Client 7, Select Cluster: 0\n",
            "acc clusters: [tensor(99.2274), tensor(99.2274), tensor(99.2274), tensor(99.2274), tensor(99.2274), tensor(99.2274), tensor(99.2274), tensor(99.2274), tensor(99.2274), tensor(99.2274)]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.006\n",
            "AVG Init Test Loss: 0.039, AVG Init Test Acc: 98.910\n",
            "AVG Final Test Loss: 0.102, AVG Final Test Acc: 97.451\n",
            "Round 9 Upload Cost: 1.777 MB\n",
            "Round 9 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 4, Count: 0, Labels: {0: 758, 1: 3, 2: 298, 4: 510, 5: 86, 6: 568, 7: 114, 8: 3176, 9: 213}\n",
            "Client 9, Count: 0, Labels: {0: 207, 1: 492, 2: 9, 3: 1267, 4: 682, 5: 621, 6: 91, 7: 562, 8: 12, 9: 1441}\n",
            "Client 1, Count: 0, Labels: {0: 11, 1: 906, 2: 1645, 3: 94, 4: 54, 5: 213, 6: 838, 7: 3, 8: 964, 9: 183}\n",
            "Client 8, Count: 0, Labels: {0: 558, 1: 1832, 2: 77, 3: 396, 4: 39, 5: 42, 6: 1007, 7: 746, 8: 237, 9: 1275}\n",
            "Client 5, Count: 0, Labels: {0: 218, 1: 1354, 2: 183, 3: 871, 4: 387, 5: 1466, 6: 44, 7: 14, 8: 128, 9: 2061}\n",
            "Client 2, Count: 0, Labels: {0: 103, 1: 1494, 2: 382, 3: 2, 4: 13, 5: 404, 6: 464, 7: 899, 8: 20, 9: 776}\n",
            "Client 0, Count: 0, Labels: {0: 433, 1: 402, 2: 694, 3: 641, 4: 1746, 5: 1340, 6: 259, 7: 301, 8: 846}\n",
            "Client 6, Count: 0, Labels: {0: 2271, 1: 42, 2: 1022, 3: 158, 4: 1480, 5: 21, 6: 283, 7: 2024}\n",
            "Client 3, Count: 0, Labels: {0: 1250, 1: 125, 2: 1374, 3: 22, 4: 928, 5: 111, 6: 331, 7: 1602, 8: 468}\n",
            "Client 7, Count: 0, Labels: {0: 114, 1: 92, 2: 274, 3: 2680, 4: 3, 5: 1117, 6: 2033}\n",
            "\n",
            "Client 4, Correct_pred_per_label: {0: 248, 1: 225, 2: 247, 3: 223, 4: 249, 5: 243, 6: 248, 7: 246, 8: 250, 9: 244}\n",
            "Client 9, Correct_pred_per_label: {0: 247, 1: 249, 2: 199, 3: 248, 4: 250, 5: 247, 6: 237, 7: 249, 8: 225, 9: 243}\n",
            "Client 1, Correct_pred_per_label: {0: 243, 1: 249, 2: 250, 3: 246, 4: 240, 5: 247, 6: 250, 7: 230, 8: 247, 9: 245}\n",
            "Client 8, Correct_pred_per_label: {0: 247, 1: 250, 2: 243, 3: 250, 4: 231, 5: 236, 6: 247, 7: 248, 8: 242, 9: 245}\n",
            "Client 5, Correct_pred_per_label: {0: 246, 1: 249, 2: 245, 3: 247, 4: 248, 5: 248, 6: 227, 7: 226, 8: 228, 9: 247}\n",
            "Client 2, Correct_pred_per_label: {0: 242, 1: 248, 2: 248, 3: 228, 4: 221, 5: 248, 6: 246, 7: 249, 8: 226, 9: 246}\n",
            "Client 0, Correct_pred_per_label: {0: 248, 1: 247, 2: 250, 3: 247, 4: 250, 5: 248, 6: 241, 7: 247, 8: 247, 9: 77}\n",
            "Client 6, Correct_pred_per_label: {0: 248, 1: 244, 2: 249, 3: 247, 4: 250, 5: 242, 6: 244, 7: 250, 8: 197, 9: 122}\n",
            "Client 3, Correct_pred_per_label: {0: 249, 1: 244, 2: 249, 3: 225, 4: 249, 5: 246, 6: 239, 7: 250, 8: 246, 9: 149}\n",
            "Client 7, Correct_pred_per_label: {0: 246, 1: 246, 2: 246, 3: 249, 4: 239, 5: 247, 6: 248, 7: 233, 8: 167, 9: 172}\n",
            "Clusters Glob Acc: [tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300)]\n",
            "###### ROUND 10 ######\n",
            "Clients [9 6 1 0 8 5 3 4 2 7]\n",
            "Client 9, Select Cluster: 0\n",
            "acc clusters: [tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300)]\n",
            "Client 6, Select Cluster: 0\n",
            "acc clusters: [tensor(99.2266), tensor(99.2266), tensor(99.2266), tensor(99.2266), tensor(99.2266), tensor(99.2266), tensor(99.2266), tensor(99.2266), tensor(99.2266), tensor(99.2266)]\n",
            "Client 1, Select Cluster: 0\n",
            "acc clusters: [tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300)]\n",
            "Client 0, Select Cluster: 0\n",
            "acc clusters: [tensor(99.0657), tensor(99.0657), tensor(99.0657), tensor(99.0657), tensor(99.0657), tensor(99.0657), tensor(99.0657), tensor(99.0657), tensor(99.0657), tensor(99.0657)]\n",
            "Client 8, Select Cluster: 0\n",
            "acc clusters: [tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300)]\n",
            "Client 5, Select Cluster: 0\n",
            "acc clusters: [tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300)]\n",
            "Client 3, Select Cluster: 0\n",
            "acc clusters: [tensor(99.0657), tensor(99.0657), tensor(99.0657), tensor(99.0657), tensor(99.0657), tensor(99.0657), tensor(99.0657), tensor(99.0657), tensor(99.0657), tensor(99.0657)]\n",
            "Client 4, Select Cluster: 0\n",
            "acc clusters: [tensor(98.8209), tensor(98.8209), tensor(98.8209), tensor(98.8209), tensor(98.8209), tensor(98.8209), tensor(98.8209), tensor(98.8209), tensor(98.8209), tensor(98.8209)]\n",
            "Client 2, Select Cluster: 0\n",
            "acc clusters: [tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300), tensor(98.8300)]\n",
            "Client 7, Select Cluster: 0\n",
            "acc clusters: [tensor(99.2560), tensor(99.2560), tensor(99.2560), tensor(99.2560), tensor(99.2560), tensor(99.2560), tensor(99.2560), tensor(99.2560), tensor(99.2560), tensor(99.2560)]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.005\n",
            "AVG Init Test Loss: 0.038, AVG Init Test Acc: 98.959\n",
            "AVG Final Test Loss: 0.094, AVG Final Test Acc: 97.652\n",
            "Round 10 Upload Cost: 1.777 MB\n",
            "Round 10 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 9, Count: 0, Labels: {0: 207, 1: 492, 2: 9, 3: 1267, 4: 682, 5: 621, 6: 91, 7: 562, 8: 12, 9: 1441}\n",
            "Client 6, Count: 0, Labels: {0: 2271, 1: 42, 2: 1022, 3: 158, 4: 1480, 5: 21, 6: 283, 7: 2024}\n",
            "Client 1, Count: 0, Labels: {0: 11, 1: 906, 2: 1645, 3: 94, 4: 54, 5: 213, 6: 838, 7: 3, 8: 964, 9: 183}\n",
            "Client 0, Count: 0, Labels: {0: 433, 1: 402, 2: 694, 3: 641, 4: 1746, 5: 1340, 6: 259, 7: 301, 8: 846}\n",
            "Client 8, Count: 0, Labels: {0: 558, 1: 1832, 2: 77, 3: 396, 4: 39, 5: 42, 6: 1007, 7: 746, 8: 237, 9: 1275}\n",
            "Client 5, Count: 0, Labels: {0: 218, 1: 1354, 2: 183, 3: 871, 4: 387, 5: 1466, 6: 44, 7: 14, 8: 128, 9: 2061}\n",
            "Client 3, Count: 0, Labels: {0: 1250, 1: 125, 2: 1374, 3: 22, 4: 928, 5: 111, 6: 331, 7: 1602, 8: 468}\n",
            "Client 4, Count: 0, Labels: {0: 758, 1: 3, 2: 298, 4: 510, 5: 86, 6: 568, 7: 114, 8: 3176, 9: 213}\n",
            "Client 2, Count: 0, Labels: {0: 103, 1: 1494, 2: 382, 3: 2, 4: 13, 5: 404, 6: 464, 7: 899, 8: 20, 9: 776}\n",
            "Client 7, Count: 0, Labels: {0: 114, 1: 92, 2: 274, 3: 2680, 4: 3, 5: 1117, 6: 2033}\n",
            "\n",
            "Client 9, Correct_pred_per_label: {0: 247, 1: 249, 2: 219, 3: 249, 4: 250, 5: 245, 6: 237, 7: 246, 8: 220, 9: 247}\n",
            "Client 6, Correct_pred_per_label: {0: 249, 1: 244, 2: 249, 3: 249, 4: 250, 5: 242, 6: 245, 7: 250, 8: 192, 9: 47}\n",
            "Client 1, Correct_pred_per_label: {0: 243, 1: 250, 2: 250, 3: 246, 4: 243, 5: 248, 6: 249, 7: 218, 8: 249, 9: 245}\n",
            "Client 0, Correct_pred_per_label: {0: 248, 1: 248, 2: 250, 3: 248, 4: 250, 5: 247, 6: 239, 7: 246, 8: 246, 9: 71}\n",
            "Client 8, Correct_pred_per_label: {0: 248, 1: 248, 2: 242, 3: 249, 4: 232, 5: 236, 6: 249, 7: 248, 8: 240, 9: 245}\n",
            "Client 5, Correct_pred_per_label: {0: 246, 1: 249, 2: 246, 3: 247, 4: 249, 5: 248, 6: 232, 7: 229, 8: 233, 9: 248}\n",
            "Client 3, Correct_pred_per_label: {0: 248, 1: 245, 2: 249, 3: 231, 4: 249, 5: 247, 6: 241, 7: 250, 8: 244, 9: 138}\n",
            "Client 4, Correct_pred_per_label: {0: 249, 1: 228, 2: 248, 3: 224, 4: 249, 5: 247, 6: 247, 7: 246, 8: 250, 9: 244}\n",
            "Client 2, Correct_pred_per_label: {0: 244, 1: 248, 2: 249, 3: 228, 4: 225, 5: 248, 6: 246, 7: 249, 8: 229, 9: 248}\n",
            "Client 7, Correct_pred_per_label: {0: 246, 1: 244, 2: 246, 3: 249, 4: 241, 5: 247, 6: 248, 7: 233, 8: 160, 9: 203}\n",
            "Clusters Glob Acc: [tensor(98.8600), tensor(98.8600), tensor(98.8600), tensor(98.8600), tensor(98.8600), tensor(98.8600), tensor(98.8600), tensor(98.8600), tensor(98.8600), tensor(98.8600)]\n",
            "Train Loss: 0.00014708681295575975, Test_loss: 0.09423857116542655\n",
            "Train Acc: 100.0, Test Acc: 97.65171813964844\n",
            "Best Clients AVG Acc: 97.71153259277344\n",
            "Best Global Model Acc: 98.86000061035156\n",
            "Total_clusters: 90, Avg clusters per round: 10.0\n",
            "Avg clustering error: 0.02, Avg clustering acc: 0.998\n",
            "Avg clustering error: 0.02, Avg clustering acc: 0.998\n"
          ]
        }
      ],
      "source": [
        "! bash flis_dc.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGJ03o0_Cirh"
      },
      "source": [
        "#### Fashion-MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZp9Y2aJCoBk",
        "outputId": "eb5adc0a-2f47-44de-d681-bf3b91609af2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ../../data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n",
            "100%|██████████| 26421880/26421880 [00:02<00:00, 11692267.96it/s]\n",
            "Extracting ../../data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ../../data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ../../data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n",
            "100%|██████████| 29515/29515 [00:00<00:00, 175684.54it/s]\n",
            "Extracting ../../data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ../../data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ../../data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "100%|██████████| 4422102/4422102 [00:05<00:00, 751175.56it/s] \n",
            "Extracting ../../data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ../../data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ../../data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "100%|██████████| 5148/5148 [00:00<00:00, 36473440.86it/s]\n",
            "Extracting ../../data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ../../data/FashionMNIST/raw\n",
            "\n",
            "partition: noniid-labeldir\n",
            "Data statistics Train:\n",
            " {0: {0: 1756, 2: 2036, 3: 682, 4: 233, 5: 1605}, 1: {0: 80, 1: 32, 2: 2, 3: 1, 4: 1, 5: 42, 6: 3224, 7: 1003, 8: 38, 9: 1592}, 2: {0: 43, 1: 1239, 2: 454, 3: 710, 4: 15, 5: 576, 6: 288, 7: 16, 8: 469, 9: 402}, 3: {0: 506, 1: 2044, 2: 8, 3: 169, 4: 1959, 5: 8, 6: 83, 7: 1912}, 4: {0: 12, 1: 2, 2: 517, 3: 106, 4: 742, 5: 1744, 6: 1981, 7: 400, 8: 2341}, 5: {0: 9, 1: 13, 2: 659, 3: 1159, 4: 751, 5: 834, 6: 26, 7: 925, 8: 199, 9: 307}, 6: {0: 84, 1: 1221, 2: 656, 3: 4, 4: 1316, 5: 399, 6: 29, 7: 615, 8: 1291, 9: 282}, 7: {0: 2580, 1: 204, 2: 17, 3: 1039, 4: 6, 5: 32, 6: 213, 7: 2, 8: 733, 9: 154}, 8: {0: 891, 1: 82, 2: 1347, 3: 1198, 4: 235, 5: 7, 6: 90, 8: 533, 9: 141}, 9: {0: 39, 1: 1163, 2: 304, 3: 932, 4: 742, 5: 753, 6: 66, 7: 1127, 8: 396, 9: 3122}} \n",
            "\n",
            "Data statistics Test:\n",
            " {0: {0: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000}, 1: {0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000, 9: 1000}, 2: {0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000, 9: 1000}, 3: {0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000}, 4: {0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000}, 5: {0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000, 9: 1000}, 6: {0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000, 9: 1000}, 7: {0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000, 9: 1000}, 8: {0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 8: 1000, 9: 1000}, 9: {0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000, 9: 1000}} \n",
            "\n",
            "len train_ds_global: 60000\n",
            "len test_ds_global: 10000\n",
            "Shared data has label: 0, 250 samples\n",
            "Shared data has label: 1, 250 samples\n",
            "Shared data has label: 2, 250 samples\n",
            "Shared data has label: 3, 250 samples\n",
            "Shared data has label: 4, 250 samples\n",
            "Shared data has label: 5, 250 samples\n",
            "Shared data has label: 6, 250 samples\n",
            "Shared data has label: 7, 250 samples\n",
            "Shared data has label: 8, 250 samples\n",
            "Shared data has label: 9, 250 samples\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "MODEL: simple-cnn, Dataset: fmnist\n",
            "SimpleCNNMNIST(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "conv1.weight torch.Size([6, 1, 5, 5])\n",
            "conv1.bias torch.Size([6])\n",
            "conv2.weight torch.Size([16, 6, 5, 5])\n",
            "conv2.bias torch.Size([16])\n",
            "fc1.weight torch.Size([120, 256])\n",
            "fc1.bias torch.Size([120])\n",
            "fc2.weight torch.Size([84, 120])\n",
            "fc2.bias torch.Size([84])\n",
            "fc3.weight torch.Size([10, 84])\n",
            "fc3.bias torch.Size([10])\n",
            "44426\n",
            "###### ROUND 1 ######\n",
            "Clients [2 3 0 1 8 5 7 9 4 6]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.329\n",
            "AVG Init Test Loss: 2.303, AVG Init Test Acc: 11.575\n",
            "AVG Final Test Loss: 1.416, AVG Final Test Acc: 66.807\n",
            "Round 1 Upload Cost: 1.777 MB\n",
            "Round 1 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 2, Count: 0, Labels: {0: 43, 1: 1239, 2: 454, 3: 710, 4: 15, 5: 576, 6: 288, 7: 16, 8: 469, 9: 402}\n",
            "Client 3, Count: 0, Labels: {0: 506, 1: 2044, 2: 8, 3: 169, 4: 1959, 5: 8, 6: 83, 7: 1912}\n",
            "Client 0, Count: 0, Labels: {0: 1756, 2: 2036, 3: 682, 4: 233, 5: 1605}\n",
            "Client 1, Count: 0, Labels: {0: 80, 1: 32, 2: 2, 3: 1, 4: 1, 5: 42, 6: 3224, 7: 1003, 8: 38, 9: 1592}\n",
            "Client 8, Count: 0, Labels: {0: 891, 1: 82, 2: 1347, 3: 1198, 4: 235, 5: 7, 6: 90, 8: 533, 9: 141}\n",
            "Client 5, Count: 0, Labels: {0: 9, 1: 13, 2: 659, 3: 1159, 4: 751, 5: 834, 6: 26, 7: 925, 8: 199, 9: 307}\n",
            "Client 7, Count: 0, Labels: {0: 2580, 1: 204, 2: 17, 3: 1039, 4: 6, 5: 32, 6: 213, 7: 2, 8: 733, 9: 154}\n",
            "Client 9, Count: 0, Labels: {0: 39, 1: 1163, 2: 304, 3: 932, 4: 742, 5: 753, 6: 66, 7: 1127, 8: 396, 9: 3122}\n",
            "Client 4, Count: 0, Labels: {0: 12, 1: 2, 2: 517, 3: 106, 4: 742, 5: 1744, 6: 1981, 7: 400, 8: 2341}\n",
            "Client 6, Count: 0, Labels: {0: 84, 1: 1221, 2: 656, 3: 4, 4: 1316, 5: 399, 6: 29, 7: 615, 8: 1291, 9: 282}\n",
            "\n",
            "Client 2, Correct_pred_per_label: {0: 109, 1: 242, 2: 184, 3: 239, 4: 0, 5: 236, 6: 129, 7: 40, 8: 241, 9: 248}\n",
            "Client 3, Correct_pred_per_label: {0: 225, 1: 247, 2: 0, 3: 191, 4: 245, 5: 0, 6: 6, 7: 250, 8: 0, 9: 0}\n",
            "Client 0, Correct_pred_per_label: {0: 231, 1: 0, 2: 220, 3: 223, 4: 125, 5: 250, 6: 0, 7: 0, 8: 0, 9: 0}\n",
            "Client 1, Correct_pred_per_label: {0: 2, 1: 202, 2: 0, 3: 0, 4: 0, 5: 87, 6: 250, 7: 229, 8: 159, 9: 242}\n",
            "Client 8, Correct_pred_per_label: {0: 216, 1: 232, 2: 237, 3: 239, 4: 31, 5: 70, 6: 0, 7: 0, 8: 238, 9: 248}\n",
            "Client 5, Correct_pred_per_label: {0: 0, 1: 189, 2: 232, 3: 244, 4: 141, 5: 243, 6: 0, 7: 240, 8: 226, 9: 212}\n",
            "Client 7, Correct_pred_per_label: {0: 235, 1: 240, 2: 1, 3: 237, 4: 0, 5: 192, 6: 138, 7: 0, 8: 246, 9: 246}\n",
            "Client 9, Correct_pred_per_label: {0: 139, 1: 246, 2: 110, 3: 230, 4: 240, 5: 223, 6: 14, 7: 226, 8: 242, 9: 245}\n",
            "Client 4, Correct_pred_per_label: {0: 0, 1: 0, 2: 166, 3: 160, 4: 147, 5: 249, 6: 236, 7: 223, 8: 245, 9: 0}\n",
            "Client 6, Correct_pred_per_label: {0: 185, 1: 246, 2: 173, 3: 0, 4: 235, 5: 229, 6: 0, 7: 241, 8: 248, 9: 218}\n",
            "\n",
            "Similarity Matrix: \n",
            " [[1.0000 0.0368 0.4172 0.2760 0.4204 0.4200 0.3368 0.3840 0.4820 0.3700]\n",
            " [0.0368 1.0000 0.4356 0.2416 0.4692 0.3496 0.3700 0.4912 0.2692 0.4008]\n",
            " [0.4172 0.4356 1.0000 0.2552 0.4480 0.6220 0.5112 0.6728 0.6972 0.6332]\n",
            " [0.2760 0.2416 0.2552 1.0000 0.2624 0.3508 0.4812 0.3192 0.3036 0.5552]\n",
            " [0.4204 0.4692 0.4480 0.2624 1.0000 0.4928 0.4312 0.3472 0.2848 0.4604]\n",
            " [0.4200 0.3496 0.6220 0.3508 0.4928 1.0000 0.6260 0.4412 0.5932 0.7016]\n",
            " [0.3368 0.3700 0.5112 0.4812 0.4312 0.6260 1.0000 0.4560 0.5072 0.7616]\n",
            " [0.3840 0.4912 0.6728 0.3192 0.3472 0.4412 0.4560 1.0000 0.6316 0.5460]\n",
            " [0.4820 0.2692 0.6972 0.3036 0.2848 0.5932 0.5072 0.6316 1.0000 0.5604]\n",
            " [0.3700 0.4008 0.6332 0.5552 0.4604 0.7016 0.7616 0.5460 0.5604 1.0000]]\n",
            "\n",
            "Cluster {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5], 6: [6], 7: [7], 8: [8], 9: [9]}\n",
            "###### ROUND 2 ######\n",
            "Clients [7 4 8 6 9 0 1 3 2 5]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.147\n",
            "AVG Init Test Loss: 1.416, AVG Init Test Acc: 66.807\n",
            "AVG Final Test Loss: 1.282, AVG Final Test Acc: 69.701\n",
            "Round 2 Upload Cost: 1.777 MB\n",
            "Round 2 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 7, Count: 0, Labels: {0: 2580, 1: 204, 2: 17, 3: 1039, 4: 6, 5: 32, 6: 213, 7: 2, 8: 733, 9: 154}\n",
            "Client 4, Count: 0, Labels: {0: 12, 1: 2, 2: 517, 3: 106, 4: 742, 5: 1744, 6: 1981, 7: 400, 8: 2341}\n",
            "Client 8, Count: 0, Labels: {0: 891, 1: 82, 2: 1347, 3: 1198, 4: 235, 5: 7, 6: 90, 8: 533, 9: 141}\n",
            "Client 6, Count: 0, Labels: {0: 84, 1: 1221, 2: 656, 3: 4, 4: 1316, 5: 399, 6: 29, 7: 615, 8: 1291, 9: 282}\n",
            "Client 9, Count: 0, Labels: {0: 39, 1: 1163, 2: 304, 3: 932, 4: 742, 5: 753, 6: 66, 7: 1127, 8: 396, 9: 3122}\n",
            "Client 0, Count: 0, Labels: {0: 1756, 2: 2036, 3: 682, 4: 233, 5: 1605}\n",
            "Client 1, Count: 0, Labels: {0: 80, 1: 32, 2: 2, 3: 1, 4: 1, 5: 42, 6: 3224, 7: 1003, 8: 38, 9: 1592}\n",
            "Client 3, Count: 0, Labels: {0: 506, 1: 2044, 2: 8, 3: 169, 4: 1959, 5: 8, 6: 83, 7: 1912}\n",
            "Client 2, Count: 0, Labels: {0: 43, 1: 1239, 2: 454, 3: 710, 4: 15, 5: 576, 6: 288, 7: 16, 8: 469, 9: 402}\n",
            "Client 5, Count: 0, Labels: {0: 9, 1: 13, 2: 659, 3: 1159, 4: 751, 5: 834, 6: 26, 7: 925, 8: 199, 9: 307}\n",
            "\n",
            "Client 7, Correct_pred_per_label: {0: 235, 1: 240, 2: 1, 3: 237, 4: 0, 5: 192, 6: 138, 7: 0, 8: 246, 9: 246}\n",
            "Client 4, Correct_pred_per_label: {0: 0, 1: 0, 2: 166, 3: 160, 4: 147, 5: 249, 6: 236, 7: 223, 8: 245, 9: 0}\n",
            "Client 8, Correct_pred_per_label: {0: 216, 1: 232, 2: 237, 3: 239, 4: 31, 5: 70, 6: 0, 7: 0, 8: 238, 9: 248}\n",
            "Client 6, Correct_pred_per_label: {0: 185, 1: 246, 2: 173, 3: 0, 4: 235, 5: 229, 6: 0, 7: 241, 8: 248, 9: 218}\n",
            "Client 9, Correct_pred_per_label: {0: 139, 1: 246, 2: 110, 3: 230, 4: 240, 5: 223, 6: 14, 7: 226, 8: 242, 9: 245}\n",
            "Client 0, Correct_pred_per_label: {0: 231, 1: 0, 2: 220, 3: 223, 4: 125, 5: 250, 6: 0, 7: 0, 8: 0, 9: 0}\n",
            "Client 1, Correct_pred_per_label: {0: 2, 1: 202, 2: 0, 3: 0, 4: 0, 5: 87, 6: 250, 7: 229, 8: 159, 9: 242}\n",
            "Client 3, Correct_pred_per_label: {0: 225, 1: 247, 2: 0, 3: 191, 4: 245, 5: 0, 6: 6, 7: 250, 8: 0, 9: 0}\n",
            "Client 2, Correct_pred_per_label: {0: 109, 1: 242, 2: 184, 3: 239, 4: 0, 5: 236, 6: 129, 7: 40, 8: 241, 9: 248}\n",
            "Client 5, Correct_pred_per_label: {0: 0, 1: 189, 2: 232, 3: 244, 4: 141, 5: 243, 6: 0, 7: 240, 8: 226, 9: 212}\n",
            "\n",
            "Similarity Matrix: \n",
            " [[1.0000 0.0368 0.4172 0.2760 0.4204 0.4200 0.3368 0.3840 0.4820 0.3700]\n",
            " [0.0368 1.0000 0.4356 0.2416 0.4692 0.3496 0.3700 0.4912 0.2692 0.4008]\n",
            " [0.4172 0.4356 1.0000 0.2552 0.4480 0.6220 0.5112 0.6728 0.6972 0.6332]\n",
            " [0.2760 0.2416 0.2552 1.0000 0.2624 0.3508 0.4812 0.3192 0.3036 0.5552]\n",
            " [0.4204 0.4692 0.4480 0.2624 1.0000 0.4928 0.4312 0.3472 0.2848 0.4604]\n",
            " [0.4200 0.3496 0.6220 0.3508 0.4928 1.0000 0.6260 0.4412 0.5932 0.7016]\n",
            " [0.3368 0.3700 0.5112 0.4812 0.4312 0.6260 1.0000 0.4560 0.5072 0.7616]\n",
            " [0.3840 0.4912 0.6728 0.3192 0.3472 0.4412 0.4560 1.0000 0.6316 0.5460]\n",
            " [0.4820 0.2692 0.6972 0.3036 0.2848 0.5932 0.5072 0.6316 1.0000 0.5604]\n",
            " [0.3700 0.4008 0.6332 0.5552 0.4604 0.7016 0.7616 0.5460 0.5604 1.0000]]\n",
            "\n",
            "Cluster {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5], 6: [6], 7: [7], 8: [8], 9: [9]}\n",
            "###### ROUND 3 ######\n",
            "Clients [9 6 4 8 0 7 5 1 2 3]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.083\n",
            "AVG Init Test Loss: 1.282, AVG Init Test Acc: 69.701\n",
            "AVG Final Test Loss: 1.620, AVG Final Test Acc: 70.326\n",
            "Round 3 Upload Cost: 1.777 MB\n",
            "Round 3 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 9, Count: 0, Labels: {0: 39, 1: 1163, 2: 304, 3: 932, 4: 742, 5: 753, 6: 66, 7: 1127, 8: 396, 9: 3122}\n",
            "Client 6, Count: 0, Labels: {0: 84, 1: 1221, 2: 656, 3: 4, 4: 1316, 5: 399, 6: 29, 7: 615, 8: 1291, 9: 282}\n",
            "Client 4, Count: 0, Labels: {0: 12, 1: 2, 2: 517, 3: 106, 4: 742, 5: 1744, 6: 1981, 7: 400, 8: 2341}\n",
            "Client 8, Count: 0, Labels: {0: 891, 1: 82, 2: 1347, 3: 1198, 4: 235, 5: 7, 6: 90, 8: 533, 9: 141}\n",
            "Client 0, Count: 0, Labels: {0: 1756, 2: 2036, 3: 682, 4: 233, 5: 1605}\n",
            "Client 7, Count: 0, Labels: {0: 2580, 1: 204, 2: 17, 3: 1039, 4: 6, 5: 32, 6: 213, 7: 2, 8: 733, 9: 154}\n",
            "Client 5, Count: 0, Labels: {0: 9, 1: 13, 2: 659, 3: 1159, 4: 751, 5: 834, 6: 26, 7: 925, 8: 199, 9: 307}\n",
            "Client 1, Count: 0, Labels: {0: 80, 1: 32, 2: 2, 3: 1, 4: 1, 5: 42, 6: 3224, 7: 1003, 8: 38, 9: 1592}\n",
            "Client 2, Count: 0, Labels: {0: 43, 1: 1239, 2: 454, 3: 710, 4: 15, 5: 576, 6: 288, 7: 16, 8: 469, 9: 402}\n",
            "Client 3, Count: 0, Labels: {0: 506, 1: 2044, 2: 8, 3: 169, 4: 1959, 5: 8, 6: 83, 7: 1912}\n",
            "\n",
            "Client 9, Correct_pred_per_label: {0: 139, 1: 246, 2: 110, 3: 230, 4: 240, 5: 223, 6: 14, 7: 226, 8: 242, 9: 245}\n",
            "Client 6, Correct_pred_per_label: {0: 185, 1: 246, 2: 173, 3: 0, 4: 235, 5: 229, 6: 0, 7: 241, 8: 248, 9: 218}\n",
            "Client 4, Correct_pred_per_label: {0: 0, 1: 0, 2: 166, 3: 160, 4: 147, 5: 249, 6: 236, 7: 223, 8: 245, 9: 0}\n",
            "Client 8, Correct_pred_per_label: {0: 216, 1: 232, 2: 237, 3: 239, 4: 31, 5: 70, 6: 0, 7: 0, 8: 238, 9: 248}\n",
            "Client 0, Correct_pred_per_label: {0: 231, 1: 0, 2: 220, 3: 223, 4: 125, 5: 250, 6: 0, 7: 0, 8: 0, 9: 0}\n",
            "Client 7, Correct_pred_per_label: {0: 235, 1: 240, 2: 1, 3: 237, 4: 0, 5: 192, 6: 138, 7: 0, 8: 246, 9: 246}\n",
            "Client 5, Correct_pred_per_label: {0: 0, 1: 189, 2: 232, 3: 244, 4: 141, 5: 243, 6: 0, 7: 240, 8: 226, 9: 212}\n",
            "Client 1, Correct_pred_per_label: {0: 2, 1: 202, 2: 0, 3: 0, 4: 0, 5: 87, 6: 250, 7: 229, 8: 159, 9: 242}\n",
            "Client 2, Correct_pred_per_label: {0: 109, 1: 242, 2: 184, 3: 239, 4: 0, 5: 236, 6: 129, 7: 40, 8: 241, 9: 248}\n",
            "Client 3, Correct_pred_per_label: {0: 225, 1: 247, 2: 0, 3: 191, 4: 245, 5: 0, 6: 6, 7: 250, 8: 0, 9: 0}\n",
            "\n",
            "Similarity Matrix: \n",
            " [[1.0000 0.0368 0.4172 0.2760 0.4204 0.4200 0.3368 0.3840 0.4820 0.3700]\n",
            " [0.0368 1.0000 0.4356 0.2416 0.4692 0.3496 0.3700 0.4912 0.2692 0.4008]\n",
            " [0.4172 0.4356 1.0000 0.2552 0.4480 0.6220 0.5112 0.6728 0.6972 0.6332]\n",
            " [0.2760 0.2416 0.2552 1.0000 0.2624 0.3508 0.4812 0.3192 0.3036 0.5552]\n",
            " [0.4204 0.4692 0.4480 0.2624 1.0000 0.4928 0.4312 0.3472 0.2848 0.4604]\n",
            " [0.4200 0.3496 0.6220 0.3508 0.4928 1.0000 0.6260 0.4412 0.5932 0.7016]\n",
            " [0.3368 0.3700 0.5112 0.4812 0.4312 0.6260 1.0000 0.4560 0.5072 0.7616]\n",
            " [0.3840 0.4912 0.6728 0.3192 0.3472 0.4412 0.4560 1.0000 0.6316 0.5460]\n",
            " [0.4820 0.2692 0.6972 0.3036 0.2848 0.5932 0.5072 0.6316 1.0000 0.5604]\n",
            " [0.3700 0.4008 0.6332 0.5552 0.4604 0.7016 0.7616 0.5460 0.5604 1.0000]]\n",
            "\n",
            "Cluster {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5], 6: [6], 7: [7], 8: [8], 9: [9]}\n",
            "###### ROUND 4 ######\n",
            "Clients [4 6 9 3 7 8 0 5 2 1]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.047\n",
            "AVG Init Test Loss: 1.620, AVG Init Test Acc: 70.326\n",
            "AVG Final Test Loss: 1.909, AVG Final Test Acc: 71.892\n",
            "Round 4 Upload Cost: 1.777 MB\n",
            "Round 4 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 4, Count: 0, Labels: {0: 12, 1: 2, 2: 517, 3: 106, 4: 742, 5: 1744, 6: 1981, 7: 400, 8: 2341}\n",
            "Client 6, Count: 0, Labels: {0: 84, 1: 1221, 2: 656, 3: 4, 4: 1316, 5: 399, 6: 29, 7: 615, 8: 1291, 9: 282}\n",
            "Client 9, Count: 0, Labels: {0: 39, 1: 1163, 2: 304, 3: 932, 4: 742, 5: 753, 6: 66, 7: 1127, 8: 396, 9: 3122}\n",
            "Client 3, Count: 0, Labels: {0: 506, 1: 2044, 2: 8, 3: 169, 4: 1959, 5: 8, 6: 83, 7: 1912}\n",
            "Client 7, Count: 0, Labels: {0: 2580, 1: 204, 2: 17, 3: 1039, 4: 6, 5: 32, 6: 213, 7: 2, 8: 733, 9: 154}\n",
            "Client 8, Count: 0, Labels: {0: 891, 1: 82, 2: 1347, 3: 1198, 4: 235, 5: 7, 6: 90, 8: 533, 9: 141}\n",
            "Client 0, Count: 0, Labels: {0: 1756, 2: 2036, 3: 682, 4: 233, 5: 1605}\n",
            "Client 5, Count: 0, Labels: {0: 9, 1: 13, 2: 659, 3: 1159, 4: 751, 5: 834, 6: 26, 7: 925, 8: 199, 9: 307}\n",
            "Client 2, Count: 0, Labels: {0: 43, 1: 1239, 2: 454, 3: 710, 4: 15, 5: 576, 6: 288, 7: 16, 8: 469, 9: 402}\n",
            "Client 1, Count: 0, Labels: {0: 80, 1: 32, 2: 2, 3: 1, 4: 1, 5: 42, 6: 3224, 7: 1003, 8: 38, 9: 1592}\n",
            "\n",
            "Client 4, Correct_pred_per_label: {0: 0, 1: 0, 2: 166, 3: 160, 4: 147, 5: 249, 6: 236, 7: 223, 8: 245, 9: 0}\n",
            "Client 6, Correct_pred_per_label: {0: 185, 1: 246, 2: 173, 3: 0, 4: 235, 5: 229, 6: 0, 7: 241, 8: 248, 9: 218}\n",
            "Client 9, Correct_pred_per_label: {0: 139, 1: 246, 2: 110, 3: 230, 4: 240, 5: 223, 6: 14, 7: 226, 8: 242, 9: 245}\n",
            "Client 3, Correct_pred_per_label: {0: 225, 1: 247, 2: 0, 3: 191, 4: 245, 5: 0, 6: 6, 7: 250, 8: 0, 9: 0}\n",
            "Client 7, Correct_pred_per_label: {0: 235, 1: 240, 2: 1, 3: 237, 4: 0, 5: 192, 6: 138, 7: 0, 8: 246, 9: 246}\n",
            "Client 8, Correct_pred_per_label: {0: 216, 1: 232, 2: 237, 3: 239, 4: 31, 5: 70, 6: 0, 7: 0, 8: 238, 9: 248}\n",
            "Client 0, Correct_pred_per_label: {0: 231, 1: 0, 2: 220, 3: 223, 4: 125, 5: 250, 6: 0, 7: 0, 8: 0, 9: 0}\n",
            "Client 5, Correct_pred_per_label: {0: 0, 1: 189, 2: 232, 3: 244, 4: 141, 5: 243, 6: 0, 7: 240, 8: 226, 9: 212}\n",
            "Client 2, Correct_pred_per_label: {0: 109, 1: 242, 2: 184, 3: 239, 4: 0, 5: 236, 6: 129, 7: 40, 8: 241, 9: 248}\n",
            "Client 1, Correct_pred_per_label: {0: 2, 1: 202, 2: 0, 3: 0, 4: 0, 5: 87, 6: 250, 7: 229, 8: 159, 9: 242}\n",
            "\n",
            "Similarity Matrix: \n",
            " [[1.0000 0.0368 0.4172 0.2760 0.4204 0.4200 0.3368 0.3840 0.4820 0.3700]\n",
            " [0.0368 1.0000 0.4356 0.2416 0.4692 0.3496 0.3700 0.4912 0.2692 0.4008]\n",
            " [0.4172 0.4356 1.0000 0.2552 0.4480 0.6220 0.5112 0.6728 0.6972 0.6332]\n",
            " [0.2760 0.2416 0.2552 1.0000 0.2624 0.3508 0.4812 0.3192 0.3036 0.5552]\n",
            " [0.4204 0.4692 0.4480 0.2624 1.0000 0.4928 0.4312 0.3472 0.2848 0.4604]\n",
            " [0.4200 0.3496 0.6220 0.3508 0.4928 1.0000 0.6260 0.4412 0.5932 0.7016]\n",
            " [0.3368 0.3700 0.5112 0.4812 0.4312 0.6260 1.0000 0.4560 0.5072 0.7616]\n",
            " [0.3840 0.4912 0.6728 0.3192 0.3472 0.4412 0.4560 1.0000 0.6316 0.5460]\n",
            " [0.4820 0.2692 0.6972 0.3036 0.2848 0.5932 0.5072 0.6316 1.0000 0.5604]\n",
            " [0.3700 0.4008 0.6332 0.5552 0.4604 0.7016 0.7616 0.5460 0.5604 1.0000]]\n",
            "\n",
            "Cluster {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5], 6: [6], 7: [7], 8: [8], 9: [9]}\n",
            "###### ROUND 5 ######\n",
            "Clients [7 8 6 2 4 1 9 3 5 0]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.028\n",
            "AVG Init Test Loss: 1.909, AVG Init Test Acc: 71.892\n",
            "AVG Final Test Loss: 2.154, AVG Final Test Acc: 72.282\n",
            "Round 5 Upload Cost: 1.777 MB\n",
            "Round 5 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 7, Count: 0, Labels: {0: 2580, 1: 204, 2: 17, 3: 1039, 4: 6, 5: 32, 6: 213, 7: 2, 8: 733, 9: 154}\n",
            "Client 8, Count: 0, Labels: {0: 891, 1: 82, 2: 1347, 3: 1198, 4: 235, 5: 7, 6: 90, 8: 533, 9: 141}\n",
            "Client 6, Count: 0, Labels: {0: 84, 1: 1221, 2: 656, 3: 4, 4: 1316, 5: 399, 6: 29, 7: 615, 8: 1291, 9: 282}\n",
            "Client 2, Count: 0, Labels: {0: 43, 1: 1239, 2: 454, 3: 710, 4: 15, 5: 576, 6: 288, 7: 16, 8: 469, 9: 402}\n",
            "Client 4, Count: 0, Labels: {0: 12, 1: 2, 2: 517, 3: 106, 4: 742, 5: 1744, 6: 1981, 7: 400, 8: 2341}\n",
            "Client 1, Count: 0, Labels: {0: 80, 1: 32, 2: 2, 3: 1, 4: 1, 5: 42, 6: 3224, 7: 1003, 8: 38, 9: 1592}\n",
            "Client 9, Count: 0, Labels: {0: 39, 1: 1163, 2: 304, 3: 932, 4: 742, 5: 753, 6: 66, 7: 1127, 8: 396, 9: 3122}\n",
            "Client 3, Count: 0, Labels: {0: 506, 1: 2044, 2: 8, 3: 169, 4: 1959, 5: 8, 6: 83, 7: 1912}\n",
            "Client 5, Count: 0, Labels: {0: 9, 1: 13, 2: 659, 3: 1159, 4: 751, 5: 834, 6: 26, 7: 925, 8: 199, 9: 307}\n",
            "Client 0, Count: 0, Labels: {0: 1756, 2: 2036, 3: 682, 4: 233, 5: 1605}\n",
            "\n",
            "Client 7, Correct_pred_per_label: {0: 235, 1: 240, 2: 1, 3: 237, 4: 0, 5: 192, 6: 138, 7: 0, 8: 246, 9: 246}\n",
            "Client 8, Correct_pred_per_label: {0: 216, 1: 232, 2: 237, 3: 239, 4: 31, 5: 70, 6: 0, 7: 0, 8: 238, 9: 248}\n",
            "Client 6, Correct_pred_per_label: {0: 185, 1: 246, 2: 173, 3: 0, 4: 235, 5: 229, 6: 0, 7: 241, 8: 248, 9: 218}\n",
            "Client 2, Correct_pred_per_label: {0: 109, 1: 242, 2: 184, 3: 239, 4: 0, 5: 236, 6: 129, 7: 40, 8: 241, 9: 248}\n",
            "Client 4, Correct_pred_per_label: {0: 0, 1: 0, 2: 166, 3: 160, 4: 147, 5: 249, 6: 236, 7: 223, 8: 245, 9: 0}\n",
            "Client 1, Correct_pred_per_label: {0: 2, 1: 202, 2: 0, 3: 0, 4: 0, 5: 87, 6: 250, 7: 229, 8: 159, 9: 242}\n",
            "Client 9, Correct_pred_per_label: {0: 139, 1: 246, 2: 110, 3: 230, 4: 240, 5: 223, 6: 14, 7: 226, 8: 242, 9: 245}\n",
            "Client 3, Correct_pred_per_label: {0: 225, 1: 247, 2: 0, 3: 191, 4: 245, 5: 0, 6: 6, 7: 250, 8: 0, 9: 0}\n",
            "Client 5, Correct_pred_per_label: {0: 0, 1: 189, 2: 232, 3: 244, 4: 141, 5: 243, 6: 0, 7: 240, 8: 226, 9: 212}\n",
            "Client 0, Correct_pred_per_label: {0: 231, 1: 0, 2: 220, 3: 223, 4: 125, 5: 250, 6: 0, 7: 0, 8: 0, 9: 0}\n",
            "\n",
            "Similarity Matrix: \n",
            " [[1.0000 0.0368 0.4172 0.2760 0.4204 0.4200 0.3368 0.3840 0.4820 0.3700]\n",
            " [0.0368 1.0000 0.4356 0.2416 0.4692 0.3496 0.3700 0.4912 0.2692 0.4008]\n",
            " [0.4172 0.4356 1.0000 0.2552 0.4480 0.6220 0.5112 0.6728 0.6972 0.6332]\n",
            " [0.2760 0.2416 0.2552 1.0000 0.2624 0.3508 0.4812 0.3192 0.3036 0.5552]\n",
            " [0.4204 0.4692 0.4480 0.2624 1.0000 0.4928 0.4312 0.3472 0.2848 0.4604]\n",
            " [0.4200 0.3496 0.6220 0.3508 0.4928 1.0000 0.6260 0.4412 0.5932 0.7016]\n",
            " [0.3368 0.3700 0.5112 0.4812 0.4312 0.6260 1.0000 0.4560 0.5072 0.7616]\n",
            " [0.3840 0.4912 0.6728 0.3192 0.3472 0.4412 0.4560 1.0000 0.6316 0.5460]\n",
            " [0.4820 0.2692 0.6972 0.3036 0.2848 0.5932 0.5072 0.6316 1.0000 0.5604]\n",
            " [0.3700 0.4008 0.6332 0.5552 0.4604 0.7016 0.7616 0.5460 0.5604 1.0000]]\n",
            "\n",
            "Cluster {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5], 6: [6], 7: [7], 8: [8], 9: [9]}\n",
            "###### ROUND 6 ######\n",
            "Clients [6 5 7 2 0 3 8 1 4 9]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.015\n",
            "AVG Init Test Loss: 2.154, AVG Init Test Acc: 72.282\n",
            "AVG Final Test Loss: 2.683, AVG Final Test Acc: 72.377\n",
            "Round 6 Upload Cost: 1.777 MB\n",
            "Round 6 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 6, Count: 0, Labels: {0: 84, 1: 1221, 2: 656, 3: 4, 4: 1316, 5: 399, 6: 29, 7: 615, 8: 1291, 9: 282}\n",
            "Client 5, Count: 0, Labels: {0: 9, 1: 13, 2: 659, 3: 1159, 4: 751, 5: 834, 6: 26, 7: 925, 8: 199, 9: 307}\n",
            "Client 7, Count: 0, Labels: {0: 2580, 1: 204, 2: 17, 3: 1039, 4: 6, 5: 32, 6: 213, 7: 2, 8: 733, 9: 154}\n",
            "Client 2, Count: 0, Labels: {0: 43, 1: 1239, 2: 454, 3: 710, 4: 15, 5: 576, 6: 288, 7: 16, 8: 469, 9: 402}\n",
            "Client 0, Count: 0, Labels: {0: 1756, 2: 2036, 3: 682, 4: 233, 5: 1605}\n",
            "Client 3, Count: 0, Labels: {0: 506, 1: 2044, 2: 8, 3: 169, 4: 1959, 5: 8, 6: 83, 7: 1912}\n",
            "Client 8, Count: 0, Labels: {0: 891, 1: 82, 2: 1347, 3: 1198, 4: 235, 5: 7, 6: 90, 8: 533, 9: 141}\n",
            "Client 1, Count: 0, Labels: {0: 80, 1: 32, 2: 2, 3: 1, 4: 1, 5: 42, 6: 3224, 7: 1003, 8: 38, 9: 1592}\n",
            "Client 4, Count: 0, Labels: {0: 12, 1: 2, 2: 517, 3: 106, 4: 742, 5: 1744, 6: 1981, 7: 400, 8: 2341}\n",
            "Client 9, Count: 0, Labels: {0: 39, 1: 1163, 2: 304, 3: 932, 4: 742, 5: 753, 6: 66, 7: 1127, 8: 396, 9: 3122}\n",
            "\n",
            "Client 6, Correct_pred_per_label: {0: 185, 1: 246, 2: 173, 3: 0, 4: 235, 5: 229, 6: 0, 7: 241, 8: 248, 9: 218}\n",
            "Client 5, Correct_pred_per_label: {0: 0, 1: 189, 2: 232, 3: 244, 4: 141, 5: 243, 6: 0, 7: 240, 8: 226, 9: 212}\n",
            "Client 7, Correct_pred_per_label: {0: 235, 1: 240, 2: 1, 3: 237, 4: 0, 5: 192, 6: 138, 7: 0, 8: 246, 9: 246}\n",
            "Client 2, Correct_pred_per_label: {0: 109, 1: 242, 2: 184, 3: 239, 4: 0, 5: 236, 6: 129, 7: 40, 8: 241, 9: 248}\n",
            "Client 0, Correct_pred_per_label: {0: 231, 1: 0, 2: 220, 3: 223, 4: 125, 5: 250, 6: 0, 7: 0, 8: 0, 9: 0}\n",
            "Client 3, Correct_pred_per_label: {0: 225, 1: 247, 2: 0, 3: 191, 4: 245, 5: 0, 6: 6, 7: 250, 8: 0, 9: 0}\n",
            "Client 8, Correct_pred_per_label: {0: 216, 1: 232, 2: 237, 3: 239, 4: 31, 5: 70, 6: 0, 7: 0, 8: 238, 9: 248}\n",
            "Client 1, Correct_pred_per_label: {0: 2, 1: 202, 2: 0, 3: 0, 4: 0, 5: 87, 6: 250, 7: 229, 8: 159, 9: 242}\n",
            "Client 4, Correct_pred_per_label: {0: 0, 1: 0, 2: 166, 3: 160, 4: 147, 5: 249, 6: 236, 7: 223, 8: 245, 9: 0}\n",
            "Client 9, Correct_pred_per_label: {0: 139, 1: 246, 2: 110, 3: 230, 4: 240, 5: 223, 6: 14, 7: 226, 8: 242, 9: 245}\n",
            "\n",
            "Similarity Matrix: \n",
            " [[1.0000 0.0368 0.4172 0.2760 0.4204 0.4200 0.3368 0.3840 0.4820 0.3700]\n",
            " [0.0368 1.0000 0.4356 0.2416 0.4692 0.3496 0.3700 0.4912 0.2692 0.4008]\n",
            " [0.4172 0.4356 1.0000 0.2552 0.4480 0.6220 0.5112 0.6728 0.6972 0.6332]\n",
            " [0.2760 0.2416 0.2552 1.0000 0.2624 0.3508 0.4812 0.3192 0.3036 0.5552]\n",
            " [0.4204 0.4692 0.4480 0.2624 1.0000 0.4928 0.4312 0.3472 0.2848 0.4604]\n",
            " [0.4200 0.3496 0.6220 0.3508 0.4928 1.0000 0.6260 0.4412 0.5932 0.7016]\n",
            " [0.3368 0.3700 0.5112 0.4812 0.4312 0.6260 1.0000 0.4560 0.5072 0.7616]\n",
            " [0.3840 0.4912 0.6728 0.3192 0.3472 0.4412 0.4560 1.0000 0.6316 0.5460]\n",
            " [0.4820 0.2692 0.6972 0.3036 0.2848 0.5932 0.5072 0.6316 1.0000 0.5604]\n",
            " [0.3700 0.4008 0.6332 0.5552 0.4604 0.7016 0.7616 0.5460 0.5604 1.0000]]\n",
            "\n",
            "Cluster {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5], 6: [6], 7: [7], 8: [8], 9: [9]}\n",
            "###### ROUND 7 ######\n",
            "Clients [4 6 2 7 3 0 5 9 1 8]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.008\n",
            "AVG Init Test Loss: 2.683, AVG Init Test Acc: 72.377\n",
            "AVG Final Test Loss: 2.915, AVG Final Test Acc: 72.821\n",
            "Round 7 Upload Cost: 1.777 MB\n",
            "Round 7 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 4, Count: 0, Labels: {0: 12, 1: 2, 2: 517, 3: 106, 4: 742, 5: 1744, 6: 1981, 7: 400, 8: 2341}\n",
            "Client 6, Count: 0, Labels: {0: 84, 1: 1221, 2: 656, 3: 4, 4: 1316, 5: 399, 6: 29, 7: 615, 8: 1291, 9: 282}\n",
            "Client 2, Count: 0, Labels: {0: 43, 1: 1239, 2: 454, 3: 710, 4: 15, 5: 576, 6: 288, 7: 16, 8: 469, 9: 402}\n",
            "Client 7, Count: 0, Labels: {0: 2580, 1: 204, 2: 17, 3: 1039, 4: 6, 5: 32, 6: 213, 7: 2, 8: 733, 9: 154}\n",
            "Client 3, Count: 0, Labels: {0: 506, 1: 2044, 2: 8, 3: 169, 4: 1959, 5: 8, 6: 83, 7: 1912}\n",
            "Client 0, Count: 0, Labels: {0: 1756, 2: 2036, 3: 682, 4: 233, 5: 1605}\n",
            "Client 5, Count: 0, Labels: {0: 9, 1: 13, 2: 659, 3: 1159, 4: 751, 5: 834, 6: 26, 7: 925, 8: 199, 9: 307}\n",
            "Client 9, Count: 0, Labels: {0: 39, 1: 1163, 2: 304, 3: 932, 4: 742, 5: 753, 6: 66, 7: 1127, 8: 396, 9: 3122}\n",
            "Client 1, Count: 0, Labels: {0: 80, 1: 32, 2: 2, 3: 1, 4: 1, 5: 42, 6: 3224, 7: 1003, 8: 38, 9: 1592}\n",
            "Client 8, Count: 0, Labels: {0: 891, 1: 82, 2: 1347, 3: 1198, 4: 235, 5: 7, 6: 90, 8: 533, 9: 141}\n",
            "\n",
            "Client 4, Correct_pred_per_label: {0: 0, 1: 0, 2: 166, 3: 160, 4: 147, 5: 249, 6: 236, 7: 223, 8: 245, 9: 0}\n",
            "Client 6, Correct_pred_per_label: {0: 185, 1: 246, 2: 173, 3: 0, 4: 235, 5: 229, 6: 0, 7: 241, 8: 248, 9: 218}\n",
            "Client 2, Correct_pred_per_label: {0: 109, 1: 242, 2: 184, 3: 239, 4: 0, 5: 236, 6: 129, 7: 40, 8: 241, 9: 248}\n",
            "Client 7, Correct_pred_per_label: {0: 235, 1: 240, 2: 1, 3: 237, 4: 0, 5: 192, 6: 138, 7: 0, 8: 246, 9: 246}\n",
            "Client 3, Correct_pred_per_label: {0: 225, 1: 247, 2: 0, 3: 191, 4: 245, 5: 0, 6: 6, 7: 250, 8: 0, 9: 0}\n",
            "Client 0, Correct_pred_per_label: {0: 231, 1: 0, 2: 220, 3: 223, 4: 125, 5: 250, 6: 0, 7: 0, 8: 0, 9: 0}\n",
            "Client 5, Correct_pred_per_label: {0: 0, 1: 189, 2: 232, 3: 244, 4: 141, 5: 243, 6: 0, 7: 240, 8: 226, 9: 212}\n",
            "Client 9, Correct_pred_per_label: {0: 139, 1: 246, 2: 110, 3: 230, 4: 240, 5: 223, 6: 14, 7: 226, 8: 242, 9: 245}\n",
            "Client 1, Correct_pred_per_label: {0: 2, 1: 202, 2: 0, 3: 0, 4: 0, 5: 87, 6: 250, 7: 229, 8: 159, 9: 242}\n",
            "Client 8, Correct_pred_per_label: {0: 216, 1: 232, 2: 237, 3: 239, 4: 31, 5: 70, 6: 0, 7: 0, 8: 238, 9: 248}\n",
            "\n",
            "Similarity Matrix: \n",
            " [[1.0000 0.0368 0.4172 0.2760 0.4204 0.4200 0.3368 0.3840 0.4820 0.3700]\n",
            " [0.0368 1.0000 0.4356 0.2416 0.4692 0.3496 0.3700 0.4912 0.2692 0.4008]\n",
            " [0.4172 0.4356 1.0000 0.2552 0.4480 0.6220 0.5112 0.6728 0.6972 0.6332]\n",
            " [0.2760 0.2416 0.2552 1.0000 0.2624 0.3508 0.4812 0.3192 0.3036 0.5552]\n",
            " [0.4204 0.4692 0.4480 0.2624 1.0000 0.4928 0.4312 0.3472 0.2848 0.4604]\n",
            " [0.4200 0.3496 0.6220 0.3508 0.4928 1.0000 0.6260 0.4412 0.5932 0.7016]\n",
            " [0.3368 0.3700 0.5112 0.4812 0.4312 0.6260 1.0000 0.4560 0.5072 0.7616]\n",
            " [0.3840 0.4912 0.6728 0.3192 0.3472 0.4412 0.4560 1.0000 0.6316 0.5460]\n",
            " [0.4820 0.2692 0.6972 0.3036 0.2848 0.5932 0.5072 0.6316 1.0000 0.5604]\n",
            " [0.3700 0.4008 0.6332 0.5552 0.4604 0.7016 0.7616 0.5460 0.5604 1.0000]]\n",
            "\n",
            "Cluster {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5], 6: [6], 7: [7], 8: [8], 9: [9]}\n",
            "###### ROUND 8 ######\n",
            "Clients [9 4 0 8 3 2 5 6 1 7]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.002\n",
            "AVG Init Test Loss: 2.915, AVG Init Test Acc: 72.821\n",
            "AVG Final Test Loss: 3.239, AVG Final Test Acc: 72.604\n",
            "Round 8 Upload Cost: 1.777 MB\n",
            "Round 8 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 9, Count: 0, Labels: {0: 39, 1: 1163, 2: 304, 3: 932, 4: 742, 5: 753, 6: 66, 7: 1127, 8: 396, 9: 3122}\n",
            "Client 4, Count: 0, Labels: {0: 12, 1: 2, 2: 517, 3: 106, 4: 742, 5: 1744, 6: 1981, 7: 400, 8: 2341}\n",
            "Client 0, Count: 0, Labels: {0: 1756, 2: 2036, 3: 682, 4: 233, 5: 1605}\n",
            "Client 8, Count: 0, Labels: {0: 891, 1: 82, 2: 1347, 3: 1198, 4: 235, 5: 7, 6: 90, 8: 533, 9: 141}\n",
            "Client 3, Count: 0, Labels: {0: 506, 1: 2044, 2: 8, 3: 169, 4: 1959, 5: 8, 6: 83, 7: 1912}\n",
            "Client 2, Count: 0, Labels: {0: 43, 1: 1239, 2: 454, 3: 710, 4: 15, 5: 576, 6: 288, 7: 16, 8: 469, 9: 402}\n",
            "Client 5, Count: 0, Labels: {0: 9, 1: 13, 2: 659, 3: 1159, 4: 751, 5: 834, 6: 26, 7: 925, 8: 199, 9: 307}\n",
            "Client 6, Count: 0, Labels: {0: 84, 1: 1221, 2: 656, 3: 4, 4: 1316, 5: 399, 6: 29, 7: 615, 8: 1291, 9: 282}\n",
            "Client 1, Count: 0, Labels: {0: 80, 1: 32, 2: 2, 3: 1, 4: 1, 5: 42, 6: 3224, 7: 1003, 8: 38, 9: 1592}\n",
            "Client 7, Count: 0, Labels: {0: 2580, 1: 204, 2: 17, 3: 1039, 4: 6, 5: 32, 6: 213, 7: 2, 8: 733, 9: 154}\n",
            "\n",
            "Client 9, Correct_pred_per_label: {0: 139, 1: 246, 2: 110, 3: 230, 4: 240, 5: 223, 6: 14, 7: 226, 8: 242, 9: 245}\n",
            "Client 4, Correct_pred_per_label: {0: 0, 1: 0, 2: 166, 3: 160, 4: 147, 5: 249, 6: 236, 7: 223, 8: 245, 9: 0}\n",
            "Client 0, Correct_pred_per_label: {0: 231, 1: 0, 2: 220, 3: 223, 4: 125, 5: 250, 6: 0, 7: 0, 8: 0, 9: 0}\n",
            "Client 8, Correct_pred_per_label: {0: 216, 1: 232, 2: 237, 3: 239, 4: 31, 5: 70, 6: 0, 7: 0, 8: 238, 9: 248}\n",
            "Client 3, Correct_pred_per_label: {0: 225, 1: 247, 2: 0, 3: 191, 4: 245, 5: 0, 6: 6, 7: 250, 8: 0, 9: 0}\n",
            "Client 2, Correct_pred_per_label: {0: 109, 1: 242, 2: 184, 3: 239, 4: 0, 5: 236, 6: 129, 7: 40, 8: 241, 9: 248}\n",
            "Client 5, Correct_pred_per_label: {0: 0, 1: 189, 2: 232, 3: 244, 4: 141, 5: 243, 6: 0, 7: 240, 8: 226, 9: 212}\n",
            "Client 6, Correct_pred_per_label: {0: 185, 1: 246, 2: 173, 3: 0, 4: 235, 5: 229, 6: 0, 7: 241, 8: 248, 9: 218}\n",
            "Client 1, Correct_pred_per_label: {0: 2, 1: 202, 2: 0, 3: 0, 4: 0, 5: 87, 6: 250, 7: 229, 8: 159, 9: 242}\n",
            "Client 7, Correct_pred_per_label: {0: 235, 1: 240, 2: 1, 3: 237, 4: 0, 5: 192, 6: 138, 7: 0, 8: 246, 9: 246}\n",
            "\n",
            "Similarity Matrix: \n",
            " [[1.0000 0.0368 0.4172 0.2760 0.4204 0.4200 0.3368 0.3840 0.4820 0.3700]\n",
            " [0.0368 1.0000 0.4356 0.2416 0.4692 0.3496 0.3700 0.4912 0.2692 0.4008]\n",
            " [0.4172 0.4356 1.0000 0.2552 0.4480 0.6220 0.5112 0.6728 0.6972 0.6332]\n",
            " [0.2760 0.2416 0.2552 1.0000 0.2624 0.3508 0.4812 0.3192 0.3036 0.5552]\n",
            " [0.4204 0.4692 0.4480 0.2624 1.0000 0.4928 0.4312 0.3472 0.2848 0.4604]\n",
            " [0.4200 0.3496 0.6220 0.3508 0.4928 1.0000 0.6260 0.4412 0.5932 0.7016]\n",
            " [0.3368 0.3700 0.5112 0.4812 0.4312 0.6260 1.0000 0.4560 0.5072 0.7616]\n",
            " [0.3840 0.4912 0.6728 0.3192 0.3472 0.4412 0.4560 1.0000 0.6316 0.5460]\n",
            " [0.4820 0.2692 0.6972 0.3036 0.2848 0.5932 0.5072 0.6316 1.0000 0.5604]\n",
            " [0.3700 0.4008 0.6332 0.5552 0.4604 0.7016 0.7616 0.5460 0.5604 1.0000]]\n",
            "\n",
            "Cluster {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5], 6: [6], 7: [7], 8: [8], 9: [9]}\n",
            "###### ROUND 9 ######\n",
            "Clients [5 4 0 6 3 2 1 9 8 7]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.000\n",
            "AVG Init Test Loss: 3.239, AVG Init Test Acc: 72.604\n",
            "AVG Final Test Loss: 3.384, AVG Final Test Acc: 72.638\n",
            "Round 9 Upload Cost: 1.777 MB\n",
            "Round 9 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 5, Count: 0, Labels: {0: 9, 1: 13, 2: 659, 3: 1159, 4: 751, 5: 834, 6: 26, 7: 925, 8: 199, 9: 307}\n",
            "Client 4, Count: 0, Labels: {0: 12, 1: 2, 2: 517, 3: 106, 4: 742, 5: 1744, 6: 1981, 7: 400, 8: 2341}\n",
            "Client 0, Count: 0, Labels: {0: 1756, 2: 2036, 3: 682, 4: 233, 5: 1605}\n",
            "Client 6, Count: 0, Labels: {0: 84, 1: 1221, 2: 656, 3: 4, 4: 1316, 5: 399, 6: 29, 7: 615, 8: 1291, 9: 282}\n",
            "Client 3, Count: 0, Labels: {0: 506, 1: 2044, 2: 8, 3: 169, 4: 1959, 5: 8, 6: 83, 7: 1912}\n",
            "Client 2, Count: 0, Labels: {0: 43, 1: 1239, 2: 454, 3: 710, 4: 15, 5: 576, 6: 288, 7: 16, 8: 469, 9: 402}\n",
            "Client 1, Count: 0, Labels: {0: 80, 1: 32, 2: 2, 3: 1, 4: 1, 5: 42, 6: 3224, 7: 1003, 8: 38, 9: 1592}\n",
            "Client 9, Count: 0, Labels: {0: 39, 1: 1163, 2: 304, 3: 932, 4: 742, 5: 753, 6: 66, 7: 1127, 8: 396, 9: 3122}\n",
            "Client 8, Count: 0, Labels: {0: 891, 1: 82, 2: 1347, 3: 1198, 4: 235, 5: 7, 6: 90, 8: 533, 9: 141}\n",
            "Client 7, Count: 0, Labels: {0: 2580, 1: 204, 2: 17, 3: 1039, 4: 6, 5: 32, 6: 213, 7: 2, 8: 733, 9: 154}\n",
            "\n",
            "Client 5, Correct_pred_per_label: {0: 0, 1: 189, 2: 232, 3: 244, 4: 141, 5: 243, 6: 0, 7: 240, 8: 226, 9: 212}\n",
            "Client 4, Correct_pred_per_label: {0: 0, 1: 0, 2: 166, 3: 160, 4: 147, 5: 249, 6: 236, 7: 223, 8: 245, 9: 0}\n",
            "Client 0, Correct_pred_per_label: {0: 231, 1: 0, 2: 220, 3: 223, 4: 125, 5: 250, 6: 0, 7: 0, 8: 0, 9: 0}\n",
            "Client 6, Correct_pred_per_label: {0: 185, 1: 246, 2: 173, 3: 0, 4: 235, 5: 229, 6: 0, 7: 241, 8: 248, 9: 218}\n",
            "Client 3, Correct_pred_per_label: {0: 225, 1: 247, 2: 0, 3: 191, 4: 245, 5: 0, 6: 6, 7: 250, 8: 0, 9: 0}\n",
            "Client 2, Correct_pred_per_label: {0: 109, 1: 242, 2: 184, 3: 239, 4: 0, 5: 236, 6: 129, 7: 40, 8: 241, 9: 248}\n",
            "Client 1, Correct_pred_per_label: {0: 2, 1: 202, 2: 0, 3: 0, 4: 0, 5: 87, 6: 250, 7: 229, 8: 159, 9: 242}\n",
            "Client 9, Correct_pred_per_label: {0: 139, 1: 246, 2: 110, 3: 230, 4: 240, 5: 223, 6: 14, 7: 226, 8: 242, 9: 245}\n",
            "Client 8, Correct_pred_per_label: {0: 216, 1: 232, 2: 237, 3: 239, 4: 31, 5: 70, 6: 0, 7: 0, 8: 238, 9: 248}\n",
            "Client 7, Correct_pred_per_label: {0: 235, 1: 240, 2: 1, 3: 237, 4: 0, 5: 192, 6: 138, 7: 0, 8: 246, 9: 246}\n",
            "\n",
            "Similarity Matrix: \n",
            " [[1.0000 0.0368 0.4172 0.2760 0.4204 0.4200 0.3368 0.3840 0.4820 0.3700]\n",
            " [0.0368 1.0000 0.4356 0.2416 0.4692 0.3496 0.3700 0.4912 0.2692 0.4008]\n",
            " [0.4172 0.4356 1.0000 0.2552 0.4480 0.6220 0.5112 0.6728 0.6972 0.6332]\n",
            " [0.2760 0.2416 0.2552 1.0000 0.2624 0.3508 0.4812 0.3192 0.3036 0.5552]\n",
            " [0.4204 0.4692 0.4480 0.2624 1.0000 0.4928 0.4312 0.3472 0.2848 0.4604]\n",
            " [0.4200 0.3496 0.6220 0.3508 0.4928 1.0000 0.6260 0.4412 0.5932 0.7016]\n",
            " [0.3368 0.3700 0.5112 0.4812 0.4312 0.6260 1.0000 0.4560 0.5072 0.7616]\n",
            " [0.3840 0.4912 0.6728 0.3192 0.3472 0.4412 0.4560 1.0000 0.6316 0.5460]\n",
            " [0.4820 0.2692 0.6972 0.3036 0.2848 0.5932 0.5072 0.6316 1.0000 0.5604]\n",
            " [0.3700 0.4008 0.6332 0.5552 0.4604 0.7016 0.7616 0.5460 0.5604 1.0000]]\n",
            "\n",
            "Cluster {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5], 6: [6], 7: [7], 8: [8], 9: [9]}\n",
            "###### ROUND 10 ######\n",
            "Clients [7 3 6 2 4 1 5 9 0 8]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.000\n",
            "AVG Init Test Loss: 3.384, AVG Init Test Acc: 72.638\n",
            "AVG Final Test Loss: 3.484, AVG Final Test Acc: 72.657\n",
            "Round 10 Upload Cost: 1.777 MB\n",
            "Round 10 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 7, Count: 0, Labels: {0: 2580, 1: 204, 2: 17, 3: 1039, 4: 6, 5: 32, 6: 213, 7: 2, 8: 733, 9: 154}\n",
            "Client 3, Count: 0, Labels: {0: 506, 1: 2044, 2: 8, 3: 169, 4: 1959, 5: 8, 6: 83, 7: 1912}\n",
            "Client 6, Count: 0, Labels: {0: 84, 1: 1221, 2: 656, 3: 4, 4: 1316, 5: 399, 6: 29, 7: 615, 8: 1291, 9: 282}\n",
            "Client 2, Count: 0, Labels: {0: 43, 1: 1239, 2: 454, 3: 710, 4: 15, 5: 576, 6: 288, 7: 16, 8: 469, 9: 402}\n",
            "Client 4, Count: 0, Labels: {0: 12, 1: 2, 2: 517, 3: 106, 4: 742, 5: 1744, 6: 1981, 7: 400, 8: 2341}\n",
            "Client 1, Count: 0, Labels: {0: 80, 1: 32, 2: 2, 3: 1, 4: 1, 5: 42, 6: 3224, 7: 1003, 8: 38, 9: 1592}\n",
            "Client 5, Count: 0, Labels: {0: 9, 1: 13, 2: 659, 3: 1159, 4: 751, 5: 834, 6: 26, 7: 925, 8: 199, 9: 307}\n",
            "Client 9, Count: 0, Labels: {0: 39, 1: 1163, 2: 304, 3: 932, 4: 742, 5: 753, 6: 66, 7: 1127, 8: 396, 9: 3122}\n",
            "Client 0, Count: 0, Labels: {0: 1756, 2: 2036, 3: 682, 4: 233, 5: 1605}\n",
            "Client 8, Count: 0, Labels: {0: 891, 1: 82, 2: 1347, 3: 1198, 4: 235, 5: 7, 6: 90, 8: 533, 9: 141}\n",
            "\n",
            "Client 7, Correct_pred_per_label: {0: 235, 1: 240, 2: 1, 3: 237, 4: 0, 5: 192, 6: 138, 7: 0, 8: 246, 9: 246}\n",
            "Client 3, Correct_pred_per_label: {0: 225, 1: 247, 2: 0, 3: 191, 4: 245, 5: 0, 6: 6, 7: 250, 8: 0, 9: 0}\n",
            "Client 6, Correct_pred_per_label: {0: 185, 1: 246, 2: 173, 3: 0, 4: 235, 5: 229, 6: 0, 7: 241, 8: 248, 9: 218}\n",
            "Client 2, Correct_pred_per_label: {0: 109, 1: 242, 2: 184, 3: 239, 4: 0, 5: 236, 6: 129, 7: 40, 8: 241, 9: 248}\n",
            "Client 4, Correct_pred_per_label: {0: 0, 1: 0, 2: 166, 3: 160, 4: 147, 5: 249, 6: 236, 7: 223, 8: 245, 9: 0}\n",
            "Client 1, Correct_pred_per_label: {0: 2, 1: 202, 2: 0, 3: 0, 4: 0, 5: 87, 6: 250, 7: 229, 8: 159, 9: 242}\n",
            "Client 5, Correct_pred_per_label: {0: 0, 1: 189, 2: 232, 3: 244, 4: 141, 5: 243, 6: 0, 7: 240, 8: 226, 9: 212}\n",
            "Client 9, Correct_pred_per_label: {0: 139, 1: 246, 2: 110, 3: 230, 4: 240, 5: 223, 6: 14, 7: 226, 8: 242, 9: 245}\n",
            "Client 0, Correct_pred_per_label: {0: 231, 1: 0, 2: 220, 3: 223, 4: 125, 5: 250, 6: 0, 7: 0, 8: 0, 9: 0}\n",
            "Client 8, Correct_pred_per_label: {0: 216, 1: 232, 2: 237, 3: 239, 4: 31, 5: 70, 6: 0, 7: 0, 8: 238, 9: 248}\n",
            "\n",
            "Similarity Matrix: \n",
            " [[1.0000 0.0368 0.4172 0.2760 0.4204 0.4200 0.3368 0.3840 0.4820 0.3700]\n",
            " [0.0368 1.0000 0.4356 0.2416 0.4692 0.3496 0.3700 0.4912 0.2692 0.4008]\n",
            " [0.4172 0.4356 1.0000 0.2552 0.4480 0.6220 0.5112 0.6728 0.6972 0.6332]\n",
            " [0.2760 0.2416 0.2552 1.0000 0.2624 0.3508 0.4812 0.3192 0.3036 0.5552]\n",
            " [0.4204 0.4692 0.4480 0.2624 1.0000 0.4928 0.4312 0.3472 0.2848 0.4604]\n",
            " [0.4200 0.3496 0.6220 0.3508 0.4928 1.0000 0.6260 0.4412 0.5932 0.7016]\n",
            " [0.3368 0.3700 0.5112 0.4812 0.4312 0.6260 1.0000 0.4560 0.5072 0.7616]\n",
            " [0.3840 0.4912 0.6728 0.3192 0.3472 0.4412 0.4560 1.0000 0.6316 0.5460]\n",
            " [0.4820 0.2692 0.6972 0.3036 0.2848 0.5932 0.5072 0.6316 1.0000 0.5604]\n",
            " [0.3700 0.4008 0.6332 0.5552 0.4604 0.7016 0.7616 0.5460 0.5604 1.0000]]\n",
            "\n",
            "Cluster {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5], 6: [6], 7: [7], 8: [8], 9: [9]}\n",
            "Training Loss: 0.000, Training Accuracy: 100.000\n",
            "Testing Loss: 3.484, Testing Accuracy: 72.657\n"
          ]
        }
      ],
      "source": [
        "! bash flis_hc.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOmlYmQMBWZN",
        "outputId": "55b46d63-2c1e-4d05-ee90-25a8fcbee5be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "partition: noniid-labeldir\n",
            "Data statistics Train:\n",
            " {0: {0: 2, 1: 429, 2: 1141, 3: 189, 4: 547, 5: 521, 6: 909, 7: 314, 8: 80, 9: 178}, 1: {0: 216, 1: 1647, 2: 1941, 3: 4, 4: 2635}, 2: {0: 3321, 1: 43, 2: 1063, 3: 2064}, 3: {0: 197, 1: 92, 2: 381, 3: 2113, 4: 324, 5: 2892, 6: 50}, 4: {0: 5, 1: 184, 2: 2, 3: 10, 4: 6, 5: 6, 6: 456, 7: 146, 8: 1655, 9: 1303}, 5: {0: 640, 1: 151, 2: 73, 3: 6, 4: 106, 5: 14, 6: 648, 7: 3658, 8: 2689}, 6: {0: 38, 1: 83, 2: 322, 3: 746, 4: 316, 5: 1845, 6: 778, 7: 498, 8: 820, 9: 3543}, 7: {0: 57, 1: 794, 2: 617, 3: 315, 4: 874, 5: 15, 6: 1075, 7: 777, 8: 102, 9: 524}, 8: {0: 1094, 1: 2053, 2: 455, 3: 64, 4: 46, 5: 448, 6: 2081}, 9: {0: 430, 1: 524, 2: 5, 3: 489, 4: 1146, 5: 259, 6: 3, 7: 607, 8: 654, 9: 452}} \n",
            "\n",
            "Data statistics Test:\n",
            " {0: {0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000, 9: 1000}, 1: {0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000}, 2: {0: 1000, 1: 1000, 2: 1000, 3: 1000}, 3: {0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000}, 4: {0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000, 9: 1000}, 5: {0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000}, 6: {0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000, 9: 1000}, 7: {0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000, 9: 1000}, 8: {0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000}, 9: {0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000, 9: 1000}} \n",
            "\n",
            "len train_ds_global: 60000\n",
            "len test_ds_global: 10000\n",
            "Shared data has label: 0, 250 samples\n",
            "Shared data has label: 1, 250 samples\n",
            "Shared data has label: 2, 250 samples\n",
            "Shared data has label: 3, 250 samples\n",
            "Shared data has label: 4, 250 samples\n",
            "Shared data has label: 5, 250 samples\n",
            "Shared data has label: 6, 250 samples\n",
            "Shared data has label: 7, 250 samples\n",
            "Shared data has label: 8, 250 samples\n",
            "Shared data has label: 9, 250 samples\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "torch.Size([250, 1, 28, 28])\n",
            "MODEL: simple-cnn, Dataset: fmnist\n",
            "SimpleCNNMNIST(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "conv1.weight torch.Size([6, 1, 5, 5])\n",
            "conv1.bias torch.Size([6])\n",
            "conv2.weight torch.Size([16, 6, 5, 5])\n",
            "conv2.bias torch.Size([16])\n",
            "fc1.weight torch.Size([120, 256])\n",
            "fc1.bias torch.Size([120])\n",
            "fc2.weight torch.Size([84, 120])\n",
            "fc2.bias torch.Size([84])\n",
            "fc3.weight torch.Size([10, 84])\n",
            "fc3.bias torch.Size([10])\n",
            "44426\n",
            "###### ROUND 1 ######\n",
            "Clients [0 9 7 5 6 4 8 3 1 2]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.352\n",
            "AVG Init Test Loss: 2.286, AVG Init Test Acc: 8.703\n",
            "AVG Final Test Loss: 1.225, AVG Final Test Acc: 70.651\n",
            "Round 1 Upload Cost: 1.777 MB\n",
            "Round 1 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 0, Count: 0, Labels: {0: 2, 1: 429, 2: 1141, 3: 189, 4: 547, 5: 521, 6: 909, 7: 314, 8: 80, 9: 178}\n",
            "Client 9, Count: 0, Labels: {0: 430, 1: 524, 2: 5, 3: 489, 4: 1146, 5: 259, 6: 3, 7: 607, 8: 654, 9: 452}\n",
            "Client 7, Count: 0, Labels: {0: 57, 1: 794, 2: 617, 3: 315, 4: 874, 5: 15, 6: 1075, 7: 777, 8: 102, 9: 524}\n",
            "Client 5, Count: 0, Labels: {0: 640, 1: 151, 2: 73, 3: 6, 4: 106, 5: 14, 6: 648, 7: 3658, 8: 2689}\n",
            "Client 6, Count: 0, Labels: {0: 38, 1: 83, 2: 322, 3: 746, 4: 316, 5: 1845, 6: 778, 7: 498, 8: 820, 9: 3543}\n",
            "Client 4, Count: 0, Labels: {0: 5, 1: 184, 2: 2, 3: 10, 4: 6, 5: 6, 6: 456, 7: 146, 8: 1655, 9: 1303}\n",
            "Client 8, Count: 0, Labels: {0: 1094, 1: 2053, 2: 455, 3: 64, 4: 46, 5: 448, 6: 2081}\n",
            "Client 3, Count: 0, Labels: {0: 197, 1: 92, 2: 381, 3: 2113, 4: 324, 5: 2892, 6: 50}\n",
            "Client 1, Count: 0, Labels: {0: 216, 1: 1647, 2: 1941, 3: 4, 4: 2635}\n",
            "Client 2, Count: 0, Labels: {0: 3321, 1: 43, 2: 1063, 3: 2064}\n",
            "\n",
            "Client 0, Correct_pred_per_label: {0: 0, 1: 243, 2: 182, 3: 195, 4: 153, 5: 234, 6: 217, 7: 232, 8: 214, 9: 230}\n",
            "Client 9, Correct_pred_per_label: {0: 230, 1: 242, 2: 0, 3: 195, 4: 241, 5: 211, 6: 0, 7: 240, 8: 241, 9: 229}\n",
            "Client 7, Correct_pred_per_label: {0: 42, 1: 244, 2: 186, 3: 198, 4: 204, 5: 20, 6: 190, 7: 232, 8: 177, 9: 243}\n",
            "Client 5, Correct_pred_per_label: {0: 221, 1: 239, 2: 51, 3: 0, 4: 157, 5: 44, 6: 155, 7: 250, 8: 246, 9: 0}\n",
            "Client 6, Correct_pred_per_label: {0: 23, 1: 225, 2: 123, 3: 230, 4: 187, 5: 237, 6: 209, 7: 218, 8: 236, 9: 244}\n",
            "Client 4, Correct_pred_per_label: {0: 0, 1: 227, 2: 0, 3: 35, 4: 0, 5: 0, 6: 229, 7: 190, 8: 247, 9: 250}\n",
            "Client 8, Correct_pred_per_label: {0: 199, 1: 243, 2: 109, 3: 161, 4: 7, 5: 247, 6: 209, 7: 0, 8: 0, 9: 0}\n",
            "Client 3, Correct_pred_per_label: {0: 207, 1: 225, 2: 201, 3: 247, 4: 170, 5: 250, 6: 0, 7: 0, 8: 0, 9: 0}\n",
            "Client 1, Correct_pred_per_label: {0: 214, 1: 247, 2: 196, 3: 0, 4: 245, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}\n",
            "Client 2, Correct_pred_per_label: {0: 227, 1: 219, 2: 228, 3: 245, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}\n",
            "Clusters Glob Acc: [tensor(60.0100), tensor(70.9000), tensor(66.2800), tensor(66.2800), tensor(60.0100), tensor(60.3600), tensor(45.1500), tensor(45.3900), tensor(37.8300), tensor(34.1700)]\n",
            "###### ROUND 2 ######\n",
            "Clients [8 3 4 1 5 2 9 6 7 0]\n",
            "Client 8, Select Cluster: 1\n",
            "acc clusters: [tensor(63.1429), tensor(72.1857), tensor(56.6571), tensor(56.6571), tensor(63.1429), tensor(49.0857), tensor(61.6429), tensor(64.8429), tensor(54.0429), tensor(48.8143)]\n",
            "Client 3, Select Cluster: 1\n",
            "acc clusters: [tensor(63.1429), tensor(72.1857), tensor(56.6571), tensor(56.6571), tensor(63.1429), tensor(49.0857), tensor(61.6429), tensor(64.8429), tensor(54.0429), tensor(48.8143)]\n",
            "Client 4, Select Cluster: 1\n",
            "acc clusters: [tensor(60.0100), tensor(70.9000), tensor(66.2800), tensor(66.2800), tensor(60.0100), tensor(60.3600), tensor(45.1500), tensor(45.3900), tensor(37.8300), tensor(34.1700)]\n",
            "Client 1, Select Cluster: 8\n",
            "acc clusters: [tensor(51.2000), tensor(72.3600), tensor(49.6200), tensor(49.6200), tensor(51.2000), tensor(38.3400), tensor(49.2600), tensor(72.1000), tensor(75.6600), tensor(68.3400)]\n",
            "Client 5, Select Cluster: 1\n",
            "acc clusters: [tensor(66.3333), tensor(76.7111), tensor(65.7556), tensor(65.7556), tensor(66.3333), tensor(59.8333), tensor(50.1667), tensor(50.4333), tensor(42.0333), tensor(37.9667)]\n",
            "Client 2, Select Cluster: 7\n",
            "acc clusters: [tensor(51.4250), tensor(69.3250), tensor(45.8000), tensor(45.8000), tensor(51.4250), tensor(41.1500), tensor(52.4000), tensor(85.6250), tensor(72.5000), tensor(85.4250)]\n",
            "Client 9, Select Cluster: 1\n",
            "acc clusters: [tensor(60.0100), tensor(70.9000), tensor(66.2800), tensor(66.2800), tensor(60.0100), tensor(60.3600), tensor(45.1500), tensor(45.3900), tensor(37.8300), tensor(34.1700)]\n",
            "Client 6, Select Cluster: 1\n",
            "acc clusters: [tensor(60.0100), tensor(70.9000), tensor(66.2800), tensor(66.2800), tensor(60.0100), tensor(60.3600), tensor(45.1500), tensor(45.3900), tensor(37.8300), tensor(34.1700)]\n",
            "Client 7, Select Cluster: 1\n",
            "acc clusters: [tensor(60.0100), tensor(70.9000), tensor(66.2800), tensor(66.2800), tensor(60.0100), tensor(60.3600), tensor(45.1500), tensor(45.3900), tensor(37.8300), tensor(34.1700)]\n",
            "Client 0, Select Cluster: 1\n",
            "acc clusters: [tensor(60.0100), tensor(70.9000), tensor(66.2800), tensor(66.2800), tensor(60.0100), tensor(60.3600), tensor(45.1500), tensor(45.3900), tensor(37.8300), tensor(34.1700)]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.204\n",
            "AVG Init Test Loss: 0.834, AVG Init Test Acc: 73.687\n",
            "AVG Final Test Loss: 1.040, AVG Final Test Acc: 74.214\n",
            "Round 2 Upload Cost: 1.777 MB\n",
            "Round 2 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 8, Count: 0, Labels: {0: 1094, 1: 2053, 2: 455, 3: 64, 4: 46, 5: 448, 6: 2081}\n",
            "Client 3, Count: 0, Labels: {0: 197, 1: 92, 2: 381, 3: 2113, 4: 324, 5: 2892, 6: 50}\n",
            "Client 4, Count: 0, Labels: {0: 5, 1: 184, 2: 2, 3: 10, 4: 6, 5: 6, 6: 456, 7: 146, 8: 1655, 9: 1303}\n",
            "Client 1, Count: 0, Labels: {0: 216, 1: 1647, 2: 1941, 3: 4, 4: 2635}\n",
            "Client 5, Count: 0, Labels: {0: 640, 1: 151, 2: 73, 3: 6, 4: 106, 5: 14, 6: 648, 7: 3658, 8: 2689}\n",
            "Client 2, Count: 0, Labels: {0: 3321, 1: 43, 2: 1063, 3: 2064}\n",
            "Client 9, Count: 0, Labels: {0: 430, 1: 524, 2: 5, 3: 489, 4: 1146, 5: 259, 6: 3, 7: 607, 8: 654, 9: 452}\n",
            "Client 6, Count: 0, Labels: {0: 38, 1: 83, 2: 322, 3: 746, 4: 316, 5: 1845, 6: 778, 7: 498, 8: 820, 9: 3543}\n",
            "Client 7, Count: 0, Labels: {0: 57, 1: 794, 2: 617, 3: 315, 4: 874, 5: 15, 6: 1075, 7: 777, 8: 102, 9: 524}\n",
            "Client 0, Count: 0, Labels: {0: 2, 1: 429, 2: 1141, 3: 189, 4: 547, 5: 521, 6: 909, 7: 314, 8: 80, 9: 178}\n",
            "\n",
            "Client 8, Correct_pred_per_label: {0: 181, 1: 245, 2: 174, 3: 108, 4: 52, 5: 248, 6: 213, 7: 0, 8: 2, 9: 0}\n",
            "Client 3, Correct_pred_per_label: {0: 190, 1: 230, 2: 212, 3: 247, 4: 165, 5: 250, 6: 9, 7: 0, 8: 0, 9: 0}\n",
            "Client 4, Correct_pred_per_label: {0: 0, 1: 238, 2: 0, 3: 82, 4: 0, 5: 138, 6: 242, 7: 226, 8: 248, 9: 246}\n",
            "Client 1, Correct_pred_per_label: {0: 228, 1: 245, 2: 230, 3: 5, 4: 227, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}\n",
            "Client 5, Correct_pred_per_label: {0: 218, 1: 231, 2: 152, 3: 13, 4: 77, 5: 98, 6: 150, 7: 250, 8: 247, 9: 0}\n",
            "Client 2, Correct_pred_per_label: {0: 239, 1: 223, 2: 229, 3: 241, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}\n",
            "Client 9, Correct_pred_per_label: {0: 235, 1: 241, 2: 7, 3: 209, 4: 237, 5: 228, 6: 0, 7: 215, 8: 241, 9: 242}\n",
            "Client 6, Correct_pred_per_label: {0: 40, 1: 227, 2: 132, 3: 234, 4: 182, 5: 245, 6: 203, 7: 216, 8: 240, 9: 246}\n",
            "Client 7, Correct_pred_per_label: {0: 59, 1: 242, 2: 186, 3: 212, 4: 189, 5: 131, 6: 204, 7: 242, 8: 226, 9: 235}\n",
            "Client 0, Correct_pred_per_label: {0: 0, 1: 244, 2: 224, 3: 196, 4: 194, 5: 236, 6: 156, 7: 240, 8: 222, 9: 229}\n",
            "Clusters Glob Acc: [tensor(80.9900), tensor(66.6700), tensor(82.2200), tensor(38.8400), tensor(82.8200), tensor(38.8400), tensor(83.9500), tensor(83.6700), tensor(83.6700), tensor(83.6700)]\n",
            "###### ROUND 3 ######\n",
            "Clients [2 7 3 4 9 1 6 0 8 5]\n",
            "Client 2, Select Cluster: 3\n",
            "acc clusters: [tensor(76.3750), tensor(90.5000), tensor(71.5750), tensor(90.9500), tensor(73.8500), tensor(90.9500), tensor(76.4000), tensor(77.3000), tensor(77.3000), tensor(77.3000)]\n",
            "Client 7, Select Cluster: 6\n",
            "acc clusters: [tensor(80.9900), tensor(66.6700), tensor(82.2200), tensor(38.8400), tensor(82.8200), tensor(38.8400), tensor(83.9500), tensor(83.6700), tensor(83.6700), tensor(83.6700)]\n",
            "Client 3, Select Cluster: 7\n",
            "acc clusters: [tensor(79.4429), tensor(75.2714), tensor(76.3286), tensor(55.4857), tensor(78.0143), tensor(55.4857), tensor(79.3143), tensor(80.2857), tensor(80.2857), tensor(80.2857)]\n",
            "Client 4, Select Cluster: 6\n",
            "acc clusters: [tensor(80.9900), tensor(66.6700), tensor(82.2200), tensor(38.8400), tensor(82.8200), tensor(38.8400), tensor(83.9500), tensor(83.6700), tensor(83.6700), tensor(83.6700)]\n",
            "Client 9, Select Cluster: 6\n",
            "acc clusters: [tensor(80.9900), tensor(66.6700), tensor(82.2200), tensor(38.8400), tensor(82.8200), tensor(38.8400), tensor(83.9500), tensor(83.6700), tensor(83.6700), tensor(83.6700)]\n",
            "Client 1, Select Cluster: 1\n",
            "acc clusters: [tensor(75.5800), tensor(84.0400), tensor(73.8200), tensor(76.2200), tensor(74.2800), tensor(76.2200), tensor(78.1000), tensor(77.8000), tensor(77.8000), tensor(77.8000)]\n",
            "Client 6, Select Cluster: 6\n",
            "acc clusters: [tensor(80.9900), tensor(66.6700), tensor(82.2200), tensor(38.8400), tensor(82.8200), tensor(38.8400), tensor(83.9500), tensor(83.6700), tensor(83.6700), tensor(83.6700)]\n",
            "Client 0, Select Cluster: 6\n",
            "acc clusters: [tensor(80.9900), tensor(66.6700), tensor(82.2200), tensor(38.8400), tensor(82.8200), tensor(38.8400), tensor(83.9500), tensor(83.6700), tensor(83.6700), tensor(83.6700)]\n",
            "Client 8, Select Cluster: 7\n",
            "acc clusters: [tensor(79.4429), tensor(75.2714), tensor(76.3286), tensor(55.4857), tensor(78.0143), tensor(55.4857), tensor(79.3143), tensor(80.2857), tensor(80.2857), tensor(80.2857)]\n",
            "Client 5, Select Cluster: 7\n",
            "acc clusters: [tensor(81.9333), tensor(68.8667), tensor(81.0667), tensor(43.1556), tensor(81.9889), tensor(43.1556), tensor(83.0111), tensor(83.1333), tensor(83.1333), tensor(83.1333)]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.156\n",
            "AVG Init Test Loss: 0.434, AVG Init Test Acc: 83.844\n",
            "AVG Final Test Loss: 0.994, AVG Final Test Acc: 76.389\n",
            "Round 3 Upload Cost: 1.777 MB\n",
            "Round 3 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 2, Count: 0, Labels: {0: 3321, 1: 43, 2: 1063, 3: 2064}\n",
            "Client 7, Count: 0, Labels: {0: 57, 1: 794, 2: 617, 3: 315, 4: 874, 5: 15, 6: 1075, 7: 777, 8: 102, 9: 524}\n",
            "Client 3, Count: 0, Labels: {0: 197, 1: 92, 2: 381, 3: 2113, 4: 324, 5: 2892, 6: 50}\n",
            "Client 4, Count: 0, Labels: {0: 5, 1: 184, 2: 2, 3: 10, 4: 6, 5: 6, 6: 456, 7: 146, 8: 1655, 9: 1303}\n",
            "Client 9, Count: 0, Labels: {0: 430, 1: 524, 2: 5, 3: 489, 4: 1146, 5: 259, 6: 3, 7: 607, 8: 654, 9: 452}\n",
            "Client 1, Count: 0, Labels: {0: 216, 1: 1647, 2: 1941, 3: 4, 4: 2635}\n",
            "Client 6, Count: 0, Labels: {0: 38, 1: 83, 2: 322, 3: 746, 4: 316, 5: 1845, 6: 778, 7: 498, 8: 820, 9: 3543}\n",
            "Client 0, Count: 0, Labels: {0: 2, 1: 429, 2: 1141, 3: 189, 4: 547, 5: 521, 6: 909, 7: 314, 8: 80, 9: 178}\n",
            "Client 8, Count: 0, Labels: {0: 1094, 1: 2053, 2: 455, 3: 64, 4: 46, 5: 448, 6: 2081}\n",
            "Client 5, Count: 0, Labels: {0: 640, 1: 151, 2: 73, 3: 6, 4: 106, 5: 14, 6: 648, 7: 3658, 8: 2689}\n",
            "\n",
            "Client 2, Correct_pred_per_label: {0: 207, 1: 224, 2: 247, 3: 229, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}\n",
            "Client 7, Correct_pred_per_label: {0: 56, 1: 241, 2: 191, 3: 171, 4: 221, 5: 128, 6: 183, 7: 247, 8: 224, 9: 233}\n",
            "Client 3, Correct_pred_per_label: {0: 202, 1: 231, 2: 218, 3: 249, 4: 165, 5: 250, 6: 67, 7: 0, 8: 6, 9: 0}\n",
            "Client 4, Correct_pred_per_label: {0: 87, 1: 242, 2: 86, 3: 147, 4: 68, 5: 131, 6: 220, 7: 221, 8: 248, 9: 247}\n",
            "Client 9, Correct_pred_per_label: {0: 225, 1: 246, 2: 26, 3: 198, 4: 243, 5: 234, 6: 1, 7: 231, 8: 243, 9: 236}\n",
            "Client 1, Correct_pred_per_label: {0: 216, 1: 247, 2: 212, 3: 11, 4: 236, 5: 67, 6: 0, 7: 0, 8: 0, 9: 12}\n",
            "Client 6, Correct_pred_per_label: {0: 41, 1: 233, 2: 169, 3: 231, 4: 214, 5: 246, 6: 174, 7: 208, 8: 244, 9: 245}\n",
            "Client 0, Correct_pred_per_label: {0: 0, 1: 241, 2: 220, 3: 203, 4: 185, 5: 243, 6: 171, 7: 206, 8: 216, 9: 237}\n",
            "Client 8, Correct_pred_per_label: {0: 186, 1: 243, 2: 175, 3: 139, 4: 131, 5: 250, 6: 202, 7: 0, 8: 8, 9: 0}\n",
            "Client 5, Correct_pred_per_label: {0: 201, 1: 239, 2: 109, 3: 57, 4: 127, 5: 83, 6: 187, 7: 250, 8: 246, 9: 0}\n",
            "Clusters Glob Acc: [tensor(48.1800), tensor(85.4000), tensor(78.9600), tensor(84.3400), tensor(85.2000), tensor(53.1800), tensor(85.4000), tensor(85.4000), tensor(85.2000), tensor(84.3400)]\n",
            "###### ROUND 4 ######\n",
            "Clients [9 8 2 5 1 3 0 7 6 4]\n",
            "Client 9, Select Cluster: 1\n",
            "acc clusters: [tensor(48.1800), tensor(85.4000), tensor(78.9600), tensor(84.3400), tensor(85.2000), tensor(53.1800), tensor(85.4000), tensor(85.4000), tensor(85.2000), tensor(84.3400)]\n",
            "Client 8, Select Cluster: 4\n",
            "acc clusters: [tensor(68.8286), tensor(81.8000), tensor(79.8143), tensor(79.8286), tensor(81.9143), tensor(75.3857), tensor(81.8000), tensor(81.8000), tensor(81.9143), tensor(79.8286)]\n",
            "Client 2, Select Cluster: 5\n",
            "acc clusters: [tensor(92.2250), tensor(79.0500), tensor(90.1250), tensor(75.9000), tensor(82.4250), tensor(92.4500), tensor(79.0500), tensor(79.0500), tensor(82.4250), tensor(75.9000)]\n",
            "Client 5, Select Cluster: 1\n",
            "acc clusters: [tensor(53.5333), tensor(84.7000), tensor(78.3333), tensor(83.5444), tensor(84.5111), tensor(58.8556), tensor(84.7000), tensor(84.7000), tensor(84.5111), tensor(83.5444)]\n",
            "Client 1, Select Cluster: 2\n",
            "acc clusters: [tensor(78.6800), tensor(80.0800), tensor(87.4800), tensor(77.4600), tensor(83.7000), tensor(85.6000), tensor(80.0800), tensor(80.0800), tensor(83.7000), tensor(77.4600)]\n",
            "Client 3, Select Cluster: 4\n",
            "acc clusters: [tensor(68.8286), tensor(81.8000), tensor(79.8143), tensor(79.8286), tensor(81.9143), tensor(75.3857), tensor(81.8000), tensor(81.8000), tensor(81.9143), tensor(79.8286)]\n",
            "Client 0, Select Cluster: 1\n",
            "acc clusters: [tensor(48.1800), tensor(85.4000), tensor(78.9600), tensor(84.3400), tensor(85.2000), tensor(53.1800), tensor(85.4000), tensor(85.4000), tensor(85.2000), tensor(84.3400)]\n",
            "Client 7, Select Cluster: 1\n",
            "acc clusters: [tensor(48.1800), tensor(85.4000), tensor(78.9600), tensor(84.3400), tensor(85.2000), tensor(53.1800), tensor(85.4000), tensor(85.4000), tensor(85.2000), tensor(84.3400)]\n",
            "Client 6, Select Cluster: 1\n",
            "acc clusters: [tensor(48.1800), tensor(85.4000), tensor(78.9600), tensor(84.3400), tensor(85.2000), tensor(53.1800), tensor(85.4000), tensor(85.4000), tensor(85.2000), tensor(84.3400)]\n",
            "Client 4, Select Cluster: 1\n",
            "acc clusters: [tensor(48.1800), tensor(85.4000), tensor(78.9600), tensor(84.3400), tensor(85.2000), tensor(53.1800), tensor(85.4000), tensor(85.4000), tensor(85.2000), tensor(84.3400)]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.132\n",
            "AVG Init Test Loss: 0.405, AVG Init Test Acc: 85.546\n",
            "AVG Final Test Loss: 1.095, AVG Final Test Acc: 77.302\n",
            "Round 4 Upload Cost: 1.777 MB\n",
            "Round 4 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 9, Count: 0, Labels: {0: 430, 1: 524, 2: 5, 3: 489, 4: 1146, 5: 259, 6: 3, 7: 607, 8: 654, 9: 452}\n",
            "Client 8, Count: 0, Labels: {0: 1094, 1: 2053, 2: 455, 3: 64, 4: 46, 5: 448, 6: 2081}\n",
            "Client 2, Count: 0, Labels: {0: 3321, 1: 43, 2: 1063, 3: 2064}\n",
            "Client 5, Count: 0, Labels: {0: 640, 1: 151, 2: 73, 3: 6, 4: 106, 5: 14, 6: 648, 7: 3658, 8: 2689}\n",
            "Client 1, Count: 0, Labels: {0: 216, 1: 1647, 2: 1941, 3: 4, 4: 2635}\n",
            "Client 3, Count: 0, Labels: {0: 197, 1: 92, 2: 381, 3: 2113, 4: 324, 5: 2892, 6: 50}\n",
            "Client 0, Count: 0, Labels: {0: 2, 1: 429, 2: 1141, 3: 189, 4: 547, 5: 521, 6: 909, 7: 314, 8: 80, 9: 178}\n",
            "Client 7, Count: 0, Labels: {0: 57, 1: 794, 2: 617, 3: 315, 4: 874, 5: 15, 6: 1075, 7: 777, 8: 102, 9: 524}\n",
            "Client 6, Count: 0, Labels: {0: 38, 1: 83, 2: 322, 3: 746, 4: 316, 5: 1845, 6: 778, 7: 498, 8: 820, 9: 3543}\n",
            "Client 4, Count: 0, Labels: {0: 5, 1: 184, 2: 2, 3: 10, 4: 6, 5: 6, 6: 456, 7: 146, 8: 1655, 9: 1303}\n",
            "\n",
            "Client 9, Correct_pred_per_label: {0: 224, 1: 245, 2: 52, 3: 204, 4: 244, 5: 230, 6: 4, 7: 241, 8: 242, 9: 235}\n",
            "Client 8, Correct_pred_per_label: {0: 201, 1: 246, 2: 192, 3: 160, 4: 59, 5: 250, 6: 195, 7: 1, 8: 14, 9: 5}\n",
            "Client 2, Correct_pred_per_label: {0: 243, 1: 218, 2: 228, 3: 240, 4: 0, 5: 9, 6: 0, 7: 0, 8: 0, 9: 0}\n",
            "Client 5, Correct_pred_per_label: {0: 193, 1: 242, 2: 98, 3: 61, 4: 151, 5: 85, 6: 195, 7: 250, 8: 247, 9: 14}\n",
            "Client 1, Correct_pred_per_label: {0: 213, 1: 248, 2: 215, 3: 33, 4: 235, 5: 192, 6: 0, 7: 6, 8: 0, 9: 151}\n",
            "Client 3, Correct_pred_per_label: {0: 202, 1: 238, 2: 202, 3: 246, 4: 205, 5: 250, 6: 73, 7: 0, 8: 55, 9: 0}\n",
            "Client 0, Correct_pred_per_label: {0: 0, 1: 244, 2: 215, 3: 198, 4: 202, 5: 242, 6: 189, 7: 213, 8: 225, 9: 245}\n",
            "Client 7, Correct_pred_per_label: {0: 86, 1: 241, 2: 182, 3: 213, 4: 219, 5: 149, 6: 192, 7: 246, 8: 212, 9: 230}\n",
            "Client 6, Correct_pred_per_label: {0: 105, 1: 235, 2: 162, 3: 223, 4: 201, 5: 238, 6: 180, 7: 192, 8: 244, 9: 248}\n",
            "Client 4, Correct_pred_per_label: {0: 43, 1: 242, 2: 62, 3: 137, 4: 47, 5: 158, 6: 237, 7: 213, 8: 248, 9: 248}\n",
            "Clusters Glob Acc: [tensor(86.7000), tensor(86.0400), tensor(51.6300), tensor(85.4400), tensor(85.2500), tensor(83.3800), tensor(86.7000), tensor(86.7000), tensor(86.7000), tensor(85.4400)]\n",
            "###### ROUND 5 ######\n",
            "Clients [4 7 5 6 2 3 8 1 0 9]\n",
            "Client 4, Select Cluster: 0\n",
            "acc clusters: [tensor(86.7000), tensor(86.0400), tensor(51.6300), tensor(85.4400), tensor(85.2500), tensor(83.3800), tensor(86.7000), tensor(86.7000), tensor(86.7000), tensor(85.4400)]\n",
            "Client 7, Select Cluster: 0\n",
            "acc clusters: [tensor(86.7000), tensor(86.0400), tensor(51.6300), tensor(85.4400), tensor(85.2500), tensor(83.3800), tensor(86.7000), tensor(86.7000), tensor(86.7000), tensor(85.4400)]\n",
            "Client 5, Select Cluster: 0\n",
            "acc clusters: [tensor(85.9222), tensor(85.2333), tensor(57.3556), tensor(84.5222), tensor(84.2444), tensor(82.2889), tensor(85.9222), tensor(85.9222), tensor(85.9222), tensor(84.5222)]\n",
            "Client 6, Select Cluster: 0\n",
            "acc clusters: [tensor(86.7000), tensor(86.0400), tensor(51.6300), tensor(85.4400), tensor(85.2500), tensor(83.3800), tensor(86.7000), tensor(86.7000), tensor(86.7000), tensor(85.4400)]\n",
            "Client 2, Select Cluster: 2\n",
            "acc clusters: [tensor(84.2750), tensor(89.4250), tensor(94.8250), tensor(77.9750), tensor(86.4750), tensor(91.0750), tensor(84.2750), tensor(84.2750), tensor(84.2750), tensor(77.9750)]\n",
            "Client 3, Select Cluster: 0\n",
            "acc clusters: [tensor(83.5000), tensor(83.0286), tensor(73.2857), tensor(81.1143), tensor(83.4571), tensor(81.9286), tensor(83.5000), tensor(83.5000), tensor(83.5000), tensor(81.1143)]\n",
            "Client 8, Select Cluster: 0\n",
            "acc clusters: [tensor(83.5000), tensor(83.0286), tensor(73.2857), tensor(81.1143), tensor(83.4571), tensor(81.9286), tensor(83.5000), tensor(83.5000), tensor(83.5000), tensor(81.1143)]\n",
            "Client 1, Select Cluster: 5\n",
            "acc clusters: [tensor(85.0600), tensor(87.2200), tensor(79.3800), tensor(78.9400), tensor(87.0600), tensor(87.9400), tensor(85.0600), tensor(85.0600), tensor(85.0600), tensor(78.9400)]\n",
            "Client 0, Select Cluster: 0\n",
            "acc clusters: [tensor(86.7000), tensor(86.0400), tensor(51.6300), tensor(85.4400), tensor(85.2500), tensor(83.3800), tensor(86.7000), tensor(86.7000), tensor(86.7000), tensor(85.4400)]\n",
            "Client 9, Select Cluster: 0\n",
            "acc clusters: [tensor(86.7000), tensor(86.0400), tensor(51.6300), tensor(85.4400), tensor(85.2500), tensor(83.3800), tensor(86.7000), tensor(86.7000), tensor(86.7000), tensor(85.4400)]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.112\n",
            "AVG Init Test Loss: 0.399, AVG Init Test Acc: 86.919\n",
            "AVG Final Test Loss: 1.100, AVG Final Test Acc: 77.906\n",
            "Round 5 Upload Cost: 1.777 MB\n",
            "Round 5 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 4, Count: 0, Labels: {0: 5, 1: 184, 2: 2, 3: 10, 4: 6, 5: 6, 6: 456, 7: 146, 8: 1655, 9: 1303}\n",
            "Client 7, Count: 0, Labels: {0: 57, 1: 794, 2: 617, 3: 315, 4: 874, 5: 15, 6: 1075, 7: 777, 8: 102, 9: 524}\n",
            "Client 5, Count: 0, Labels: {0: 640, 1: 151, 2: 73, 3: 6, 4: 106, 5: 14, 6: 648, 7: 3658, 8: 2689}\n",
            "Client 6, Count: 0, Labels: {0: 38, 1: 83, 2: 322, 3: 746, 4: 316, 5: 1845, 6: 778, 7: 498, 8: 820, 9: 3543}\n",
            "Client 2, Count: 0, Labels: {0: 3321, 1: 43, 2: 1063, 3: 2064}\n",
            "Client 3, Count: 0, Labels: {0: 197, 1: 92, 2: 381, 3: 2113, 4: 324, 5: 2892, 6: 50}\n",
            "Client 8, Count: 0, Labels: {0: 1094, 1: 2053, 2: 455, 3: 64, 4: 46, 5: 448, 6: 2081}\n",
            "Client 1, Count: 0, Labels: {0: 216, 1: 1647, 2: 1941, 3: 4, 4: 2635}\n",
            "Client 0, Count: 0, Labels: {0: 2, 1: 429, 2: 1141, 3: 189, 4: 547, 5: 521, 6: 909, 7: 314, 8: 80, 9: 178}\n",
            "Client 9, Count: 0, Labels: {0: 430, 1: 524, 2: 5, 3: 489, 4: 1146, 5: 259, 6: 3, 7: 607, 8: 654, 9: 452}\n",
            "\n",
            "Client 4, Correct_pred_per_label: {0: 43, 1: 242, 2: 55, 3: 141, 4: 83, 5: 177, 6: 238, 7: 222, 8: 248, 9: 249}\n",
            "Client 7, Correct_pred_per_label: {0: 79, 1: 238, 2: 200, 3: 218, 4: 221, 5: 149, 6: 188, 7: 248, 8: 211, 9: 217}\n",
            "Client 5, Correct_pred_per_label: {0: 209, 1: 240, 2: 175, 3: 82, 4: 166, 5: 120, 6: 168, 7: 250, 8: 246, 9: 14}\n",
            "Client 6, Correct_pred_per_label: {0: 70, 1: 237, 2: 201, 3: 229, 4: 135, 5: 241, 6: 199, 7: 219, 8: 245, 9: 245}\n",
            "Client 2, Correct_pred_per_label: {0: 237, 1: 226, 2: 228, 3: 240, 4: 0, 5: 26, 6: 0, 7: 0, 8: 0, 9: 1}\n",
            "Client 3, Correct_pred_per_label: {0: 194, 1: 238, 2: 199, 3: 243, 4: 204, 5: 250, 6: 62, 7: 0, 8: 59, 9: 2}\n",
            "Client 8, Correct_pred_per_label: {0: 177, 1: 246, 2: 180, 3: 133, 4: 87, 5: 247, 6: 220, 7: 4, 8: 13, 9: 54}\n",
            "Client 1, Correct_pred_per_label: {0: 225, 1: 247, 2: 200, 3: 34, 4: 243, 5: 226, 6: 0, 7: 108, 8: 1, 9: 200}\n",
            "Client 0, Correct_pred_per_label: {0: 0, 1: 244, 2: 212, 3: 187, 4: 200, 5: 243, 6: 191, 7: 235, 8: 230, 9: 223}\n",
            "Client 9, Correct_pred_per_label: {0: 225, 1: 246, 2: 41, 3: 198, 4: 241, 5: 231, 6: 10, 7: 237, 8: 244, 9: 241}\n",
            "Clusters Glob Acc: [tensor(85.), tensor(86.8800), tensor(86.8800), tensor(86.8800), tensor(50.8000), tensor(86.2000), tensor(86.8800), tensor(86.5600), tensor(86.8800), tensor(86.8800)]\n",
            "###### ROUND 6 ######\n",
            "Clients [6 7 5 9 0 1 3 4 2 8]\n",
            "Client 6, Select Cluster: 1\n",
            "acc clusters: [tensor(85.), tensor(86.8800), tensor(86.8800), tensor(86.8800), tensor(50.8000), tensor(86.2000), tensor(86.8800), tensor(86.5600), tensor(86.8800), tensor(86.8800)]\n",
            "Client 7, Select Cluster: 1\n",
            "acc clusters: [tensor(85.), tensor(86.8800), tensor(86.8800), tensor(86.8800), tensor(50.8000), tensor(86.2000), tensor(86.8800), tensor(86.5600), tensor(86.8800), tensor(86.8800)]\n",
            "Client 5, Select Cluster: 1\n",
            "acc clusters: [tensor(84.1000), tensor(86.1889), tensor(86.1889), tensor(86.1889), tensor(56.4222), tensor(85.6889), tensor(86.1889), tensor(86.0111), tensor(86.1889), tensor(86.1889)]\n",
            "Client 9, Select Cluster: 1\n",
            "acc clusters: [tensor(85.), tensor(86.8800), tensor(86.8800), tensor(86.8800), tensor(50.8000), tensor(86.2000), tensor(86.8800), tensor(86.5600), tensor(86.8800), tensor(86.8800)]\n",
            "Client 0, Select Cluster: 1\n",
            "acc clusters: [tensor(85.), tensor(86.8800), tensor(86.8800), tensor(86.8800), tensor(50.8000), tensor(86.2000), tensor(86.8800), tensor(86.5600), tensor(86.8800), tensor(86.8800)]\n",
            "Client 1, Select Cluster: 5\n",
            "acc clusters: [tensor(77.7600), tensor(84.2600), tensor(84.2600), tensor(84.2600), tensor(80.2400), tensor(87.3800), tensor(84.2600), tensor(85.3600), tensor(84.2600), tensor(84.2600)]\n",
            "Client 3, Select Cluster: 5\n",
            "acc clusters: [tensor(80.4714), tensor(83.7286), tensor(83.7286), tensor(83.7286), tensor(72.4000), tensor(83.9143), tensor(83.7286), tensor(83.6714), tensor(83.7286), tensor(83.7286)]\n",
            "Client 4, Select Cluster: 1\n",
            "acc clusters: [tensor(85.), tensor(86.8800), tensor(86.8800), tensor(86.8800), tensor(50.8000), tensor(86.2000), tensor(86.8800), tensor(86.5600), tensor(86.8800), tensor(86.8800)]\n",
            "Client 2, Select Cluster: 4\n",
            "acc clusters: [tensor(78.0250), tensor(84.1000), tensor(84.1000), tensor(84.1000), tensor(94.3000), tensor(89.7000), tensor(84.1000), tensor(85.3500), tensor(84.1000), tensor(84.1000)]\n",
            "Client 8, Select Cluster: 5\n",
            "acc clusters: [tensor(80.4714), tensor(83.7286), tensor(83.7286), tensor(83.7286), tensor(72.4000), tensor(83.9143), tensor(83.7286), tensor(83.6714), tensor(83.7286), tensor(83.7286)]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.097\n",
            "AVG Init Test Loss: 0.418, AVG Init Test Acc: 87.010\n",
            "AVG Final Test Loss: 1.202, AVG Final Test Acc: 77.902\n",
            "Round 6 Upload Cost: 1.777 MB\n",
            "Round 6 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 6, Count: 0, Labels: {0: 38, 1: 83, 2: 322, 3: 746, 4: 316, 5: 1845, 6: 778, 7: 498, 8: 820, 9: 3543}\n",
            "Client 7, Count: 0, Labels: {0: 57, 1: 794, 2: 617, 3: 315, 4: 874, 5: 15, 6: 1075, 7: 777, 8: 102, 9: 524}\n",
            "Client 5, Count: 0, Labels: {0: 640, 1: 151, 2: 73, 3: 6, 4: 106, 5: 14, 6: 648, 7: 3658, 8: 2689}\n",
            "Client 9, Count: 0, Labels: {0: 430, 1: 524, 2: 5, 3: 489, 4: 1146, 5: 259, 6: 3, 7: 607, 8: 654, 9: 452}\n",
            "Client 0, Count: 0, Labels: {0: 2, 1: 429, 2: 1141, 3: 189, 4: 547, 5: 521, 6: 909, 7: 314, 8: 80, 9: 178}\n",
            "Client 1, Count: 0, Labels: {0: 216, 1: 1647, 2: 1941, 3: 4, 4: 2635}\n",
            "Client 3, Count: 0, Labels: {0: 197, 1: 92, 2: 381, 3: 2113, 4: 324, 5: 2892, 6: 50}\n",
            "Client 4, Count: 0, Labels: {0: 5, 1: 184, 2: 2, 3: 10, 4: 6, 5: 6, 6: 456, 7: 146, 8: 1655, 9: 1303}\n",
            "Client 2, Count: 0, Labels: {0: 3321, 1: 43, 2: 1063, 3: 2064}\n",
            "Client 8, Count: 0, Labels: {0: 1094, 1: 2053, 2: 455, 3: 64, 4: 46, 5: 448, 6: 2081}\n",
            "\n",
            "Client 6, Correct_pred_per_label: {0: 89, 1: 238, 2: 176, 3: 206, 4: 165, 5: 243, 6: 201, 7: 208, 8: 242, 9: 247}\n",
            "Client 7, Correct_pred_per_label: {0: 112, 1: 241, 2: 198, 3: 207, 4: 227, 5: 144, 6: 164, 7: 247, 8: 220, 9: 224}\n",
            "Client 5, Correct_pred_per_label: {0: 210, 1: 240, 2: 117, 3: 78, 4: 141, 5: 125, 6: 191, 7: 250, 8: 245, 9: 55}\n",
            "Client 9, Correct_pred_per_label: {0: 221, 1: 245, 2: 47, 3: 223, 4: 244, 5: 226, 6: 11, 7: 245, 8: 243, 9: 235}\n",
            "Client 0, Correct_pred_per_label: {0: 2, 1: 246, 2: 217, 3: 186, 4: 176, 5: 226, 6: 194, 7: 245, 8: 236, 9: 222}\n",
            "Client 1, Correct_pred_per_label: {0: 209, 1: 244, 2: 217, 3: 51, 4: 239, 5: 194, 6: 0, 7: 84, 8: 7, 9: 186}\n",
            "Client 3, Correct_pred_per_label: {0: 213, 1: 239, 2: 223, 3: 242, 4: 190, 5: 250, 6: 82, 7: 0, 8: 38, 9: 0}\n",
            "Client 4, Correct_pred_per_label: {0: 45, 1: 241, 2: 71, 3: 144, 4: 87, 5: 184, 6: 236, 7: 214, 8: 248, 9: 249}\n",
            "Client 2, Correct_pred_per_label: {0: 239, 1: 226, 2: 233, 3: 239, 4: 0, 5: 134, 6: 0, 7: 0, 8: 0, 9: 2}\n",
            "Client 8, Correct_pred_per_label: {0: 197, 1: 246, 2: 174, 3: 138, 4: 54, 5: 248, 6: 213, 7: 3, 8: 21, 9: 7}\n",
            "Clusters Glob Acc: [tensor(87.2100), tensor(87.2100), tensor(87.2100), tensor(87.2500), tensor(87.2100), tensor(87.0500), tensor(86.3200), tensor(85.9600), tensor(54.2500), tensor(86.2700)]\n",
            "###### ROUND 7 ######\n",
            "Clients [9 8 6 3 0 4 5 7 1 2]\n",
            "Client 9, Select Cluster: 3\n",
            "acc clusters: [tensor(87.2100), tensor(87.2100), tensor(87.2100), tensor(87.2500), tensor(87.2100), tensor(87.0500), tensor(86.3200), tensor(85.9600), tensor(54.2500), tensor(86.2700)]\n",
            "Client 8, Select Cluster: 0\n",
            "acc clusters: [tensor(84.2714), tensor(84.2714), tensor(84.2714), tensor(83.6571), tensor(84.2714), tensor(83.6286), tensor(84.0571), tensor(81.6857), tensor(77.0714), tensor(83.6571)]\n",
            "Client 6, Select Cluster: 3\n",
            "acc clusters: [tensor(87.2100), tensor(87.2100), tensor(87.2100), tensor(87.2500), tensor(87.2100), tensor(87.0500), tensor(86.3200), tensor(85.9600), tensor(54.2500), tensor(86.2700)]\n",
            "Client 3, Select Cluster: 0\n",
            "acc clusters: [tensor(84.2714), tensor(84.2714), tensor(84.2714), tensor(83.6571), tensor(84.2714), tensor(83.6286), tensor(84.0571), tensor(81.6857), tensor(77.0714), tensor(83.6571)]\n",
            "Client 0, Select Cluster: 3\n",
            "acc clusters: [tensor(87.2100), tensor(87.2100), tensor(87.2100), tensor(87.2500), tensor(87.2100), tensor(87.0500), tensor(86.3200), tensor(85.9600), tensor(54.2500), tensor(86.2700)]\n",
            "Client 4, Select Cluster: 3\n",
            "acc clusters: [tensor(87.2100), tensor(87.2100), tensor(87.2100), tensor(87.2500), tensor(87.2100), tensor(87.0500), tensor(86.3200), tensor(85.9600), tensor(54.2500), tensor(86.2700)]\n",
            "Client 5, Select Cluster: 0\n",
            "acc clusters: [tensor(86.6222), tensor(86.6222), tensor(86.6222), tensor(86.4778), tensor(86.6222), tensor(86.4000), tensor(85.9111), tensor(85.0444), tensor(60.2556), tensor(85.7000)]\n",
            "Client 7, Select Cluster: 3\n",
            "acc clusters: [tensor(87.2100), tensor(87.2100), tensor(87.2100), tensor(87.2500), tensor(87.2100), tensor(87.0500), tensor(86.3200), tensor(85.9600), tensor(54.2500), tensor(86.2700)]\n",
            "Client 1, Select Cluster: 6\n",
            "acc clusters: [tensor(85.1400), tensor(85.1400), tensor(85.1400), tensor(85.7400), tensor(85.1400), tensor(86.7400), tensor(87.7200), tensor(78.9000), tensor(82.4200), tensor(83.7000)]\n",
            "Client 2, Select Cluster: 8\n",
            "acc clusters: [tensor(84.9750), tensor(84.9750), tensor(84.9750), tensor(85.0750), tensor(84.9750), tensor(86.1250), tensor(89.8250), tensor(79.2000), tensor(94.1750), tensor(88.0500)]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.086\n",
            "AVG Init Test Loss: 0.429, AVG Init Test Acc: 87.331\n",
            "AVG Final Test Loss: 1.149, AVG Final Test Acc: 78.900\n",
            "Round 7 Upload Cost: 1.777 MB\n",
            "Round 7 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 9, Count: 0, Labels: {0: 430, 1: 524, 2: 5, 3: 489, 4: 1146, 5: 259, 6: 3, 7: 607, 8: 654, 9: 452}\n",
            "Client 8, Count: 0, Labels: {0: 1094, 1: 2053, 2: 455, 3: 64, 4: 46, 5: 448, 6: 2081}\n",
            "Client 6, Count: 0, Labels: {0: 38, 1: 83, 2: 322, 3: 746, 4: 316, 5: 1845, 6: 778, 7: 498, 8: 820, 9: 3543}\n",
            "Client 3, Count: 0, Labels: {0: 197, 1: 92, 2: 381, 3: 2113, 4: 324, 5: 2892, 6: 50}\n",
            "Client 0, Count: 0, Labels: {0: 2, 1: 429, 2: 1141, 3: 189, 4: 547, 5: 521, 6: 909, 7: 314, 8: 80, 9: 178}\n",
            "Client 4, Count: 0, Labels: {0: 5, 1: 184, 2: 2, 3: 10, 4: 6, 5: 6, 6: 456, 7: 146, 8: 1655, 9: 1303}\n",
            "Client 5, Count: 0, Labels: {0: 640, 1: 151, 2: 73, 3: 6, 4: 106, 5: 14, 6: 648, 7: 3658, 8: 2689}\n",
            "Client 7, Count: 0, Labels: {0: 57, 1: 794, 2: 617, 3: 315, 4: 874, 5: 15, 6: 1075, 7: 777, 8: 102, 9: 524}\n",
            "Client 1, Count: 0, Labels: {0: 216, 1: 1647, 2: 1941, 3: 4, 4: 2635}\n",
            "Client 2, Count: 0, Labels: {0: 3321, 1: 43, 2: 1063, 3: 2064}\n",
            "\n",
            "Client 9, Correct_pred_per_label: {0: 233, 1: 245, 2: 51, 3: 211, 4: 243, 5: 227, 6: 29, 7: 241, 8: 245, 9: 241}\n",
            "Client 8, Correct_pred_per_label: {0: 186, 1: 245, 2: 187, 3: 169, 4: 88, 5: 249, 6: 215, 7: 3, 8: 48, 9: 20}\n",
            "Client 6, Correct_pred_per_label: {0: 88, 1: 240, 2: 171, 3: 231, 4: 168, 5: 240, 6: 200, 7: 192, 8: 243, 9: 247}\n",
            "Client 3, Correct_pred_per_label: {0: 193, 1: 240, 2: 206, 3: 245, 4: 195, 5: 250, 6: 103, 7: 3, 8: 85, 9: 5}\n",
            "Client 0, Correct_pred_per_label: {0: 16, 1: 243, 2: 208, 3: 206, 4: 200, 5: 243, 6: 197, 7: 223, 8: 227, 9: 236}\n",
            "Client 4, Correct_pred_per_label: {0: 35, 1: 242, 2: 69, 3: 139, 4: 84, 5: 170, 6: 236, 7: 223, 8: 248, 9: 249}\n",
            "Client 5, Correct_pred_per_label: {0: 190, 1: 240, 2: 143, 3: 81, 4: 165, 5: 123, 6: 201, 7: 250, 8: 246, 9: 48}\n",
            "Client 7, Correct_pred_per_label: {0: 101, 1: 241, 2: 183, 3: 208, 4: 214, 5: 139, 6: 197, 7: 242, 8: 226, 9: 236}\n",
            "Client 1, Correct_pred_per_label: {0: 219, 1: 248, 2: 214, 3: 50, 4: 239, 5: 233, 6: 1, 7: 168, 8: 39, 9: 222}\n",
            "Client 2, Correct_pred_per_label: {0: 240, 1: 228, 2: 229, 3: 237, 4: 0, 5: 163, 6: 3, 7: 0, 8: 0, 9: 5}\n",
            "Clusters Glob Acc: [tensor(87.5800), tensor(87.4100), tensor(87.5800), tensor(86.9800), tensor(87.5800), tensor(86.9900), tensor(87.5800), tensor(87.5800), tensor(87.4100), tensor(63.0100)]\n",
            "###### ROUND 8 ######\n",
            "Clients [3 1 0 9 5 6 7 4 8 2]\n",
            "Client 3, Select Cluster: 1\n",
            "acc clusters: [tensor(84.7143), tensor(85.), tensor(84.7143), tensor(84.8857), tensor(84.7143), tensor(83.2857), tensor(84.7143), tensor(84.7143), tensor(85.), tensor(79.6714)]\n",
            "Client 1, Select Cluster: 3\n",
            "acc clusters: [tensor(84.8400), tensor(86.9800), tensor(84.8400), tensor(87.6600), tensor(84.8400), tensor(82.6600), tensor(84.8400), tensor(84.8400), tensor(86.9800), tensor(86.5400)]\n",
            "Client 0, Select Cluster: 0\n",
            "acc clusters: [tensor(87.5800), tensor(87.4100), tensor(87.5800), tensor(86.9800), tensor(87.5800), tensor(86.9900), tensor(87.5800), tensor(87.5800), tensor(87.4100), tensor(63.0100)]\n",
            "Client 9, Select Cluster: 0\n",
            "acc clusters: [tensor(87.5800), tensor(87.4100), tensor(87.5800), tensor(86.9800), tensor(87.5800), tensor(86.9900), tensor(87.5800), tensor(87.5800), tensor(87.4100), tensor(63.0100)]\n",
            "Client 5, Select Cluster: 0\n",
            "acc clusters: [tensor(86.8778), tensor(86.8222), tensor(86.8778), tensor(86.4556), tensor(86.8778), tensor(86.1444), tensor(86.8778), tensor(86.8778), tensor(86.8222), tensor(67.1222)]\n",
            "Client 6, Select Cluster: 0\n",
            "acc clusters: [tensor(87.5800), tensor(87.4100), tensor(87.5800), tensor(86.9800), tensor(87.5800), tensor(86.9900), tensor(87.5800), tensor(87.5800), tensor(87.4100), tensor(63.0100)]\n",
            "Client 7, Select Cluster: 0\n",
            "acc clusters: [tensor(87.5800), tensor(87.4100), tensor(87.5800), tensor(86.9800), tensor(87.5800), tensor(86.9900), tensor(87.5800), tensor(87.5800), tensor(87.4100), tensor(63.0100)]\n",
            "Client 4, Select Cluster: 0\n",
            "acc clusters: [tensor(87.5800), tensor(87.4100), tensor(87.5800), tensor(86.9800), tensor(87.5800), tensor(86.9900), tensor(87.5800), tensor(87.5800), tensor(87.4100), tensor(63.0100)]\n",
            "Client 8, Select Cluster: 1\n",
            "acc clusters: [tensor(84.7143), tensor(85.), tensor(84.7143), tensor(84.8857), tensor(84.7143), tensor(83.2857), tensor(84.7143), tensor(84.7143), tensor(85.), tensor(79.6714)]\n",
            "Client 2, Select Cluster: 9\n",
            "acc clusters: [tensor(84.5000), tensor(88.9000), tensor(84.5000), tensor(89.8500), tensor(84.5000), tensor(81.8500), tensor(84.5000), tensor(84.5000), tensor(88.9000), tensor(93.5750)]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.080\n",
            "AVG Init Test Loss: 0.432, AVG Init Test Acc: 87.601\n",
            "AVG Final Test Loss: 1.284, AVG Final Test Acc: 78.456\n",
            "Round 8 Upload Cost: 1.777 MB\n",
            "Round 8 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 3, Count: 0, Labels: {0: 197, 1: 92, 2: 381, 3: 2113, 4: 324, 5: 2892, 6: 50}\n",
            "Client 1, Count: 0, Labels: {0: 216, 1: 1647, 2: 1941, 3: 4, 4: 2635}\n",
            "Client 0, Count: 0, Labels: {0: 2, 1: 429, 2: 1141, 3: 189, 4: 547, 5: 521, 6: 909, 7: 314, 8: 80, 9: 178}\n",
            "Client 9, Count: 0, Labels: {0: 430, 1: 524, 2: 5, 3: 489, 4: 1146, 5: 259, 6: 3, 7: 607, 8: 654, 9: 452}\n",
            "Client 5, Count: 0, Labels: {0: 640, 1: 151, 2: 73, 3: 6, 4: 106, 5: 14, 6: 648, 7: 3658, 8: 2689}\n",
            "Client 6, Count: 0, Labels: {0: 38, 1: 83, 2: 322, 3: 746, 4: 316, 5: 1845, 6: 778, 7: 498, 8: 820, 9: 3543}\n",
            "Client 7, Count: 0, Labels: {0: 57, 1: 794, 2: 617, 3: 315, 4: 874, 5: 15, 6: 1075, 7: 777, 8: 102, 9: 524}\n",
            "Client 4, Count: 0, Labels: {0: 5, 1: 184, 2: 2, 3: 10, 4: 6, 5: 6, 6: 456, 7: 146, 8: 1655, 9: 1303}\n",
            "Client 8, Count: 0, Labels: {0: 1094, 1: 2053, 2: 455, 3: 64, 4: 46, 5: 448, 6: 2081}\n",
            "Client 2, Count: 0, Labels: {0: 3321, 1: 43, 2: 1063, 3: 2064}\n",
            "\n",
            "Client 3, Correct_pred_per_label: {0: 209, 1: 236, 2: 208, 3: 245, 4: 189, 5: 250, 6: 105, 7: 0, 8: 122, 9: 1}\n",
            "Client 1, Correct_pred_per_label: {0: 212, 1: 246, 2: 206, 3: 46, 4: 241, 5: 227, 6: 0, 7: 187, 8: 17, 9: 218}\n",
            "Client 0, Correct_pred_per_label: {0: 4, 1: 244, 2: 204, 3: 178, 4: 200, 5: 244, 6: 205, 7: 238, 8: 217, 9: 224}\n",
            "Client 9, Correct_pred_per_label: {0: 231, 1: 244, 2: 57, 3: 211, 4: 241, 5: 232, 6: 20, 7: 238, 8: 245, 9: 239}\n",
            "Client 5, Correct_pred_per_label: {0: 212, 1: 240, 2: 141, 3: 108, 4: 191, 5: 141, 6: 181, 7: 250, 8: 247, 9: 67}\n",
            "Client 6, Correct_pred_per_label: {0: 95, 1: 236, 2: 146, 3: 226, 4: 162, 5: 247, 6: 212, 7: 47, 8: 241, 9: 240}\n",
            "Client 7, Correct_pred_per_label: {0: 122, 1: 243, 2: 185, 3: 189, 4: 220, 5: 154, 6: 178, 7: 244, 8: 237, 9: 235}\n",
            "Client 4, Correct_pred_per_label: {0: 49, 1: 240, 2: 87, 3: 148, 4: 102, 5: 191, 6: 236, 7: 216, 8: 247, 9: 249}\n",
            "Client 8, Correct_pred_per_label: {0: 182, 1: 245, 2: 176, 3: 167, 4: 48, 5: 249, 6: 222, 7: 3, 8: 32, 9: 17}\n",
            "Client 2, Correct_pred_per_label: {0: 239, 1: 225, 2: 230, 3: 240, 4: 2, 5: 156, 6: 3, 7: 9, 8: 0, 9: 15}\n",
            "Clusters Glob Acc: [tensor(87.0600), tensor(86.9700), tensor(86.9700), tensor(87.0600), tensor(86.9700), tensor(86.9700), tensor(86.9700), tensor(86.9700), tensor(87.0600), tensor(70.3900)]\n",
            "###### ROUND 9 ######\n",
            "Clients [3 2 5 4 0 6 9 7 8 1]\n",
            "Client 3, Select Cluster: 0\n",
            "acc clusters: [tensor(85.2571), tensor(84.5143), tensor(84.5143), tensor(85.2571), tensor(84.5143), tensor(84.5143), tensor(84.5143), tensor(84.5143), tensor(85.2571), tensor(82.3857)]\n",
            "Client 2, Select Cluster: 9\n",
            "acc clusters: [tensor(88.3500), tensor(84.), tensor(84.), tensor(88.3500), tensor(84.), tensor(84.), tensor(84.), tensor(84.), tensor(88.3500), tensor(93.7750)]\n",
            "Client 5, Select Cluster: 0\n",
            "acc clusters: [tensor(86.5556), tensor(86.2889), tensor(86.2889), tensor(86.5556), tensor(86.2889), tensor(86.2889), tensor(86.2889), tensor(86.2889), tensor(86.5556), tensor(74.6111)]\n",
            "Client 4, Select Cluster: 0\n",
            "acc clusters: [tensor(87.0600), tensor(86.9700), tensor(86.9700), tensor(87.0600), tensor(86.9700), tensor(86.9700), tensor(86.9700), tensor(86.9700), tensor(87.0600), tensor(70.3900)]\n",
            "Client 0, Select Cluster: 0\n",
            "acc clusters: [tensor(87.0600), tensor(86.9700), tensor(86.9700), tensor(87.0600), tensor(86.9700), tensor(86.9700), tensor(86.9700), tensor(86.9700), tensor(87.0600), tensor(70.3900)]\n",
            "Client 6, Select Cluster: 0\n",
            "acc clusters: [tensor(87.0600), tensor(86.9700), tensor(86.9700), tensor(87.0600), tensor(86.9700), tensor(86.9700), tensor(86.9700), tensor(86.9700), tensor(87.0600), tensor(70.3900)]\n",
            "Client 9, Select Cluster: 0\n",
            "acc clusters: [tensor(87.0600), tensor(86.9700), tensor(86.9700), tensor(87.0600), tensor(86.9700), tensor(86.9700), tensor(86.9700), tensor(86.9700), tensor(87.0600), tensor(70.3900)]\n",
            "Client 7, Select Cluster: 0\n",
            "acc clusters: [tensor(87.0600), tensor(86.9700), tensor(86.9700), tensor(87.0600), tensor(86.9700), tensor(86.9700), tensor(86.9700), tensor(86.9700), tensor(87.0600), tensor(70.3900)]\n",
            "Client 8, Select Cluster: 0\n",
            "acc clusters: [tensor(85.2571), tensor(84.5143), tensor(84.5143), tensor(85.2571), tensor(84.5143), tensor(84.5143), tensor(84.5143), tensor(84.5143), tensor(85.2571), tensor(82.3857)]\n",
            "Client 1, Select Cluster: 9\n",
            "acc clusters: [tensor(86.9600), tensor(84.4600), tensor(84.4600), tensor(86.9600), tensor(84.4600), tensor(84.4600), tensor(84.4600), tensor(84.4600), tensor(86.9600), tensor(87.2000)]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.074\n",
            "AVG Init Test Loss: 0.471, AVG Init Test Acc: 87.334\n",
            "AVG Final Test Loss: 1.234, AVG Final Test Acc: 79.270\n",
            "Round 9 Upload Cost: 1.777 MB\n",
            "Round 9 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 3, Count: 0, Labels: {0: 197, 1: 92, 2: 381, 3: 2113, 4: 324, 5: 2892, 6: 50}\n",
            "Client 2, Count: 0, Labels: {0: 3321, 1: 43, 2: 1063, 3: 2064}\n",
            "Client 5, Count: 0, Labels: {0: 640, 1: 151, 2: 73, 3: 6, 4: 106, 5: 14, 6: 648, 7: 3658, 8: 2689}\n",
            "Client 4, Count: 0, Labels: {0: 5, 1: 184, 2: 2, 3: 10, 4: 6, 5: 6, 6: 456, 7: 146, 8: 1655, 9: 1303}\n",
            "Client 0, Count: 0, Labels: {0: 2, 1: 429, 2: 1141, 3: 189, 4: 547, 5: 521, 6: 909, 7: 314, 8: 80, 9: 178}\n",
            "Client 6, Count: 0, Labels: {0: 38, 1: 83, 2: 322, 3: 746, 4: 316, 5: 1845, 6: 778, 7: 498, 8: 820, 9: 3543}\n",
            "Client 9, Count: 0, Labels: {0: 430, 1: 524, 2: 5, 3: 489, 4: 1146, 5: 259, 6: 3, 7: 607, 8: 654, 9: 452}\n",
            "Client 7, Count: 0, Labels: {0: 57, 1: 794, 2: 617, 3: 315, 4: 874, 5: 15, 6: 1075, 7: 777, 8: 102, 9: 524}\n",
            "Client 8, Count: 0, Labels: {0: 1094, 1: 2053, 2: 455, 3: 64, 4: 46, 5: 448, 6: 2081}\n",
            "Client 1, Count: 0, Labels: {0: 216, 1: 1647, 2: 1941, 3: 4, 4: 2635}\n",
            "\n",
            "Client 3, Correct_pred_per_label: {0: 212, 1: 241, 2: 209, 3: 240, 4: 203, 5: 250, 6: 82, 7: 12, 8: 55, 9: 6}\n",
            "Client 2, Correct_pred_per_label: {0: 241, 1: 230, 2: 229, 3: 240, 4: 2, 5: 129, 6: 1, 7: 5, 8: 0, 9: 41}\n",
            "Client 5, Correct_pred_per_label: {0: 219, 1: 240, 2: 162, 3: 100, 4: 162, 5: 111, 6: 172, 7: 250, 8: 247, 9: 66}\n",
            "Client 4, Correct_pred_per_label: {0: 45, 1: 242, 2: 95, 3: 146, 4: 100, 5: 174, 6: 238, 7: 226, 8: 248, 9: 248}\n",
            "Client 0, Correct_pred_per_label: {0: 16, 1: 243, 2: 227, 3: 199, 4: 170, 5: 247, 6: 197, 7: 232, 8: 226, 9: 229}\n",
            "Client 6, Correct_pred_per_label: {0: 81, 1: 240, 2: 181, 3: 224, 4: 197, 5: 234, 6: 184, 7: 231, 8: 240, 9: 243}\n",
            "Client 9, Correct_pred_per_label: {0: 231, 1: 244, 2: 66, 3: 217, 4: 241, 5: 227, 6: 15, 7: 244, 8: 241, 9: 230}\n",
            "Client 7, Correct_pred_per_label: {0: 129, 1: 241, 2: 212, 3: 203, 4: 205, 5: 177, 6: 171, 7: 233, 8: 228, 9: 244}\n",
            "Client 8, Correct_pred_per_label: {0: 192, 1: 244, 2: 190, 3: 138, 4: 86, 5: 249, 6: 213, 7: 3, 8: 71, 9: 52}\n",
            "Client 1, Correct_pred_per_label: {0: 218, 1: 246, 2: 220, 3: 40, 4: 235, 5: 182, 6: 2, 7: 6, 8: 4, 9: 112}\n",
            "Clusters Glob Acc: [tensor(87.1600), tensor(73.8100), tensor(87.5900), tensor(86.5300), tensor(87.5900), tensor(87.5900), tensor(87.5500), tensor(87.5900), tensor(87.5500), tensor(87.1600)]\n",
            "###### ROUND 10 ######\n",
            "Clients [1 7 5 6 4 2 0 8 3 9]\n",
            "Client 1, Select Cluster: 1\n",
            "acc clusters: [tensor(88.0800), tensor(89.4000), tensor(85.5400), tensor(80.3400), tensor(85.5400), tensor(85.5400), tensor(87.4400), tensor(85.5400), tensor(87.4400), tensor(88.0800)]\n",
            "Client 7, Select Cluster: 2\n",
            "acc clusters: [tensor(87.1600), tensor(73.8100), tensor(87.5900), tensor(86.5300), tensor(87.5900), tensor(87.5900), tensor(87.5500), tensor(87.5900), tensor(87.5500), tensor(87.1600)]\n",
            "Client 5, Select Cluster: 6\n",
            "acc clusters: [tensor(86.7333), tensor(75.9778), tensor(86.9889), tensor(85.6667), tensor(86.9889), tensor(86.9889), tensor(87.), tensor(86.9889), tensor(87.), tensor(86.7333)]\n",
            "Client 6, Select Cluster: 2\n",
            "acc clusters: [tensor(87.1600), tensor(73.8100), tensor(87.5900), tensor(86.5300), tensor(87.5900), tensor(87.5900), tensor(87.5500), tensor(87.5900), tensor(87.5500), tensor(87.1600)]\n",
            "Client 4, Select Cluster: 2\n",
            "acc clusters: [tensor(87.1600), tensor(73.8100), tensor(87.5900), tensor(86.5300), tensor(87.5900), tensor(87.5900), tensor(87.5500), tensor(87.5900), tensor(87.5500), tensor(87.1600)]\n",
            "Client 2, Select Cluster: 1\n",
            "acc clusters: [tensor(90.), tensor(92.5000), tensor(85.5750), tensor(80.8750), tensor(85.5750), tensor(85.5750), tensor(89.3750), tensor(85.5750), tensor(89.3750), tensor(90.)]\n",
            "Client 0, Select Cluster: 2\n",
            "acc clusters: [tensor(87.1600), tensor(73.8100), tensor(87.5900), tensor(86.5300), tensor(87.5900), tensor(87.5900), tensor(87.5500), tensor(87.5900), tensor(87.5500), tensor(87.1600)]\n",
            "Client 8, Select Cluster: 6\n",
            "acc clusters: [tensor(84.9714), tensor(82.0143), tensor(84.7429), tensor(82.3857), tensor(84.7429), tensor(84.7429), tensor(85.0571), tensor(84.7429), tensor(85.0571), tensor(84.9714)]\n",
            "Client 3, Select Cluster: 6\n",
            "acc clusters: [tensor(84.9714), tensor(82.0143), tensor(84.7429), tensor(82.3857), tensor(84.7429), tensor(84.7429), tensor(85.0571), tensor(84.7429), tensor(85.0571), tensor(84.9714)]\n",
            "Client 9, Select Cluster: 2\n",
            "acc clusters: [tensor(87.1600), tensor(73.8100), tensor(87.5900), tensor(86.5300), tensor(87.5900), tensor(87.5900), tensor(87.5500), tensor(87.5900), tensor(87.5500), tensor(87.1600)]\n",
            "## END OF ROUND ##\n",
            "Average Train loss 0.068\n",
            "AVG Init Test Loss: 0.456, AVG Init Test Acc: 87.696\n",
            "AVG Final Test Loss: 1.307, AVG Final Test Acc: 79.222\n",
            "Round 10 Upload Cost: 1.777 MB\n",
            "Round 10 Download Cost: 0.178 MB\n",
            "----- Analysis End of Round -------\n",
            "Client 1, Count: 0, Labels: {0: 216, 1: 1647, 2: 1941, 3: 4, 4: 2635}\n",
            "Client 7, Count: 0, Labels: {0: 57, 1: 794, 2: 617, 3: 315, 4: 874, 5: 15, 6: 1075, 7: 777, 8: 102, 9: 524}\n",
            "Client 5, Count: 0, Labels: {0: 640, 1: 151, 2: 73, 3: 6, 4: 106, 5: 14, 6: 648, 7: 3658, 8: 2689}\n",
            "Client 6, Count: 0, Labels: {0: 38, 1: 83, 2: 322, 3: 746, 4: 316, 5: 1845, 6: 778, 7: 498, 8: 820, 9: 3543}\n",
            "Client 4, Count: 0, Labels: {0: 5, 1: 184, 2: 2, 3: 10, 4: 6, 5: 6, 6: 456, 7: 146, 8: 1655, 9: 1303}\n",
            "Client 2, Count: 0, Labels: {0: 3321, 1: 43, 2: 1063, 3: 2064}\n",
            "Client 0, Count: 0, Labels: {0: 2, 1: 429, 2: 1141, 3: 189, 4: 547, 5: 521, 6: 909, 7: 314, 8: 80, 9: 178}\n",
            "Client 8, Count: 0, Labels: {0: 1094, 1: 2053, 2: 455, 3: 64, 4: 46, 5: 448, 6: 2081}\n",
            "Client 3, Count: 0, Labels: {0: 197, 1: 92, 2: 381, 3: 2113, 4: 324, 5: 2892, 6: 50}\n",
            "Client 9, Count: 0, Labels: {0: 430, 1: 524, 2: 5, 3: 489, 4: 1146, 5: 259, 6: 3, 7: 607, 8: 654, 9: 452}\n",
            "\n",
            "Client 1, Correct_pred_per_label: {0: 211, 1: 246, 2: 226, 3: 35, 4: 230, 5: 225, 6: 1, 7: 80, 8: 6, 9: 141}\n",
            "Client 7, Correct_pred_per_label: {0: 62, 1: 243, 2: 197, 3: 194, 4: 204, 5: 184, 6: 204, 7: 238, 8: 227, 9: 236}\n",
            "Client 5, Correct_pred_per_label: {0: 207, 1: 241, 2: 142, 3: 93, 4: 177, 5: 126, 6: 188, 7: 250, 8: 246, 9: 61}\n",
            "Client 6, Correct_pred_per_label: {0: 109, 1: 240, 2: 177, 3: 212, 4: 196, 5: 244, 6: 198, 7: 203, 8: 239, 9: 244}\n",
            "Client 4, Correct_pred_per_label: {0: 54, 1: 241, 2: 84, 3: 133, 4: 94, 5: 179, 6: 236, 7: 229, 8: 248, 9: 247}\n",
            "Client 2, Correct_pred_per_label: {0: 239, 1: 232, 2: 229, 3: 241, 4: 5, 5: 80, 6: 4, 7: 39, 8: 0, 9: 35}\n",
            "Client 0, Correct_pred_per_label: {0: 1, 1: 244, 2: 202, 3: 187, 4: 202, 5: 241, 6: 205, 7: 239, 8: 219, 9: 228}\n",
            "Client 8, Correct_pred_per_label: {0: 203, 1: 245, 2: 176, 3: 143, 4: 105, 5: 248, 6: 208, 7: 16, 8: 36, 9: 65}\n",
            "Client 3, Correct_pred_per_label: {0: 200, 1: 240, 2: 216, 3: 238, 4: 206, 5: 250, 6: 95, 7: 5, 8: 104, 9: 5}\n",
            "Client 9, Correct_pred_per_label: {0: 226, 1: 244, 2: 54, 3: 215, 4: 242, 5: 229, 6: 22, 7: 242, 8: 244, 9: 241}\n",
            "Clusters Glob Acc: [tensor(87.2700), tensor(87.4300), tensor(87.4300), tensor(87.4300), tensor(86.9000), tensor(65.6200), tensor(87.4300), tensor(87.6100), tensor(87.6100), tensor(87.4300)]\n",
            "Train Loss: 0.02131779944281576, Test_loss: 1.3068464206655326\n",
            "Train Acc: 99.29902648925781, Test Acc: 79.22179412841797\n",
            "Best Clients AVG Acc: 80.14864349365234\n",
            "Best Global Model Acc: 87.61000061035156\n",
            "Total_clusters: 90, Avg clusters per round: 10.0\n",
            "Avg clustering error: 1.72, Avg clustering acc: 0.8280000000000001\n",
            "Avg clustering error: 1.72, Avg clustering acc: 0.8280000000000001\n"
          ]
        }
      ],
      "source": [
        "! bash flis_dc.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQJFxsQNlXcC"
      },
      "source": [
        "#### FEMNIST (The implementation of FEMNIST for FLIS was reproduced from scratch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0_fiw0Y9m0X",
        "outputId": "8046b16e-89d3-46b1-8dc6-e59383046c5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-09-14 09:57:22.983880: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-14 09:57:23.003476: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-14 09:57:23.009444: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-14 09:57:23.023625: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-09-14 09:57:24.193198: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1726307848.838174   22439 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1726307848.845747   22439 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1726307848.845942   22439 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1726307848.847174   22439 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1726307848.847442   22439 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1726307848.847619   22439 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1726307848.945393   22439 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1726307848.945633   22439 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-09-14 09:57:28.945772: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0000 00:00:1726307848.945854   22439 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-09-14 09:57:28.945987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13949 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "2024-09-14 09:58:04.020874: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8906\n",
            "W0000 00:00:1726307884.108355   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.193857   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.195005   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.196191   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.197438   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.198597   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.199725   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.207330   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.208599   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.209795   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.211021   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.213861   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.216837   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.218056   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.249476   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.261290   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.262507   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.375812   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.384616   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.386302   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.387445   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.388578   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.389726   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.390894   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.392014   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.393146   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.394329   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.395487   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.672298   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.673516   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.674674   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.676459   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.677619   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.678785   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.680418   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.681612   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.682812   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.684217   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.725526   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.727462   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.728627   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.730788   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.734314   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.735489   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.737040   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.738478   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.740408   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.741566   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.756724   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.757887   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.759018   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.760153   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.768015   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.769155   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.770357   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.771558   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.772713   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.775390   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.776602   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.777874   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.779605   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.781722   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.782967   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.784129   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.785583   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307884.788331   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.147183   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.148353   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.149488   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.150699   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.151862   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.153026   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.154147   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.155284   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.156396   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.157576   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.158688   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.159845   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.161001   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.162120   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.163257   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.169713   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.170917   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.172066   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.173227   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.174528   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.175724   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.176875   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.178018   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.179159   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.180341   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.181472   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.206914   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.208117   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.209267   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.210370   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.211485   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.212603   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.213723   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.214909   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.216116   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.217465   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.219923   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.221039   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.222155   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.223297   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.224596   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.225730   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.227161   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.228325   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.230011   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.231170   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.235146   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.236311   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.237462   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.238639   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.239792   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.240949   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.242154   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.243350   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.244509   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.246826   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.248047   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.249281   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.250992   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.253075   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.254297   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.255447   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.256861   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726307888.259570   22439 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "2024-09-14 09:58:08.268133: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "2024-09-14 09:58:11.591345: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "2024-09-14 09:58:18.285649: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "2024-09-14 09:58:31.779470: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "2024-09-14 09:58:58.785822: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "2024-09-14 09:59:52.770594: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "2024-09-14 10:01:40.078235: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        }
      ],
      "source": [
        "! python flis_dc.py --dataset emnist --strategy fedprox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxlangvEK_CR",
        "outputId": "66443856-3e51-4b78-d4c3-e6f19b99e425"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-09-14 10:05:04.066301: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-14 10:05:04.084192: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-14 10:05:04.089608: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-14 10:05:04.103017: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-09-14 10:05:05.219761: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1726308312.524781     963 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1726308312.855339     963 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1726308312.855619     963 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1726308312.858715     963 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1726308312.858908     963 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1726308312.859045     963 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1726308312.980339     963 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1726308312.980594     963 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-09-14 10:05:12.980713: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0000 00:00:1726308312.980786     963 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-09-14 10:05:12.982216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13949 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "Downloading https://biometrics.nist.gov/cs_links/EMNIST/gzip.zip to ./data/EMNIST/raw/gzip.zip\n",
            "100% 561753746/561753746 [00:31<00:00, 17839685.85it/s]\n",
            "Extracting ./data/EMNIST/raw/gzip.zip to ./data/EMNIST/raw\n",
            "2024-09-14 10:06:33.874697: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8906\n",
            "W0000 00:00:1726308394.381496     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308394.467854     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308394.469011     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308394.470185     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308394.471417     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308394.472579     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308394.473703     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308394.511822     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308394.513068     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308394.514225     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308394.515403     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308394.532515     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308394.551295     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308394.552533     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308394.607442     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308394.673544     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308394.674730     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308394.811276     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308394.822588     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308394.827686     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308394.828827     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308394.829960     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308394.831098     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308394.832239     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308394.833345     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308394.834453     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308394.835606     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308394.836724     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.328180     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.329569     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.330726     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.340010     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.341206     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.342422     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.349130     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.350337     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.351557     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.352992     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.437537     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.446915     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.448112     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.458144     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.482909     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.484072     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.485626     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.487009     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.488920     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.490084     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.545290     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.546454     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.547605     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.548732     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.578184     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.579318     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.580516     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.581701     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.582853     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.585449     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.586648     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.587855     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.589582     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.591686     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.592918     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.594070     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.595504     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308395.598233     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.729040     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.730215     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.731343     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.732502     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.733637     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.734754     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.735878     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.737003     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.738117     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.739293     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.740425     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.741581     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.742750     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.743877     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.745005     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.751639     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.752878     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.754025     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.755177     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.756340     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.757512     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.758661     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.759784     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.760908     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.762078     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.763223     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.787643     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.788840     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.789969     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.791086     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.792232     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.793369     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.794477     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.795663     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.796859     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.798232     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.800713     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.801839     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.802964     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.804112     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.805451     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.806584     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.808011     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.809168     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.810872     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.812027     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.815883     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.817022     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.818140     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.819275     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.820422     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.821564     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.822786     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.823977     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.825126     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.827343     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.828555     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.829752     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.831457     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.833531     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.834731     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.835882     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.837291     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308398.839994     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "2024-09-14 10:06:38.847938: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "2024-09-14 10:06:41.919449: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "2024-09-14 10:06:48.328597: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "2024-09-14 10:07:00.661506: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "2024-09-14 10:07:25.229392: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "2024-09-14 10:08:14.450955: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "2024-09-14 10:09:53.580501: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "W0000 00:00:1726308706.620609     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308706.621818     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308706.622932     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308706.624068     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308706.625210     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308706.626316     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308706.627432     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308706.628551     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308706.629665     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308706.630852     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308706.631972     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308706.633136     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308706.634303     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308706.635421     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308706.636543     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308706.642300     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308706.643496     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308706.644633     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308706.645792     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308706.646921     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308706.648063     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308706.649197     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308706.650304     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308706.651410     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308706.652567     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726308706.653697     963 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "Round 1:\n",
            "  Average Loss: 2.5032\n",
            "  Accuracy: 9.40%\n",
            "  Upload Cost: 4884600 elements\n",
            "  Download Cost: 488460 elements\n",
            "2024-09-14 10:13:11.116271: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "Round 2:\n",
            "  Average Loss: 1.5759\n",
            "  Accuracy: 15.87%\n",
            "  Upload Cost: 4884600 elements\n",
            "  Download Cost: 488460 elements\n",
            "2024-09-14 10:19:44.777339: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "Round 3:\n",
            "  Average Loss: 1.0987\n",
            "  Accuracy: 27.40%\n",
            "  Upload Cost: 4884600 elements\n",
            "  Download Cost: 488460 elements\n",
            "Round 4:\n",
            "  Average Loss: 0.8508\n",
            "  Accuracy: 29.67%\n",
            "  Upload Cost: 4884600 elements\n",
            "  Download Cost: 488460 elements\n",
            "Round 5:\n",
            "  Average Loss: 0.7083\n",
            "  Accuracy: 33.47%\n",
            "  Upload Cost: 4884600 elements\n",
            "  Download Cost: 488460 elements\n",
            "2024-09-14 10:32:57.133622: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "Round 6:\n",
            "  Average Loss: 0.6313\n",
            "  Accuracy: 40.33%\n",
            "  Upload Cost: 4884600 elements\n",
            "  Download Cost: 488460 elements\n",
            "Round 7:\n",
            "  Average Loss: 0.5889\n",
            "  Accuracy: 46.20%\n",
            "  Upload Cost: 4884600 elements\n",
            "  Download Cost: 488460 elements\n",
            "Round 8:\n",
            "  Average Loss: 0.5745\n",
            "  Accuracy: 48.80%\n",
            "  Upload Cost: 4884600 elements\n",
            "  Download Cost: 488460 elements\n",
            "Round 9:\n",
            "  Average Loss: 0.5536\n",
            "  Accuracy: 48.47%\n",
            "  Upload Cost: 4884600 elements\n",
            "  Download Cost: 488460 elements\n",
            "Round 10:\n",
            "  Average Loss: 0.5351\n",
            "  Accuracy: 52.80%\n",
            "  Upload Cost: 4884600 elements\n",
            "  Download Cost: 488460 elements\n"
          ]
        }
      ],
      "source": [
        "! python flis_hc.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfKG9M3VzX7g",
        "outputId": "4c4dba42-f5ff-482f-e548-83d339504d41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-09-14 10:59:04.231191: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-14 10:59:04.249890: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-14 10:59:04.255413: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-14 10:59:04.270376: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-09-14 10:59:05.378581: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1726311549.740868   24445 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1726311549.748123   24445 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1726311549.748331   24445 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1726311549.749472   24445 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1726311549.749723   24445 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1726311549.749873   24445 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1726311549.840407   24445 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "I0000 00:00:1726311549.840675   24445 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-09-14 10:59:09.840802: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0000 00:00:1726311549.840879   24445 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-09-14 10:59:09.840999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13949 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "2024-09-14 10:59:42.965000: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8906\n",
            "W0000 00:00:1726311583.045626   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.120767   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.121913   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.123110   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.124316   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.125479   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.126597   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.133660   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.134900   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.136074   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.137289   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.139921   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.142589   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.143811   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.167090   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.178056   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.179253   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.282607   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.290657   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.292265   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.293407   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.294537   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.295672   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.296803   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.297910   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.299018   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.300176   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.301305   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.371150   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.372354   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.373486   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.375187   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.376345   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.377514   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.379167   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.380369   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.381551   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.382973   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.420865   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.422846   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.424038   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.426301   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.429628   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.430803   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.432403   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.433631   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.435537   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.436711   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.451707   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.452893   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.454034   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.455175   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.462677   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.463830   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.465042   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.466230   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.467381   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.470059   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.471264   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.472530   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.474260   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.476370   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.477609   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.478766   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.480205   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311583.482933   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.288660   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.289820   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.290938   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.292074   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.293221   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.294344   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.295460   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.296580   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.297690   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.298867   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.299986   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.301137   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.302303   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.303420   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.304541   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.310735   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.311944   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.313088   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.314230   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.315375   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.316525   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.317668   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.318785   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.319898   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.321069   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.322217   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.334538   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.335732   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.336868   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.337982   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.339096   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.340237   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.341361   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.342558   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.343752   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.345125   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.347727   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.348861   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.349983   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.351122   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.352435   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.353576   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.355022   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.356240   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.357955   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.359102   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.362843   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.363999   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.365142   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.366269   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.367419   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.368586   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.369791   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.370981   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.372145   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.374313   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.375518   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.376737   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.378451   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.380536   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.381742   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.382907   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.384330   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311585.387100   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "2024-09-14 10:59:45.395147: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "2024-09-14 10:59:47.303410: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "2024-09-14 10:59:50.951199: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "2024-09-14 10:59:58.262758: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "2024-09-14 11:00:12.978075: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "2024-09-14 11:00:42.172545: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "2024-09-14 11:01:41.292904: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "W0000 00:00:1726311767.494399   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311767.495571   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311767.496709   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311767.497859   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311767.499003   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311767.500114   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311767.501227   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311767.502347   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311767.503460   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311767.504644   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311767.505769   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311767.506922   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311767.508090   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311767.509209   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311767.510310   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311767.516285   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311767.517485   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311767.518626   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311767.519766   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311767.520902   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311767.522045   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311767.523180   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311767.524285   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311767.525392   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311767.526556   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "W0000 00:00:1726311767.527700   24445 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\n",
            "Round 1:\n",
            "  Average Loss: 2.5960\n",
            "  Accuracy: 10.80%\n",
            "  Upload Cost: 4884600 elements\n",
            "  Download Cost: 488460 elements\n",
            "2024-09-14 11:03:37.027236: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "Round 2:\n",
            "  Average Loss: 1.5680\n",
            "  Accuracy: 27.80%\n",
            "  Upload Cost: 4884600 elements\n",
            "  Download Cost: 488460 elements\n",
            "2024-09-14 11:07:29.993478: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "Round 3:\n",
            "  Average Loss: 1.0955\n",
            "  Accuracy: 35.07%\n",
            "  Upload Cost: 4884600 elements\n",
            "  Download Cost: 488460 elements\n",
            "Round 4:\n",
            "  Average Loss: 0.8193\n",
            "  Accuracy: 40.33%\n",
            "  Upload Cost: 4884600 elements\n",
            "  Download Cost: 488460 elements\n",
            "Round 5:\n",
            "  Average Loss: 0.6646\n",
            "  Accuracy: 41.40%\n",
            "  Upload Cost: 4884600 elements\n",
            "  Download Cost: 488460 elements\n",
            "2024-09-14 11:15:14.451386: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "Round 6:\n",
            "  Average Loss: 0.5692\n",
            "  Accuracy: 42.20%\n",
            "  Upload Cost: 4884600 elements\n",
            "  Download Cost: 488460 elements\n",
            "Round 7:\n",
            "  Average Loss: 0.5078\n",
            "  Accuracy: 50.73%\n",
            "  Upload Cost: 4884600 elements\n",
            "  Download Cost: 488460 elements\n",
            "Round 8:\n",
            "  Average Loss: 0.4772\n",
            "  Accuracy: 52.80%\n",
            "  Upload Cost: 4884600 elements\n",
            "  Download Cost: 488460 elements\n",
            "Round 9:\n",
            "  Average Loss: 0.4899\n",
            "  Accuracy: 55.60%\n",
            "  Upload Cost: 4884600 elements\n",
            "  Download Cost: 488460 elements\n",
            "Round 10:\n",
            "  Average Loss: 0.4824\n",
            "  Accuracy: 58.20%\n",
            "  Upload Cost: 4884600 elements\n",
            "  Download Cost: 488460 elements\n"
          ]
        }
      ],
      "source": [
        "! python flis_dc.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}