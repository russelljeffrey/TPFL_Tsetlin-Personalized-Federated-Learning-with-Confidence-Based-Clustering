{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### This notebook provides the size of baseline models parameters.\n",
        "#### N.B.: Numerous models exist in baseline methods. The models that exist in this notebook are the ones that were used to be compared with TPFL model. For FLIS baseline for instance, pretrained models are used as well that are not included in here."
      ],
      "metadata": {
        "id": "unJC0kQmvQ26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### IFCA baseline"
      ],
      "metadata": {
        "id": "Dt4RwPsxiPm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "\n",
        "IMAGE_SIZE = 28\n",
        "\n",
        "class ClientModel:\n",
        "    def __init__(self, seed, lr, num_classes):\n",
        "        self.num_classes = num_classes\n",
        "        self.seed = seed\n",
        "        self.lr = lr\n",
        "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.lr)\n",
        "\n",
        "    def create_model(self):\n",
        "        \"\"\"Model function for CNN.\"\"\"\n",
        "        features = tf.placeholder(tf.float32, shape=[None, IMAGE_SIZE, IMAGE_SIZE, 1], name='features')\n",
        "        labels = tf.placeholder(tf.int64, shape=[None], name='labels')\n",
        "\n",
        "        conv1 = tf.keras.layers.Conv2D(filters=32, kernel_size=[5, 5], padding=\"same\", activation='relu')(features)\n",
        "        pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2)(conv1)\n",
        "        conv2 = tf.keras.layers.Conv2D(filters=64, kernel_size=[5, 5], padding=\"same\", activation='relu')(pool1)\n",
        "        pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2)(conv2)\n",
        "        pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
        "        dense = tf.keras.layers.Dense(units=2048, activation='relu')(pool2_flat)\n",
        "        logits = tf.keras.layers.Dense(units=self.num_classes)(dense)\n",
        "\n",
        "        predictions = {\n",
        "            \"classes\": tf.argmax(input=logits, axis=1),\n",
        "            \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
        "        }\n",
        "\n",
        "        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
        "\n",
        "        eval_metric_ops = tf.count_nonzero(tf.equal(labels, predictions[\"classes\"]))\n",
        "        self.trainable_variables = tf.trainable_variables()\n",
        "\n",
        "        train_op = self.optimizer.minimize(\n",
        "            loss=loss,\n",
        "            var_list=self.trainable_variables\n",
        "        )\n",
        "\n",
        "        return features, labels, train_op, eval_metric_ops, loss\n",
        "\n",
        "    def process_x(self, raw_x_batch):\n",
        "        return np.array(raw_x_batch)\n",
        "\n",
        "    def process_y(self, raw_y_batch):\n",
        "        return np.array(raw_y_batch)\n",
        "import numpy as np\n",
        "\n",
        "def calculate_model_size(trainable_variables):\n",
        "    total_params = 0\n",
        "\n",
        "    for var in trainable_variables:\n",
        "        var_shape = var.shape.as_list()\n",
        "        var_size = np.prod(var_shape)\n",
        "        total_params += var_size\n",
        "\n",
        "    # Assuming each parameter is a float32 (4 bytes)\n",
        "    total_size_bytes = total_params * 4  # 4 bytes per parameter (float32)\n",
        "    total_size_megabytes = total_size_bytes / (1024 * 1024)  # bytes to megabytes\n",
        "\n",
        "    return total_params, total_size_megabytes\n",
        "\n",
        "model1 = ClientModel(seed=123, lr=0.01, num_classes=10)\n",
        "model2 = ClientModel(seed=123, lr=0.01, num_classes=62)\n",
        "features, labels, train_op, eval_metric_ops, loss = model1.create_model()\n",
        "features, labels, train_op, eval_metric_ops, loss = model2.create_model()\n",
        "\n",
        "print(\"10 classes\")\n",
        "total_params, total_size_megabytes = calculate_model_size(model1.trainable_variables)\n",
        "print(f\"Total number of parameters: {total_params}\")\n",
        "print(f\"Total model size: {total_size_megabytes:.2f} MB\")\n",
        "\n",
        "print(\"62 classes\")\n",
        "total_params, total_size_megabytes = calculate_model_size(model2.trainable_variables)\n",
        "print(f\"Total number of parameters: {total_params}\")\n",
        "print(f\"Total model size: {total_size_megabytes:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L4YEmVMCRkK",
        "outputId": "a057a2c4-6d1c-43ff-f8a2-b74a8284d191"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 classes\n",
            "Total number of parameters: 6497162\n",
            "Total model size: 24.78 MB\n",
            "62 classes\n",
            "Total number of parameters: 13100872\n",
            "Total model size: 49.98 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### FLIS CNN baseline"
      ],
      "metadata": {
        "id": "Z_0tSTwHfnc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class SimpleCNNMNIST(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, output_dim=10):\n",
        "        super(SimpleCNNMNIST, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
        "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
        "        self.fc3 = nn.Linear(hidden_dims[1], output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 4 * 4)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class SimpleCNNMNIST2(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, output_dim=62):\n",
        "        super(SimpleCNNMNIST2, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
        "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
        "        self.fc3 = nn.Linear(hidden_dims[1], output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 4 * 4)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "def calculate_model_size(model):\n",
        "    total_params = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        param_shape = param.shape\n",
        "        var_size = np.prod(param_shape)\n",
        "        total_params += var_size\n",
        "\n",
        "    total_size_bytes = total_params * 4  # 4 bytes per parameter (float32)\n",
        "    total_size_megabytes = total_size_bytes / (1024 * 1024)  # bytes to megabytes\n",
        "\n",
        "    return total_params, total_size_megabytes\n",
        "\n",
        "input_dim = 28 * 28\n",
        "hidden_dims = [120, 84]\n",
        "model1 = SimpleCNNMNIST(input_dim=input_dim, hidden_dims=hidden_dims)\n",
        "model2 = SimpleCNNMNIST2(input_dim=input_dim, hidden_dims=hidden_dims)\n",
        "\n",
        "# Calculate the size\n",
        "total_params, total_size_megabytes = calculate_model_size(model1)\n",
        "print(f\"Total number of parameters: {total_params}\")\n",
        "print(f\"Total model size: {total_size_megabytes:.2f} MB\")\n",
        "\n",
        "# Calculate the size\n",
        "total_params, total_size_megabytes = calculate_model_size(model2)\n",
        "print(f\"Total number of parameters: {total_params}\")\n",
        "print(f\"Total model size: {total_size_megabytes:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dVZ1KQ9elDd",
        "outputId": "896919de-bc80-455d-86ca-e9ec14c2dfad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 107786\n",
            "Total model size: 0.41 MB\n",
            "Total number of parameters: 112206\n",
            "Total model size: 0.43 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FedAvg and FedProx baseline"
      ],
      "metadata": {
        "id": "b152g7LEgNhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "def create_cnn_model():\n",
        "    return models.Sequential([\n",
        "        layers.Conv2D(20, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1)),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(50, kernel_size=(5, 5), activation='relu'),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(500, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "def create_cnn_model2():\n",
        "    return models.Sequential([\n",
        "        layers.Conv2D(20, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1)),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(50, kernel_size=(5, 5), activation='relu'),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(500, activation='relu'),\n",
        "        layers.Dense(62, activation='softmax')\n",
        "    ])\n",
        "\n",
        "def calculate_model_size(model):\n",
        "    total_params = 0\n",
        "    for layer in model.layers:\n",
        "        for var in layer.trainable_variables:\n",
        "            var_shape = var.shape.as_list()\n",
        "            var_size = np.prod(var_shape)\n",
        "            total_params += var_size\n",
        "\n",
        "    total_size_bytes = total_params * 4  # 4 bytes per parameter (float32)\n",
        "    total_size_megabytes = total_size_bytes / (1024 * 1024)  # bytes to megabytes\n",
        "\n",
        "    return total_params, total_size_megabytes\n",
        "\n",
        "model1 = create_cnn_model()\n",
        "model2 = create_cnn_model2()\n",
        "\n",
        "print(\"10 classes\")\n",
        "total_params, total_size_megabytes = calculate_model_size(model1)\n",
        "print(f\"Total number of parameters: {total_params}\")\n",
        "print(f\"Total model size: {total_size_megabytes:.2f} MB\")\n",
        "\n",
        "print(\"62 classes\")\n",
        "total_params, total_size_megabytes = calculate_model_size(model2)\n",
        "print(f\"Total number of parameters: {total_params}\")\n",
        "print(f\"Total model size: {total_size_megabytes:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzZO-gCcfGh3",
        "outputId": "22f9c8cf-4afa-40dd-8e35-6d99dd3dde5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 classes\n",
            "Total number of parameters: 431080\n",
            "Total model size: 1.64 MB\n",
            "62 classes\n",
            "Total number of parameters: 457132\n",
            "Total model size: 1.74 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    }
  ]
}